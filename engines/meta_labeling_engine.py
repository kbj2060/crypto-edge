#!/usr/bin/env python3
"""
Meta-Labeling Engine
ë§ˆë¥´ì½”ìŠ¤ ë¡œí˜ì¦ˆ ë° í”„ë¼ë„ì˜ ë©”íƒ€ ë¼ë²¨ë§ ê¸°ë²• êµ¬í˜„

ë©”íƒ€ ë¼ë²¨ë§ì€ 2ë‹¨ê³„ ì ‘ê·¼ë²•:
1. 1ë‹¨ê³„: ë°©í–¥ ì˜ˆì¸¡ (ê¸°ì¡´ TradeDecisionEngine)
2. 2ë‹¨ê³„: ë©”íƒ€ ë¼ë²¨ë§ - ê±°ë˜ ì‹¤í–‰ ì—¬ë¶€ ê²°ì • (ì´ ëª¨ë“ˆ)
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime, timedelta
from pathlib import Path
import pickle
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, precision_recall_curve, roc_auc_score
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')


class MetaLabelingEngine:
    """
    ë©”íƒ€ ë¼ë²¨ë§ ì—”ì§„
    
    ê¸°ì¡´ ëª¨ë¸ì˜ ë°©í–¥ ì˜ˆì¸¡ì´ ë§ëŠ”ì§€ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•˜ì—¬
    ê±°ë˜ ì‹¤í–‰ ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        model_type: str = "random_forest",
        min_samples_for_training: int = 100,
        confidence_threshold: float = 0.6,
        model_save_path: Optional[str] = None
    ):
        """
        Args:
            model_type: ëª¨ë¸ íƒ€ì… ("random_forest", "gradient_boosting")
            min_samples_for_training: í•™ìŠµì— í•„ìš”í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
            confidence_threshold: ê±°ë˜ ì‹¤í–‰ì„ ìœ„í•œ ìµœì†Œ ì‹ ë¢°ë„
            model_save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        """
        self.model_type = model_type
        self.min_samples_for_training = min_samples_for_training
        self.confidence_threshold = confidence_threshold
        self.model_save_path = model_save_path or "engines/meta_labeling_model.pkl"
        
        self.model = None
        self.scaler = StandardScaler()
        self.is_trained = False
        self.feature_names = []
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self._init_model()
    
    def _init_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        if self.model_type == "random_forest":
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=20,
                min_samples_leaf=10,
                random_state=42,
                class_weight='balanced'
            )
        elif self.model_type == "gradient_boosting":
            self.model = GradientBoostingClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=42
            )
        else:
            raise ValueError(f"Unknown model_type: {self.model_type}")
    
    def extract_features(self, decision: Dict[str, Any], market_data: Optional[Dict[str, Any]] = None) -> np.ndarray:
        """
        ê²°ì •ì—ì„œ íŠ¹ì„± ì¶”ì¶œ
        
        Args:
            decision: ê±°ë˜ ê²°ì • ë”•ì…”ë„ˆë¦¬
            market_data: ì‹œì¥ ë°ì´í„° (ì„ íƒì )
            
        Returns:
            íŠ¹ì„± ë²¡í„°
        """
        features = []
        
        # 1. ê²°ì • ê´€ë ¨ íŠ¹ì„±
        net_score = decision.get("net_score", 0.0)
        action = decision.get("action", "HOLD")
        
        # Action ì¸ì½”ë”©
        action_encoded = {"LONG": 1, "SHORT": -1, "HOLD": 0}.get(action, 0)
        features.append(action_encoded)
        features.append(net_score)
        features.append(abs(net_score))  # ì ˆëŒ€ê°’
        
        # 2. ì‹ ë¢°ë„ ê´€ë ¨ íŠ¹ì„±
        meta = decision.get("meta", {})
        synergy_meta = meta.get("synergy_meta", {})
        
        confidence = synergy_meta.get("confidence", "LOW")
        confidence_map = {"HIGH": 0.8, "MEDIUM": 0.5, "LOW": 0.2}
        confidence_value = confidence_map.get(confidence, 0.2)
        features.append(confidence_value)
        
        # 3. ì „ëµ ì‚¬ìš© ìˆ˜
        strategies_used = decision.get("strategies_used", [])
        features.append(len(strategies_used))
        
        # 4. ì‹œë„ˆì§€ ë©”íƒ€ íŠ¹ì„±
        buy_score = synergy_meta.get("buy_score", 0.0)
        sell_score = synergy_meta.get("sell_score", 0.0)
        signals_used = synergy_meta.get("signals_used", 0)
        
        features.append(buy_score)
        features.append(sell_score)
        features.append(signals_used)
        features.append(abs(buy_score - sell_score))  # ì ìˆ˜ ì°¨ì´
        
        # 5. í¬ì§€ì…˜ í¬ê¸° ê´€ë ¨
        sizing = decision.get("sizing", {})
        risk_usd = sizing.get("risk_usd", 0.0)
        leverage = decision.get("leverage", 1)
        
        features.append(risk_usd)
        features.append(leverage)
        
        # 6. ì¹´í…Œê³ ë¦¬ ì •ë³´
        category = decision.get("category", "")
        category_map = {"SHORT_TERM": 0, "MEDIUM_TERM": 1, "LONG_TERM": 2}
        category_encoded = category_map.get(category, 0)
        features.append(category_encoded)
        
        # 7. ì‹œì¥ ë°ì´í„° íŠ¹ì„± (ìˆëŠ” ê²½ìš°)
        if market_data:
            # ATR, ë³¼ë¥¨, ë³€ë™ì„± ë“± ì¶”ê°€ ê°€ëŠ¥
            atr = market_data.get("atr", 0.0)
            volume = market_data.get("volume", 0.0)
            volatility = market_data.get("volatility", 0.0)
            
            features.append(atr)
            features.append(volume)
            features.append(volatility)
        else:
            # ê¸°ë³¸ê°’
            features.extend([0.0, 0.0, 0.0])
        
        return np.array(features, dtype=np.float32)
    
    def create_meta_labels(
        self,
        decisions_df: pd.DataFrame,
        price_data: pd.DataFrame,
        lookforward_periods: int = 20
    ) -> pd.DataFrame:
        """
        ê³¼ê±° ê²°ì • ë°ì´í„°ì—ì„œ ë©”íƒ€ ë¼ë²¨ ìƒì„±
        
        ë©”íƒ€ ë¼ë²¨: ë°©í–¥ ì˜ˆì¸¡ì´ ë§ì•˜ëŠ”ì§€ ì—¬ë¶€ (1: ë§ìŒ, 0: í‹€ë¦¼)
        
        Args:
            decisions_df: ê³¼ê±° ê²°ì • ë°ì´í„°í”„ë ˆì„
            price_data: ê°€ê²© ë°ì´í„°í”„ë ˆì„ (close ì»¬ëŸ¼ í•„ìš”)
            lookforward_periods: ë¯¸ë˜ ëª‡ ê¸°ê°„ì„ ë³´ê³  ì„±ê³µ ì—¬ë¶€ íŒë‹¨
            
        Returns:
            ë©”íƒ€ ë¼ë²¨ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
        """
        df = decisions_df.copy()
        
        # timestampë¥¼ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)
            df = df.set_index('timestamp').sort_index()
        
        if 'close' not in price_data.columns:
            raise ValueError("price_data must have 'close' column")
        
        price_data = price_data.copy()
        if not isinstance(price_data.index, pd.DatetimeIndex):
            if 'timestamp' in price_data.columns:
                price_data['timestamp'] = pd.to_datetime(price_data['timestamp'], utc=True)
                price_data = price_data.set_index('timestamp')
        
        price_data = price_data.sort_index()
        
        # ë©”íƒ€ ë¼ë²¨ ìƒì„±
        meta_labels = []
        
        for idx, row in df.iterrows():
            # í•´ë‹¹ ì‹œì ì˜ ê°€ê²© ì°¾ê¸°
            try:
                current_price = price_data.loc[idx, 'close']
            except KeyError:
                # ê°€ì¥ ê°€ê¹Œìš´ ê°€ê²© ì°¾ê¸°
                try:
                    nearest_idx = price_data.index.get_indexer([idx], method='nearest')[0]
                    current_price = price_data.iloc[nearest_idx]['close']
                except:
                    meta_labels.append(0)
                    continue
            
            # ë¯¸ë˜ ê°€ê²© ì°¾ê¸°
            try:
                future_idx = price_data.index[price_data.index > idx][:lookforward_periods]
                if len(future_idx) < lookforward_periods:
                    meta_labels.append(0)
                    continue
                
                future_price = price_data.loc[future_idx[-1], 'close']
            except:
                meta_labels.append(0)
                continue
            
            # ë°©í–¥ ì˜ˆì¸¡ í™•ì¸
            action = row.get('action', 'HOLD')
            if action == 'HOLD':
                meta_labels.append(0)  # HOLDëŠ” ê±°ë˜í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ 0
                continue
            
            # ì‹¤ì œ ê°€ê²© ë³€í™”
            price_change = (future_price - current_price) / current_price
            
            # ë°©í–¥ì´ ë§ì•˜ëŠ”ì§€ í™•ì¸
            if action == 'LONG':
                # LONG ì˜ˆì¸¡ì´ ë§ì•˜ëŠ”ì§€ (ê°€ê²© ìƒìŠ¹)
                is_correct = 1 if price_change > 0 else 0
            elif action == 'SHORT':
                # SHORT ì˜ˆì¸¡ì´ ë§ì•˜ëŠ”ì§€ (ê°€ê²© í•˜ë½)
                is_correct = 1 if price_change < 0 else 0
            else:
                is_correct = 0
            
            meta_labels.append(is_correct)
        
        df['meta_label'] = meta_labels
        return df.reset_index()
    
    def train(
        self,
        decisions_df: pd.DataFrame,
        price_data: pd.DataFrame,
        test_size: float = 0.2,
        retrain: bool = False
    ) -> Dict[str, Any]:
        """
        ë©”íƒ€ ë¼ë²¨ë§ ëª¨ë¸ í•™ìŠµ
        
        Args:
            decisions_df: ê²°ì • ë°ì´í„°í”„ë ˆì„
            price_data: ê°€ê²© ë°ì´í„°í”„ë ˆì„
            test_size: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨
            retrain: ê¸°ì¡´ ëª¨ë¸ì´ ìˆì–´ë„ ì¬í•™ìŠµí• ì§€ ì—¬ë¶€
            
        Returns:
            í•™ìŠµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ë©”íƒ€ ë¼ë²¨ ìƒì„±
        print("ğŸ“Š ë©”íƒ€ ë¼ë²¨ ìƒì„± ì¤‘...")
        labeled_df = self.create_meta_labels(decisions_df, price_data)
        
        # ê±°ë˜ê°€ ìˆëŠ” ê²°ì •ë§Œ í•„í„°ë§ (HOLD ì œì™¸)
        labeled_df = labeled_df[labeled_df['action'].isin(['LONG', 'SHORT'])]
        
        if len(labeled_df) < self.min_samples_for_training:
            print(f"âš ï¸ í•™ìŠµ ë°ì´í„° ë¶€ì¡±: {len(labeled_df)} < {self.min_samples_for_training}")
            return {
                "success": False,
                "message": f"í•™ìŠµ ë°ì´í„° ë¶€ì¡±: {len(labeled_df)}ê°œ"
            }
        
        # íŠ¹ì„± ì¶”ì¶œ
        print("ğŸ” íŠ¹ì„± ì¶”ì¶œ ì¤‘...")
        X = []
        y = []
        
        for _, row in labeled_df.iterrows():
            try:
                # rowë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                if isinstance(row, pd.Series):
                    decision_dict = row.to_dict()
                else:
                    decision_dict = dict(row)
                
                features = self.extract_features(decision_dict)
                X.append(features)
                y.append(row['meta_label'])
            except Exception as e:
                print(f"âš ï¸ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨ (ê±´ë„ˆëœ€): {e}")
                continue
        
        if len(X) < self.min_samples_for_training:
            print(f"âš ï¸ ìœ íš¨í•œ íŠ¹ì„± ë¶€ì¡±: {len(X)} < {self.min_samples_for_training}")
            return {
                "success": False,
                "message": f"ìœ íš¨í•œ íŠ¹ì„± ë¶€ì¡±: {len(X)}ê°œ"
            }
        
        X = np.array(X)
        y = np.array(y)
        
        # íŠ¹ì„± ì´ë¦„ ì €ì¥
        self.feature_names = [
            'action_encoded', 'net_score', 'abs_net_score', 'confidence',
            'num_strategies', 'buy_score', 'sell_score', 'signals_used',
            'score_diff', 'risk_usd', 'leverage', 'category',
            'atr', 'volume', 'volatility'
        ]
        
        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
        try:
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_test_scaled = self.scaler.transform(X_test)
        except Exception as e:
            print(f"âŒ íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ ì‹¤íŒ¨: {e}")
            # ìŠ¤ì¼€ì¼ë§ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°ì´í„° ì‚¬ìš©
            X_train_scaled = X_train
            X_test_scaled = X_test
        
        # ëª¨ë¸ í•™ìŠµ
        print(f"ğŸ“ ëª¨ë¸ í•™ìŠµ ì¤‘... ({len(X_train)}ê°œ ìƒ˜í”Œ)")
        self.model.fit(X_train_scaled, y_train)
        
        # ì˜ˆì¸¡ ë° í‰ê°€
        y_pred = self.model.predict(X_test_scaled)
        y_pred_proba = self.model.predict_proba(X_test_scaled)[:, 1]
        
        # ì •í™•ë„ ê³„ì‚°
        accuracy = np.mean(y_pred == y_test)
        
        # ROC-AUC ê³„ì‚°
        try:
            roc_auc = roc_auc_score(y_test, y_pred_proba)
        except:
            roc_auc = 0.0
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸
        report = classification_report(y_test, y_pred, output_dict=True)
        
        print(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
        print(f"   ì •í™•ë„: {accuracy:.3f}")
        print(f"   ROC-AUC: {roc_auc:.3f}")
        print(f"   Precision: {report['1']['precision']:.3f}")
        print(f"   Recall: {report['1']['recall']:.3f}")
        
        self.is_trained = True
        
        # ëª¨ë¸ ì €ì¥
        self.save_model()
        
        return {
            "success": True,
            "accuracy": accuracy,
            "roc_auc": roc_auc,
            "classification_report": report,
            "train_samples": len(X_train),
            "test_samples": len(X_test),
            "feature_importance": dict(zip(
                self.feature_names,
                self.model.feature_importances_
            )) if hasattr(self.model, 'feature_importances_') else {}
        }
    
    def predict(self, decision: Dict[str, Any], market_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        ê±°ë˜ ì‹¤í–‰ ì—¬ë¶€ ì˜ˆì¸¡
        
        Args:
            decision: ê±°ë˜ ê²°ì • ë”•ì…”ë„ˆë¦¬
            market_data: ì‹œì¥ ë°ì´í„° (ì„ íƒì )
            
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.is_trained:
            # ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìœ¼ë©´ ê¸°ë³¸ ë¡œì§ ì‚¬ìš©
            return self._default_prediction(decision)
        
        # íŠ¹ì„± ì¶”ì¶œ
        try:
            features = self.extract_features(decision, market_data)
            features_scaled = self.scaler.transform([features])
            
            # ì˜ˆì¸¡
            prediction = self.model.predict(features_scaled)[0]
            probability = self.model.predict_proba(features_scaled)[0][1]
        except Exception as e:
            # ì˜ˆì¸¡ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë¡œì§ ì‚¬ìš©
            print(f"âš ï¸ ë©”íƒ€ ë¼ë²¨ë§ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return self._default_prediction(decision)
        
        # ê±°ë˜ ì‹¤í–‰ ì—¬ë¶€ ê²°ì •
        should_execute = (
            prediction == 1 and 
            probability >= self.confidence_threshold
        )
        
        return {
            "should_execute": should_execute,
            "prediction": int(prediction),
            "probability": float(probability),
            "confidence": "HIGH" if probability >= 0.7 else "MEDIUM" if probability >= 0.5 else "LOW"
        }
    
    def _default_prediction(self, decision: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ì„ ë•Œ ê¸°ë³¸ ì˜ˆì¸¡ ë¡œì§"""
        net_score = decision.get("net_score", 0.0)
        meta = decision.get("meta", {})
        synergy_meta = meta.get("synergy_meta", {})
        confidence = synergy_meta.get("confidence", "LOW")
        
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±
        confidence_map = {"HIGH": 0.8, "MEDIUM": 0.5, "LOW": 0.2}
        confidence_value = confidence_map.get(confidence, 0.2)
        
        # ë†’ì€ ì ìˆ˜ì™€ ì‹ ë¢°ë„ì¼ ë•Œë§Œ ì‹¤í–‰
        should_execute = (
            abs(net_score) > 0.3 and
            confidence_value >= 0.5
        )
        
        return {
            "should_execute": should_execute,
            "prediction": 1 if should_execute else 0,
            "probability": confidence_value * abs(net_score),
            "confidence": confidence,
            "note": "ê¸°ë³¸ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš© (ëª¨ë¸ ë¯¸í•™ìŠµ)"
        }
    
    def save_model(self, path: Optional[str] = None):
        """ëª¨ë¸ ì €ì¥"""
        if self.model is None:
            return
        
        save_path = path or self.model_save_path
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(save_path, 'wb') as f:
            pickle.dump({
                'model': self.model,
                'scaler': self.scaler,
                'feature_names': self.feature_names,
                'model_type': self.model_type,
                'is_trained': self.is_trained
            }, f)
        
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
    
    def load_model(self, path: Optional[str] = None):
        """ëª¨ë¸ ë¡œë“œ"""
        load_path = path or self.model_save_path
        
        if not Path(load_path).exists():
            print(f"âš ï¸ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {load_path}")
            return False
        
        try:
            with open(load_path, 'rb') as f:
                data = pickle.load(f)
            
            self.model = data['model']
            self.scaler = data['scaler']
            self.feature_names = data.get('feature_names', [])
            self.model_type = data.get('model_type', self.model_type)
            self.is_trained = data.get('is_trained', False)
            
            print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {load_path}")
            return True
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

