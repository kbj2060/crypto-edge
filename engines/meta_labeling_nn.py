#!/usr/bin/env python3
"""
ë”¥ëŸ¬ë‹ ê¸°ë°˜ ë©”íƒ€ ë¼ë²¨ë§ ì‹ ê²½ë§ ì—”ì§„ (scikit-learn MLPClassifier ì‚¬ìš©)

ì…ë ¥: í‰ë©´í™”ëœ ë°ì´í„° (ì§€í‘œ ê°’, ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ë“±)
ì€ë‹‰ ë ˆì´ì–´: MLP (ReLU)
ì¶œë ¥: Sigmoidë¡œ 0~1 ì‚¬ì´ì˜ ì„±ê³µ í™•ë¥ 

Mac í˜¸í™˜: PyTorch ëŒ€ì‹  scikit-learnì˜ MLPClassifier ì‚¬ìš©
"""

import numpy as np
import pandas as pd
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path
import pickle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, roc_auc_score
import warnings
warnings.filterwarnings('ignore')


class MetaLabelingNNEngine:
    """ë”¥ëŸ¬ë‹ ê¸°ë°˜ ë©”íƒ€ ë¼ë²¨ë§ ì—”ì§„ (scikit-learn MLPClassifier ì‚¬ìš©)"""
    
    MODEL_PATH = "data/meta_labeling_nn_model.pkl"
    SCALER_PATH = "data/meta_labeling_nn_scaler.pkl"
    FEATURE_NAMES_PATH = "data/meta_labeling_nn_feature_names.pkl"
    
    def __init__(
        self,
        hidden_layer_sizes: Tuple[int, ...] = (128, 64, 32),
        dropout: float = 0.3,
        learning_rate: float = 0.001,
        max_iter: int = 500,
        confidence_threshold: float = 0.5
    ):
        """
        Args:
            hidden_layer_sizes: ì€ë‹‰ ë ˆì´ì–´ í¬ê¸° íŠœí”Œ (ì˜ˆ: (128, 64, 32))
            dropout: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ (MLPClassifierëŠ” alphaë¡œ L2 ì •ê·œí™”)
            learning_rate: í•™ìŠµë¥  (MLPClassifierëŠ” learning_rate_init)
            max_iter: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
            confidence_threshold: ê±°ë˜ ì‹¤í–‰ ìµœì†Œ ì‹ ë¢°ë„
        """
        self.hidden_layer_sizes = hidden_layer_sizes
        self.dropout = dropout
        self.learning_rate = learning_rate
        self.max_iter = max_iter
        self.confidence_threshold = confidence_threshold
        
        # scikit-learn MLPClassifier ì‚¬ìš©
        self.model: Optional[MLPClassifier] = None
        self.scaler = StandardScaler()
        self.feature_names: List[str] = []
        self.input_dim: Optional[int] = None
        self.is_trained = False
        
        print(f"ğŸ”§ MLPClassifier ì‚¬ìš© (ì€ë‹‰ ë ˆì´ì–´: {hidden_layer_sizes})")
    
    def extract_features(
        self,
        decision: Dict[str, Any],
        market_data: Optional[Dict[str, Any]] = None,
        indicators: Optional[Dict[str, Any]] = None
    ) -> np.ndarray:
        """
        í‰ë©´í™”ëœ ë°ì´í„°ì—ì„œ íŠ¹ì„± ì¶”ì¶œ - ì „ëµ scoreë§Œ ì‚¬ìš©
        
        Args:
            decision: ê±°ë˜ ê²°ì • ë”•ì…”ë„ˆë¦¬
            market_data: ì‹œì¥ ë°ì´í„° (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            indicators: ì§€í‘œ ë°ì´í„° (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            
        Returns:
            íŠ¹ì„± ë²¡í„° (16ê°œ ì „ëµ scoreë§Œ)
        """
        features = []
        
        # ê°œë³„ ì „ëµì˜ score ê°’ë§Œ ì‚¬ìš©
        # ëª¨ë“  ì „ëµ ëª©ë¡ (STRATEGY_CATEGORIES ê¸°ë°˜)
        all_strategies = [
            # SHORT_TERM
            'vol_spike', 'orderflow_cvd', 'vpvr_micro', 
            'liquidity_grab', 'vwap_pinball', 'zscore_mean_reversion',
            # MEDIUM_TERM
            'multi_timeframe', 'htf_trend', 'bollinger_squeeze', 
            'support_resistance', 'ema_confluence',
            # LONG_TERM
            'oi_delta', 'vpvr', 'ichimoku', 'funding_rate'
        ]
        
        # ê° ì „ëµì˜ score ê°’ë§Œ ì¶”ì¶œ (í‰ë©´í™”ëœ ë°ì´í„°ì—ì„œ)
        for strategy_name in all_strategies:
            strategy_score_key = f"{strategy_name}_score"
            
            # decision ë”•ì…”ë„ˆë¦¬ì—ì„œ ì§ì ‘ ì°¾ê¸° (í‰ë©´í™”ëœ í˜•íƒœ)
            strategy_score = decision.get(strategy_score_key, 0.0)
            
            # scoreë§Œ íŠ¹ì„±ìœ¼ë¡œ ì¶”ê°€
            features.append(float(strategy_score) if strategy_score is not None else 0.0)
        
        return np.array(features, dtype=np.float32)
    
    def train(
        self,
        decisions_df: pd.DataFrame,
        price_data: pd.DataFrame,
        test_size: float = 0.2,
        min_profit_threshold: float = 0.005,
        use_profit_based: bool = True
    ) -> Dict[str, Any]:
        """
        ì‹ ê²½ë§ ëª¨ë¸ í•™ìŠµ
        
        Args:
            decisions_df: ê²°ì • ë°ì´í„°í”„ë ˆì„
            price_data: ê°€ê²© ë°ì´í„°í”„ë ˆì„
            test_size: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨
            min_profit_threshold: ìµœì†Œ ìˆ˜ìµë¥  ì„ê³„ê°’
            use_profit_based: ì‹¤ì œ ìˆ˜ìµë¥  ê¸°ë°˜ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            í•™ìŠµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        from engines.meta_labeling_engine import MetaLabelingEngine
        
        # ê¸°ì¡´ ì—”ì§„ì„ ì‚¬ìš©í•˜ì—¬ ë©”íƒ€ ë¼ë²¨ ìƒì„±
        temp_engine = MetaLabelingEngine()
        print("ğŸ“Š ë©”íƒ€ ë¼ë²¨ ìƒì„± ì¤‘...")
        labeled_df = temp_engine.create_meta_labels(
            decisions_df, price_data,
            min_profit_threshold=min_profit_threshold,
            use_profit_based=use_profit_based
        )
        
        # ê±°ë˜ê°€ ìˆëŠ” ê²°ì •ë§Œ í•„í„°ë§
        labeled_df = labeled_df[labeled_df['action'].isin(['LONG', 'SHORT'])]
        
        if len(labeled_df) < 100:
            return {"success": False, "message": f"í•™ìŠµ ë°ì´í„° ë¶€ì¡±: {len(labeled_df)}ê°œ (ìµœì†Œ 100ê°œ í•„ìš”)"}
        
        # íŠ¹ì„± ì¶”ì¶œ
        print("ğŸ” íŠ¹ì„± ì¶”ì¶œ ì¤‘...")
        X = []
        y = []
        
        for _, row in labeled_df.iterrows():
            try:
                decision_dict = row.to_dict()
                # indicators ì¶”ì¶œ (í‰ë©´í™”ëœ ë°ì´í„°ì—ì„œ)
                indicators = {}
                for key in decision_dict.keys():
                    if key.startswith('indicator_'):
                        indicator_name = key.replace('indicator_', '')
                        indicators[indicator_name] = decision_dict[key]
                
                features = self.extract_features(decision_dict, indicators=indicators)
                X.append(features)
                y.append(row['meta_label'])
            except Exception as e:
                print(f"âš ï¸ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨ (ê±´ë„ˆëœ€): {e}")
                continue
        
        if len(X) < 100:
            return {"success": False, "message": f"ìœ íš¨í•œ íŠ¹ì„± ë¶€ì¡±: {len(X)}ê°œ"}
        
        X = np.array(X)
        y = np.array(y)
        
        # íŠ¹ì„± ì´ë¦„ ì €ì¥ (ì „ëµ scoreë§Œ ì‚¬ìš©)
        all_strategies = [
            'vol_spike', 'orderflow_cvd', 'vpvr_micro', 
            'liquidity_grab', 'vwap_pinball', 'zscore_mean_reversion',
            'multi_timeframe', 'htf_trend', 'bollinger_squeeze', 
            'support_resistance', 'ema_confluence',
            'oi_delta', 'vpvr', 'ichimoku', 'funding_rate'
        ]
        
        # ì „ëµ scoreë§Œ ì‚¬ìš©
        self.feature_names = [f'{strategy_name}_score' for strategy_name in all_strategies]
        
        self.input_dim = X.shape[1]
        print(f"ğŸ“ ì…ë ¥ ì°¨ì›: {self.input_dim}ê°œ íŠ¹ì„±")
        
        # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
        unique, counts = np.unique(y, return_counts=True)
        class_dist = dict(zip(unique, counts))
        print(f"ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬: {class_dist}")
        
        if len(class_dist) == 2:
            success_count = class_dist.get(1, 0)
            fail_count = class_dist.get(0, 0)
            total = success_count + fail_count
            success_rate = success_count / total if total > 0 else 0
            ratio = min(class_dist.values()) / max(class_dist.values())
            
            print(f"   âœ… ì„±ê³µí•œ ê±°ë˜(1): {success_count:,}ê°œ ({success_rate:.1%})")
            print(f"   âŒ ì‹¤íŒ¨í•œ ê±°ë˜(0): {fail_count:,}ê°œ ({(1-success_rate):.1%})")
            
            if ratio < 0.3:
                print(f"âš ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜• ê°ì§€ (ë¹„ìœ¨: {ratio:.2f})")
        
        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        # ìŠ¤ì¼€ì¼ë§
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # MLPClassifier ëª¨ë¸ ì´ˆê¸°í™”
        # alphaëŠ” L2 ì •ê·œí™” (dropout ëŒ€ì‹  ì‚¬ìš©)
        # learning_rate_initëŠ” í•™ìŠµë¥ 
        self.model = MLPClassifier(
            hidden_layer_sizes=self.hidden_layer_sizes,
            activation='relu',
            solver='adam',
            alpha=self.dropout * 0.01,  # dropoutì„ alphaë¡œ ë³€í™˜
            batch_size=min(200, len(X_train)),  # ë°°ì¹˜ í¬ê¸°
            learning_rate_init=self.learning_rate,
            max_iter=self.max_iter,
            random_state=42,
            early_stopping=True,
            validation_fraction=0.1,
            n_iter_no_change=10,
            verbose=True
        )
        
        # í•™ìŠµ
        print(f"ğŸ“ ëª¨ë¸ í•™ìŠµ ì¤‘... ({len(X_train)}ê°œ ìƒ˜í”Œ, ìµœëŒ€ {self.max_iter} ë°˜ë³µ)")
        self.model.fit(X_train_scaled, y_train)
        
        # ìµœì¢… í‰ê°€
        y_pred = self.model.predict(X_test_scaled)
        y_pred_proba = self.model.predict_proba(X_test_scaled)[:, 1]
        
        accuracy = np.mean(y_pred == y_test)
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        
        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)
        precision = report.get('1', {}).get('precision', 0.0)
        recall = report.get('1', {}).get('recall', 0.0)
        
        print(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
        print(f"   ì •í™•ë„: {accuracy:.3f}")
        print(f"   ROC-AUC: {roc_auc:.3f}")
        print(f"   Precision: {precision:.3f}")
        print(f"   Recall: {recall:.3f}")
        
        self.is_trained = True
        
        # ëª¨ë¸ ì €ì¥
        self.save_model()
        
        return {
            "success": True,
            "accuracy": accuracy,
            "roc_auc": roc_auc,
            "precision": precision,
            "recall": recall,
            "train_samples": len(X_train),
            "test_samples": len(X_test),
            "input_dim": self.input_dim
        }
    
    def predict(
        self,
        decision: Dict[str, Any],
        market_data: Optional[Dict[str, Any]] = None,
        indicators: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ê±°ë˜ ì‹¤í–‰ ì—¬ë¶€ ì˜ˆì¸¡
        
        Args:
            decision: ê±°ë˜ ê²°ì • ë”•ì…”ë„ˆë¦¬
            market_data: ì‹œì¥ ë°ì´í„°
            indicators: ì§€í‘œ ë°ì´í„°
            
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.is_trained or self.model is None:
            return self._default_prediction(decision)
        
        try:
            # íŠ¹ì„± ì¶”ì¶œ
            features = self.extract_features(decision, market_data, indicators)
            
            # feature ê°œìˆ˜ í™•ì¸ ë° ì¡°ì •
            expected_features = self.scaler.n_features_in_ if hasattr(self.scaler, 'n_features_in_') else len(self.feature_names) if self.feature_names else 16
            actual_features = len(features)
            
            if actual_features != expected_features:
                print(f"âš ï¸ Feature ê°œìˆ˜ ë¶ˆì¼ì¹˜: ì˜ˆìƒ {expected_features}ê°œ, ì‹¤ì œ {actual_features}ê°œ")
                # feature ê°œìˆ˜ë¥¼ ë§ì¶°ì£¼ê¸° (ë¶€ì¡±í•˜ë©´ 0ìœ¼ë¡œ ì±„ìš°ê¸°, ë§ìœ¼ë©´ ìë¥´ê¸°)
                if actual_features < expected_features:
                    # ë¶€ì¡±í•œ featureë¥¼ 0ìœ¼ë¡œ ì±„ìš°ê¸°
                    features = np.pad(features, (0, expected_features - actual_features), 'constant', constant_values=0.0)
                    print(f"   â†’ ë¶€ì¡±í•œ {expected_features - actual_features}ê°œ featureë¥¼ 0ìœ¼ë¡œ ì±„ì› ìŠµë‹ˆë‹¤.")
                else:
                    # ë§ì€ featureë¥¼ ìë¥´ê¸°
                    features = features[:expected_features]
                    print(f"   â†’ ì´ˆê³¼í•œ {actual_features - expected_features}ê°œ featureë¥¼ ì œê±°í–ˆìŠµë‹ˆë‹¤.")
            
            features_scaled = self.scaler.transform([features])
            
            # ì˜ˆì¸¡
            probability = self.model.predict_proba(features_scaled)[0][1]
            prediction = self.model.predict(features_scaled)[0]
            
            should_execute = probability >= (self.confidence_threshold * 0.9)
            
            return {
                "should_execute": should_execute,
                "prediction": int(prediction),
                "probability": float(probability),
                "confidence": "HIGH" if probability >= 0.7 else "MEDIUM" if probability >= 0.5 else "LOW"
            }
        except Exception as e:
            print(f"âš ï¸ ë©”íƒ€ ë¼ë²¨ë§ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return self._default_prediction(decision)
    
    def _default_prediction(self, decision: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ë³¸ ì˜ˆì¸¡ ë¡œì§"""
        net_score = decision.get("net_score", 0.0)
        meta = decision.get("meta", {})
        synergy_meta = meta.get("synergy_meta", {})
        confidence = synergy_meta.get("confidence", "LOW")
        
        confidence_map = {"HIGH": 0.8, "MEDIUM": 0.5, "LOW": 0.2}
        confidence_value = confidence_map.get(confidence, 0.2)
        
        should_execute = (
            abs(net_score) > 0.2 and
            confidence_value >= 0.3
        )
        
        return {
            "should_execute": should_execute,
            "prediction": 1 if should_execute else 0,
            "probability": confidence_value * abs(net_score),
            "confidence": confidence,
            "note": "ê¸°ë³¸ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš© (ëª¨ë¸ ë¯¸í•™ìŠµ)"
        }
    
    def save_model(self):
        """ëª¨ë¸ ì €ì¥"""
        if self.model is None:
            return
        
        Path(self.MODEL_PATH).parent.mkdir(parents=True, exist_ok=True)
        
        # scikit-learn ëª¨ë¸ì€ pickleë¡œ ì €ì¥
        with open(self.MODEL_PATH, 'wb') as f:
            pickle.dump({
                'model': self.model,
                'input_dim': self.input_dim,
                'hidden_layer_sizes': self.hidden_layer_sizes,
                'dropout': self.dropout,
                'is_trained': self.is_trained
            }, f)
        
        with open(self.SCALER_PATH, 'wb') as f:
            pickle.dump(self.scaler, f)
        
        with open(self.FEATURE_NAMES_PATH, 'wb') as f:
            pickle.dump(self.feature_names, f)
        
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {self.MODEL_PATH}")
    
    def load_model(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ"""
        model_path = Path(self.MODEL_PATH)
        scaler_path = Path(self.SCALER_PATH)
        feature_names_path = Path(self.FEATURE_NAMES_PATH)
        
        # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not model_path.exists():
            # ì´ì „ ê²½ë¡œë„ ì‹œë„
            old_model_path = Path("engines/meta_labeling_nn_model.pkl")
            if old_model_path.exists():
                self.MODEL_PATH = str(old_model_path)
                model_path = old_model_path
            else:
                print(f"âš ï¸ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œë„í•œ ê²½ë¡œ: ['{self.MODEL_PATH}', 'engines/meta_labeling_nn_model.pkl']")
                return False
        
        # scaler íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not scaler_path.exists():
            old_scaler_path = Path("engines/meta_labeling_nn_scaler.pkl")
            if old_scaler_path.exists():
                self.SCALER_PATH = str(old_scaler_path)
                scaler_path = old_scaler_path
            else:
                print(f"âš ï¸ Scaler íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œë„í•œ ê²½ë¡œ: ['{self.SCALER_PATH}', 'engines/meta_labeling_nn_scaler.pkl']")
                return False
        
        # feature_names íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not feature_names_path.exists():
            old_feature_names_path = Path("engines/meta_labeling_nn_feature_names.pkl")
            if old_feature_names_path.exists():
                self.FEATURE_NAMES_PATH = str(old_feature_names_path)
                feature_names_path = old_feature_names_path
            else:
                print(f"âš ï¸ Feature names íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œë„í•œ ê²½ë¡œ: ['{self.FEATURE_NAMES_PATH}', 'engines/meta_labeling_nn_feature_names.pkl']")
                return False
        
        try:
            # ëª¨ë¸ íŒŒì¼ ë¡œë“œ
            with open(self.MODEL_PATH, 'rb') as f:
                checkpoint = pickle.load(f)
            
            # checkpointê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
            if not isinstance(checkpoint, dict):
                print(f"âŒ ëª¨ë¸ íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜: ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤.")
                return False
            
            # í•„ìˆ˜ í‚¤ í™•ì¸
            required_keys = ['model', 'input_dim', 'hidden_layer_sizes', 'dropout', 'is_trained']
            missing_keys = [key for key in required_keys if key not in checkpoint]
            if missing_keys:
                print(f"âŒ ëª¨ë¸ íŒŒì¼ì— í•„ìˆ˜ í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: {missing_keys}")
                return False
            
            self.model = checkpoint.get('model')
            self.input_dim = checkpoint.get('input_dim')
            self.hidden_layer_sizes = checkpoint.get('hidden_layer_sizes')
            self.dropout = checkpoint.get('dropout')
            self.is_trained = checkpoint.get('is_trained')
            
            # scaler ë¡œë“œ (ë³„ë„ íŒŒì¼ ë˜ëŠ” checkpointì—ì„œ)
            if 'scaler' in checkpoint:
                # ì˜¤ë˜ëœ í˜•ì‹: checkpointì— scaler í¬í•¨
                self.scaler = checkpoint['scaler']
            else:
                # ìƒˆë¡œìš´ í˜•ì‹: ë³„ë„ íŒŒì¼ì—ì„œ ë¡œë“œ
                try:
                    with open(self.SCALER_PATH, 'rb') as f:
                        self.scaler = pickle.load(f)
                except FileNotFoundError:
                    print(f"âš ï¸ Scaler íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.SCALER_PATH}")
                    return False
            
            # feature_names ë¡œë“œ (ë³„ë„ íŒŒì¼ ë˜ëŠ” checkpointì—ì„œ)
            if 'feature_names' in checkpoint:
                # ì˜¤ë˜ëœ í˜•ì‹: checkpointì— feature_names í¬í•¨
                self.feature_names = checkpoint['feature_names']
            else:
                # ìƒˆë¡œìš´ í˜•ì‹: ë³„ë„ íŒŒì¼ì—ì„œ ë¡œë“œ
                try:
                    with open(self.FEATURE_NAMES_PATH, 'rb') as f:
                        self.feature_names = pickle.load(f)
                except FileNotFoundError:
                    print(f"âš ï¸ Feature names íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.FEATURE_NAMES_PATH}")
                    return False
            
            print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.MODEL_PATH}")
            return True
        except KeyError as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ({self.MODEL_PATH}): {e}")
            print("   ëª¨ë¸ íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ì¬í•™ìŠµí•˜ì„¸ìš”.")
            return False
        except FileNotFoundError as e:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            return False
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False
