# ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ
import pandas as pd
from datetime import timedelta
import os
import sys
import pickle
from typing import Dict, Any, List, Optional
import psutil

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from managers.strategy_executor import StrategyExecutor
from managers.data_manager import get_data_manager
from engines.trade_decision_engine import TradeDecisionEngine
from indicators.global_indicators import get_all_indicators, get_global_indicator_manager
from utils.data_flattener import flatten_decision_data


def safe_concat(existing_df, new_df):
    if existing_df is None or existing_df.empty:
        return new_df.copy() if not new_df.empty else pd.DataFrame()
    elif new_df.empty:
        return existing_df.copy()
    else:
        # ì»¬ëŸ¼ ì¼ì¹˜ í™•ì¸
        if list(existing_df.columns) != list(new_df.columns):
            # ê³µí†µ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
            common_cols = list(set(existing_df.columns) & set(new_df.columns))
            if common_cols:
                existing_df = existing_df[common_cols]
                new_df = new_df[common_cols]
            else:
                return existing_df.copy()
        
        # FutureWarning ë°©ì§€: ë¹ˆ DataFrameì´ë‚˜ ëª¨ë“  NAì¸ ì»¬ëŸ¼ ì²˜ë¦¬
        if existing_df.empty or new_df.empty:
            return existing_df if not existing_df.empty else new_df
        
        # ëª¨ë“  NAì¸ ì»¬ëŸ¼ ì œê±°
        existing_df_clean = existing_df.dropna(axis=1, how='all')
        new_df_clean = new_df.dropna(axis=1, how='all')
        
        # ê³µí†µ ì»¬ëŸ¼ë§Œ ìœ ì§€
        common_cols = list(set(existing_df_clean.columns) & set(new_df_clean.columns))
        if not common_cols:
            return existing_df_clean if not existing_df_clean.empty else new_df_clean
        
        existing_df_clean = existing_df_clean[common_cols]
        new_df_clean = new_df_clean[common_cols]
        
        return pd.concat([existing_df_clean, new_df_clean], ignore_index=True)

def save_decisions_to_parquet(
    decision_data_list: List[Dict[str, Any]], 
    filename: str = "agent/decisions_data.parquet",
    append: bool = True
):
    """Decision ë°ì´í„°ë¥¼ Parquet íŒŒì¼ë¡œ ì €ì¥"""
    try:
        if not decision_data_list:
            print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ë°ì´í„° í‰ë©´í™”
        flattened_data = [flatten_decision_data(decision) for decision in decision_data_list]
        new_df = pd.DataFrame(flattened_data)
        
        # timestampë¥¼ datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        if 'timestamp' in new_df.columns:
            new_df['timestamp'] = pd.to_datetime(new_df['timestamp'])
        
        # ê¸°ì¡´ íŒŒì¼ì´ ìˆê³  append ëª¨ë“œì¸ ê²½ìš°
        if append and os.path.exists(filename):
            try:
                existing_df = pd.read_parquet(filename)
                
                # ì¤‘ë³µ ì œê±° (timestamp ê¸°ì¤€)
                if 'timestamp' in existing_df.columns and 'timestamp' in new_df.columns:
                    # ê¸°ì¡´ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ timestamp ì´í›„ ë°ì´í„°ë§Œ ì¶”ê°€
                    last_timestamp = existing_df['timestamp'].max()
                    new_df = new_df[new_df['timestamp'] > last_timestamp]
                
                if not new_df.empty:
                    # ì»¬ëŸ¼ ìˆœì„œ ë§ì¶”ê¸°
                    common_columns = list(set(existing_df.columns) & set(new_df.columns))
                    new_columns = [col for col in new_df.columns if col not in existing_df.columns]
                    
                    # ê¸°ì¡´ DataFrameì— ìƒˆ ì»¬ëŸ¼ ì¶”ê°€ (NaNìœ¼ë¡œ ì±„ì›Œì§)
                    for col in new_columns:
                        existing_df[col] = None
                    
                    # ìƒˆ DataFrameì— ê¸°ì¡´ ì»¬ëŸ¼ ì¶”ê°€ (NaNìœ¼ë¡œ ì±„ì›Œì§)
                    for col in existing_df.columns:
                        if col not in new_df.columns:
                            new_df[col] = None
                    
                    # ì»¬ëŸ¼ ìˆœì„œ ë§ì¶”ê¸°
                    all_columns = list(existing_df.columns)
                    new_df = new_df.reindex(columns=all_columns)
                    
                    # ë°ì´í„° í•©ì¹˜ê¸°
                    combined_df = safe_concat(existing_df, new_df)

                else:
                    combined_df = existing_df
                    print("ìƒˆë¡œ ì¶”ê°€í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (ì¤‘ë³µ ì œê±°ë¨)")
            except Exception as e:
                print(f"ê¸°ì¡´ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨, ìƒˆ íŒŒì¼ë¡œ ì €ì¥: {e}")
                combined_df = new_df
        else:
            combined_df = new_df
        
        # Parquet íŒŒì¼ë¡œ ì €ì¥ (ìµœì í™”ëœ ì••ì¶• ë° ì„¤ì •)
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        combined_df.to_parquet(
            filename, 
            compression='snappy', 
            index=False,
            engine='pyarrow',  # pyarrow ì—”ì§„ ì‚¬ìš© (ë” ë¹ ë¦„)
            use_deprecated_int96_timestamps=False  # ìµœì‹  íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ ì‚¬ìš©
        )
        
        print(f"Decision ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename} ({len(combined_df)}ê°œ ë ˆì½”ë“œ)")
        print(f"íŒŒì¼ í¬ê¸°: {os.path.getsize(filename) / 1024 / 1024:.2f} MB")
        return True
        
    except Exception as e:
        print(f"Parquet ì €ì¥ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False

def load_decisions_from_parquet(filename: str = "agent/decisions_data.parquet") -> Optional[pd.DataFrame]:
    """Parquet íŒŒì¼ì—ì„œ Decision ë°ì´í„° ë¡œë“œ (ë©”ëª¨ë¦¬ ìµœì í™”)"""
    try:
        if not os.path.exists(filename):
            print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
            return None
            
        # ë©”ëª¨ë¦¬ ë§¤í•‘ì„ ì‚¬ìš©í•œ ìµœì í™”ëœ ë¡œë”©
        df = pd.read_parquet(
            filename,
            engine='pyarrow',  # pyarrow ì—”ì§„ ì‚¬ìš©
            memory_map=True    # ë©”ëª¨ë¦¬ ë§¤í•‘ í™œì„±í™”
        )
        print(f"Decision ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {filename} ({len(df)}ê°œ ë ˆì½”ë“œ)")
        print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f}MB")
        return df
        
    except Exception as e:
        print(f"Parquet ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

def inspect_parquet_structure(filename: str = "agent/decisions_data.parquet") -> None:
    """Parquet íŒŒì¼ì˜ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
    try:
        if not os.path.exists(filename):
            print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
            return
            
        df = pd.read_parquet(filename)
        print(f"\n=== Parquet íŒŒì¼ êµ¬ì¡° ë¶„ì„ ===")
        print(f"íŒŒì¼: {filename}")
        print(f"ì´ ë ˆì½”ë“œ ìˆ˜: {len(df)}")
        print(f"ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
        print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
        
        print(f"\nì»¬ëŸ¼ ëª©ë¡:")
        for i, col in enumerate(df.columns):
            non_null_count = df[col].count()
            print(f"  {i+1:2d}. {col:<30} (non-null: {non_null_count}/{len(df)})")
        
        print(f"\nì²« ë²ˆì§¸ ë ˆì½”ë“œ ìƒ˜í”Œ:")
        if len(df) > 0:
            sample_record = df.iloc[0].to_dict()
            for key, value in list(sample_record.items())[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
                print(f"  {key}: {value}")
            if len(sample_record) > 10:
                print(f"  ... (ì´ {len(sample_record)}ê°œ í•„ë“œ)")
        
        print(f"\në°ì´í„° íƒ€ì…:")
        print(df.dtypes)
        
    except Exception as e:
        print(f"Parquet êµ¬ì¡° ë¶„ì„ ì˜¤ë¥˜: {e}")

def convert_parquet_to_signal_data(
    df: pd.DataFrame, 
    max_samples: Optional[int] = None,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
) -> List[Dict[str, Any]]:
    """DataFrameì„ signal_data ë¦¬ìŠ¤íŠ¸ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ë³€í™˜
    
    Args:
        df: ë³€í™˜í•  DataFrame (ì´ë¯¸ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥ë¨)
        max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´ ì‚¬ìš©)
        start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
    """
    try:
        # ë‚ ì§œ í•„í„°ë§
        if start_date or end_date:
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                if start_date:
                    df = df[df['timestamp'] >= start_date]
                if end_date:
                    df = df[df['timestamp'] <= end_date]
                print(f"ë‚ ì§œ í•„í„°ë§ í›„: {len(df)}ê°œ ë ˆì½”ë“œ")
        
        # ìƒ˜í”Œ ìˆ˜ ì œí•œ
        if max_samples and len(df) > max_samples:
            # ìµœê·¼ ë°ì´í„°ë¶€í„° ìƒ˜í”Œë§
            df = df.tail(max_samples)
            print(f"ìƒ˜í”Œë§ í›„: {len(df)}ê°œ ë ˆì½”ë“œ")
        
        # NaN ê°’ì„ Noneìœ¼ë¡œ ë³€í™˜í•˜ê³  ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        df_clean = df.where(pd.notnull(df), None)
        signal_data = df_clean.to_dict('records')
        
        print(f"Parquet ë°ì´í„°ë¥¼ signal_dataë¡œ ë³€í™˜ ì™„ë£Œ: {len(signal_data)}ê°œ ë ˆì½”ë“œ")
        return signal_data
        
    except Exception as e:
        print(f"signal_data ë³€í™˜ ì˜¤ë¥˜: {e}")
        return []

def load_signal_data_directly(
    filename: str = "agent/decisions_data.parquet",
    max_samples: Optional[int] = 5000,  # ê¸°ë³¸ê°’ 5000ê°œë¡œ ì œí•œ
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
) -> List[Dict[str, Any]]:
    """Parquet íŒŒì¼ì—ì„œ ì§ì ‘ signal_dataë¥¼ ë¡œë“œí•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜"""
    try:
        # parquet íŒŒì¼ ë¡œë“œ
        df = pd.read_parquet(filename)
        
        # ë‚ ì§œ í•„í„°ë§
        if start_date or end_date:
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                if start_date:
                    df = df[df['timestamp'] >= start_date]
                if end_date:
                    df = df[df['timestamp'] <= end_date]
        
        # ìƒ˜í”Œ ìˆ˜ ì œí•œ
        if max_samples and len(df) > max_samples:
            df = df.tail(max_samples)
        
        # NaNì„ Noneìœ¼ë¡œ ë³€í™˜í•˜ê³  ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        signal_data = df.where(pd.notnull(df), None).to_dict('records')
        
        print(f"Parquetì—ì„œ signal_data ì§ì ‘ ë¡œë“œ ì™„ë£Œ: {len(signal_data)}ê°œ ë ˆì½”ë“œ")
        return signal_data
        
    except Exception as e:
        print(f"signal_data ì§ì ‘ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return []

def save_progress_state(current_index: int, total_count: int, filename: str = "agent/progress_state.pkl"):
    """ì§„í–‰ ìƒíƒœ ì €ì¥"""
    try:
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        state = {
            'current_index': current_index,
            'total_count': total_count,
            'timestamp': pd.Timestamp.now()
        }
        with open(filename, 'wb') as f:
            pickle.dump(state, f)
        print(f"ì§„í–‰ ìƒíƒœ ì €ì¥: ì¸ë±ìŠ¤ {current_index} (ë‹¤ìŒ ì¬ì‹œì‘ ì‹œ {current_index + 1}ë¶€í„°)")
    except Exception as e:
        print(f"ì§„í–‰ ìƒíƒœ ì €ì¥ ì˜¤ë¥˜: {e}")

def load_progress_state(filename: str = "agent/progress_state.pkl") -> Optional[Dict[str, Any]]:
    """ì§„í–‰ ìƒíƒœ ë¡œë“œ"""
    try:
        if not os.path.exists(filename):
            print("ì§„í–‰ ìƒíƒœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
            return None
        
        with open(filename, 'rb') as f:
            state = pickle.load(f)
        
        print(f"ì§„í–‰ ìƒíƒœ ë³µì›: ì¸ë±ìŠ¤ {state['current_index']}/{state['total_count']} "
              f"(ì €ì¥ ì‹œê°„: {state['timestamp']})")
        print(f"ë‹¤ìŒ ì¬ì‹œì‘ ì‹œ ì¸ë±ìŠ¤ {state['current_index'] + 1}ë¶€í„° ì‹œì‘ë©ë‹ˆë‹¤.")
        return state
    except Exception as e:
        print(f"ì§„í–‰ ìƒíƒœ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

def clear_progress_state(filename: str = "agent/progress_state.pkl"):
    """ì§„í–‰ ìƒíƒœ íŒŒì¼ ì‚­ì œ"""
    try:
        if os.path.exists(filename):
            os.remove(filename)
            print("ì§„í–‰ ìƒíƒœ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
    except Exception as e:
        print(f"ì§„í–‰ ìƒíƒœ íŒŒì¼ ì‚­ì œ ì˜¤ë¥˜: {e}")

def load_ethusdc_data():
    """ETHUSDC CSV ë°ì´í„° ë¡œë“œ - 3ë¶„, 15ë¶„, 1ì‹œê°„ë´‰ (ë©”ëª¨ë¦¬ ìµœì í™”)"""
    try:
        required_columns = ['open', 'high', 'low', 'close', 'volume', 'quote_volume']
        
        # ë°ì´í„° íƒ€ì… ìµœì í™”ë¥¼ ìœ„í•œ dtype ì„¤ì •
        dtype_optimized = {
            'open': 'float32',
            'high': 'float32', 
            'low': 'float32',
            'close': 'float32',
            'volume': 'float32',
            'quote_volume': 'float32'
        }
        
        # 3ë¶„ë´‰ ë°ì´í„° ë¡œë“œ (ë©”ëª¨ë¦¬ ìµœì í™”)
        df_3m = pd.read_csv('data/ETHUSDC_3m_historical_data.csv', dtype=dtype_optimized)
        df_3m['timestamp'] = pd.to_datetime(df_3m['timestamp'])
        df_3m = df_3m.set_index('timestamp')
        df_3m = df_3m[required_columns]

        # 15ë¶„ë´‰ ë°ì´í„° ë¡œë“œ (ë©”ëª¨ë¦¬ ìµœì í™”)
        df_15m = pd.read_csv('data/ETHUSDC_15m_historical_data.csv', dtype=dtype_optimized)
        df_15m['timestamp'] = pd.to_datetime(df_15m['timestamp'])
        df_15m = df_15m.set_index('timestamp')
        df_15m = df_15m[required_columns]

        # 1ì‹œê°„ë´‰ ë°ì´í„° ë¡œë“œ (ë©”ëª¨ë¦¬ ìµœì í™”)
        df_1h = pd.read_csv('data/ETHUSDC_1h_historical_data.csv', dtype=dtype_optimized)
        df_1h['timestamp'] = pd.to_datetime(df_1h['timestamp'])
        df_1h = df_1h.set_index('timestamp')
        df_1h = df_1h[required_columns]

        print(f"ETHUSDC 3ë¶„ë´‰ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_3m)}ê°œ ìº”ë“¤ (ë©”ëª¨ë¦¬ ìµœì í™”ë¨)")
        print(f"ETHUSDC 15ë¶„ë´‰ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(df_15m)}ê°œ ìº”ë“¤ (ë©”ëª¨ë¦¬ ìµœì í™”ë¨)")
        print(f"ETHUSDC 1ì‹œê°„ë´‰ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(df_1h)}ê°œ ìº”ë“¤ (ë©”ëª¨ë¦¬ ìµœì í™”ë¨)")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
        memory_usage_3m = df_3m.memory_usage(deep=True).sum() / 1024 / 1024
        memory_usage_15m = df_15m.memory_usage(deep=True).sum() / 1024 / 1024
        memory_usage_1h = df_1h.memory_usage(deep=True).sum() / 1024 / 1024
        print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ - 3ë¶„ë´‰: {memory_usage_3m:.2f}MB, 15ë¶„ë´‰: {memory_usage_15m:.2f}MB, 1ì‹œê°„ë´‰: {memory_usage_1h:.2f}MB")
        
        return df_3m, df_15m, df_1h

    except FileNotFoundError as e:
        print(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return None, None, None
    except Exception as e:
        print(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None, None

def generate_signal_data_with_indicators(
    price_data: pd.DataFrame, 
    price_data_15m: pd.DataFrame, 
    price_data_1h: pd.DataFrame,
    resume_from_progress: bool = True
):
    """CSV ë°ì´í„°ë¡œ ì‹¤ì œ ì§€í‘œ ì—…ë°ì´íŠ¸ ë° ì „ëµ ì‹¤í–‰ (ì¤‘ë‹¨ì  ì¬ì‹œì‘ ì§€ì›)"""
    # ì§„í–‰ ìƒíƒœ í™•ì¸
    progress_state = None
    start_idx = None
    
    if resume_from_progress:
        progress_state = load_progress_state()
    
    # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    data_manager = get_data_manager()
    
    print("CSV ë°ì´í„°ë¡œ ì§€í‘œ ì—…ë°ì´íŠ¸ ë° ì „ëµ ì‹¤í–‰ ì¤‘...")
    print(f"   - 3ë¶„ë´‰: {len(price_data)}ê°œ ìº”ë“¤")
    print(f"   - 15ë¶„ë´‰: {len(price_data_15m)}ê°œ ìº”ë“¤")
    print(f"   - 1ì‹œê°„ë´‰: {len(price_data_1h)}ê°œ ìº”ë“¤")
    
    # ì‹œì‘ ìœ„ì¹˜ ê²°ì •
    if progress_state:
        # ì €ì¥ëœ ì¸ë±ìŠ¤ëŠ” ì´ë¯¸ ì²˜ë¦¬ëœ ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ì´ë¯€ë¡œ +1ë¶€í„° ì‹œì‘
        start_idx = progress_state['current_index'] + 1
        print(f"ì´ì „ ì§„í–‰ ìƒíƒœì—ì„œ ì¬ì‹œì‘: {start_idx}ë²ˆì§¸ ìº”ë“¤ë¶€í„° (ì €ì¥ëœ ìœ„ì¹˜: {progress_state['current_index']})")
    else:
        # ìµœê·¼ ë°ì´í„°ë¶€í„° ì²˜ë¦¬ (ìµœëŒ€ max_periodsê°œ)
        target_datetime = price_data.iloc[0].name + timedelta(days=4)
        start_idx = price_data.index.get_loc(target_datetime)
        print(f"ê¸°ì¤€ ë‚ ì§œ {target_datetime}ì˜ ì¸ë±ìŠ¤ ìœ„ì¹˜: {start_idx}")
    
    # ì´ˆê¸° ë°ì´í„° ë¡œë”©
    target_time = price_data.index[start_idx]
    
    # ë°ì´í„°ëŠ” target_time ì´ì „ê¹Œì§€ ê°€ì ¸ì˜¤ê¸°
    data_manager.load_initial_data(
        symbol='ETHUSDC', 
        df_3m=price_data[price_data.index < target_time], 
        df_15m=price_data_15m[price_data_15m.index < target_time], 
        df_1h=price_data_1h[price_data_1h.index < target_time]
    ) 
    
    # indicatorëŠ” ì‹¤ì œ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ì‹œì ìœ¼ë¡œ ì´ˆê¸°í™” (target_time ì´ì „)
    last_data_time = price_data[price_data.index < target_time].index[-1]
    global_manager = get_global_indicator_manager(last_data_time)
    global_manager.initialize_indicators()

    strategy_executor = StrategyExecutor()
    decision_engine = TradeDecisionEngine()

    end_idx = len(price_data)
    batch_size = 50000  # 50,000ê°œì”© ë°°ì¹˜ë¡œ ì €ì¥ (Parquet ìµœì í™”)
    temp_decision_data = []  # ì„ì‹œ ì €ì¥ìš©
    
    try:
        for i in range(start_idx, end_idx):
            # í˜„ì¬ ìº”ë“¤ ë°ì´í„°
            series_3m = price_data.iloc[i]
            current_time = price_data.index[i]
            
            # ë°ì´í„° ë§¤ë‹ˆì €ì— ìº”ë“¤ ë°ì´í„° ì—…ë°ì´íŠ¸
            data_manager.update_with_candle(series_3m)

            # 15ë¶„ë´‰ ë§ˆê° ì‹œê°„ ì²´í¬ (15ë¶„ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì§€ëŠ” ì‹œê°„)
            if current_time.minute % 15 == 0:
                try:
                    series_15m = price_data_15m.loc[current_time]
                    data_manager.update_with_candle_15m(series_15m)
                except KeyError:
                    pass  # í•´ë‹¹ ì‹œê°„ì˜ 15ë¶„ë´‰ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            
            # 1ì‹œê°„ë´‰ ë§ˆê° ì‹œê°„ ì²´í¬ (ì •ì‹œ)
            if current_time.minute == 0:
                try:
                    series_1h = price_data_1h.loc[current_time]
                    data_manager.update_with_candle_1h(series_1h)
                except KeyError:
                    pass  # í•´ë‹¹ ì‹œê°„ì˜ 1ì‹œê°„ë´‰ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                    
            # ê¸€ë¡œë²Œ ì§€í‘œ ì—…ë°ì´íŠ¸
            global_manager.update_all_indicators(series_3m)
            indicators = get_all_indicators()
            # ì „ëµ ì‹¤í–‰
            strategy_executor.execute_all_strategies()
            
            # ì‹ í˜¸ ìˆ˜ì§‘
            signals = strategy_executor.get_signals()
            
            # ê±°ë˜ ê²°ì •
            decision = decision_engine.decide_trade_realtime(signals)
            decision.update({'timestamp': current_time, 'indicators': indicators, **series_3m.to_dict()})

            temp_decision_data.append(decision)
            
            # ë°°ì¹˜ í¬ê¸°ë§ˆë‹¤ Parquet íŒŒì¼ì— ì €ì¥
            if len(temp_decision_data) >= batch_size:
                save_decisions_to_parquet(temp_decision_data)
                temp_decision_data = []  # ì„ì‹œ ë°ì´í„° ì´ˆê¸°í™”
            
            # ì§„í–‰ ìƒíƒœ ì €ì¥ ë° ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ (5000ê°œë§ˆë‹¤)
            if (i - start_idx) % 5000 == 0:
                save_progress_state(i, end_idx)
                total_periods = end_idx - start_idx
                processed = i - start_idx + 1
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
                import psutil
                memory_usage = psutil.Process().memory_info().rss / 1024 / 1024  # MB
                temp_data_size = len(temp_decision_data)
                
                print(f"   ì§„í–‰ë¥ : {processed}/{total_periods} ({processed / total_periods * 100:.1f}%) - ì¸ë±ìŠ¤ {i} ì €ì¥ë¨")
                print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.1f}MB, ì„ì‹œ ë°ì´í„°: {temp_data_size}ê°œ")
        
        # ë‚¨ì€ decision ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì €ì¥
        if temp_decision_data:
            save_decisions_to_parquet(temp_decision_data)
        
        # ì™„ë£Œ í›„ ì§„í–‰ ìƒíƒœ íŒŒì¼ ì‚­ì œ
        clear_progress_state()
        
        total_processed = end_idx - start_idx
        print(f"ì‹ í˜¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: {total_processed}ê°œ")
        
        return True
        
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # ë‚¨ì€ ë°ì´í„° ì €ì¥
        if temp_decision_data:
            save_decisions_to_parquet(temp_decision_data)
            print("ì¤‘ë‹¨ ì „ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        
        # ì§„í–‰ ìƒíƒœ ì €ì¥ (ì‹¤ì œ ì²˜ë¦¬ ì™„ë£Œëœ ë§ˆì§€ë§‰ ì¸ë±ìŠ¤)
        save_progress_state(i-1, end_idx)
        print("ë‹¤ìŒì— '--resume' ì˜µì…˜ìœ¼ë¡œ ì¬ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return False
        
    except Exception as e:
        print(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ë‚¨ì€ ë°ì´í„° ì €ì¥
        if temp_decision_data:
            save_decisions_to_parquet(temp_decision_data)
        
        # ì§„í–‰ ìƒíƒœ ì €ì¥ (ì‹¤ì œ ì²˜ë¦¬ ì™„ë£Œëœ ë§ˆì§€ë§‰ ì¸ë±ìŠ¤)
        if 'i' in locals():
            save_progress_state(i-1, end_idx)
        
        import traceback
        traceback.print_exc()
        return False

def check_existing_decision_data() -> bool:
    """ê¸°ì¡´ decision_dataê°€ ìˆëŠ”ì§€ í™•ì¸"""
    try:
        df = load_decisions_from_parquet()
        if df is not None and not df.empty:
            print(f"âœ… ê¸°ì¡´ Decision ë°ì´í„° ë°œê²¬: {len(df)}ê°œ ë ˆì½”ë“œ")
            print(f"   - ì‹œê°„ ë²”ìœ„: {df['timestamp'].min()} ~ {df['timestamp'].max()}")
            print(f"   - ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
            return True
        else:
            print("âŒ ê¸°ì¡´ Decision ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
    except Exception as e:
        print(f"âŒ Decision ë°ì´í„° í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def analyze_decision_data(df: pd.DataFrame) -> None:
    """Decision ë°ì´í„° ë¶„ì„ ë° í†µê³„ ì¶œë ¥"""
    print("\n=== Decision ë°ì´í„° ë¶„ì„ ===")
    
    if df.empty:
        print("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ì´ ë ˆì½”ë“œ ìˆ˜: {len(df)}")
    print(f"ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
    print(f"ì‹œê°„ ë²”ìœ„: {df['timestamp'].min()} ~ {df['timestamp'].max()}")
    
    # ê° ì¹´í…Œê³ ë¦¬ë³„ ì•¡ì…˜ ë¶„í¬ ë¶„ì„ (ìˆ˜ì¹˜ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜)
    categories = ['short_term', 'medium_term', 'long_term']
    action_mapping = {1.0: 'LONG', -1.0: 'SHORT', 0.0: 'HOLD'}
    
    for category in categories:
        action_col = f'{category}_action'
        if action_col in df.columns:
            # ìˆ˜ì¹˜ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            action_values = df[action_col].map(action_mapping).fillna('UNKNOWN')
            action_counts = action_values.value_counts()
            print(f"\n{category.upper()} ì•¡ì…˜ ë¶„í¬:")
            for action, count in action_counts.items():
                percentage = (count / len(df)) * 100
                print(f"  {action}: {count}ê°œ ({percentage:.1f}%)")
    
    # ì‹ ë¢°ë„ ë¶„í¬ ë¶„ì„ (ìˆ˜ì¹˜ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜)
    confidence_mapping = {0.8: 'HIGH', 0.5: 'MEDIUM', 0.2: 'LOW'}
    
    for category in categories:
        confidence_col = f'{category}_confidence'
        if confidence_col in df.columns:
            # ìˆ˜ì¹˜ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            conf_values = df[confidence_col].map(confidence_mapping).fillna('UNKNOWN')
            confidence_counts = conf_values.value_counts()
            print(f"\n{category.upper()} ì‹ ë¢°ë„ ë¶„í¬:")
            for conf, count in confidence_counts.items():
                percentage = (count / len(df)) * 100
                print(f"  {conf}: {count}ê°œ ({percentage:.1f}%)")
    
    # ì§€í‘œ ë°ì´í„° ì™„ì„±ë„ ë¶„ì„
    indicator_cols = [col for col in df.columns if col.startswith('indicator_')]
    print(f"\nì§€í‘œ ë°ì´í„° ì™„ì„±ë„:")
    for col in indicator_cols:
        non_null_count = df[col].count()
        percentage = (non_null_count / len(df)) * 100
        print(f"  {col}: {non_null_count}/{len(df)} ({percentage:.1f}%)")
    
    # ì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” ì§€í‘œ ë¶„ì„
    print(f"\nì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” ì§€í‘œ:")
    for category in categories:
        prefix = f'{category}_'
        special_cols = [col for col in df.columns if col.startswith(prefix) and 
                       col.endswith(('_strength', '_potential', '_level', '_bias'))]
        if special_cols:
            print(f"  {category.upper()}:")
            for col in special_cols:
                non_null_count = df[col].count()
                percentage = (non_null_count / len(df)) * 100
                print(f"    {col}: {non_null_count}/{len(df)} ({percentage:.1f}%)")
    
    # ì¶©ëŒ ì •ë³´ ë¶„ì„
    conflict_cols = [col for col in df.columns if col.startswith('conflict_')]
    if conflict_cols:
        print(f"\nì¶©ëŒ ì •ë³´:")
        for col in conflict_cols:
            if col in ['conflict_conflicts_count', 'conflict_conflicts_details']:
                continue  # ì´ì „ ë°©ì‹ì˜ ì»¬ëŸ¼ì€ ìŠ¤í‚µ
            non_null_count = df[col].count()
            if non_null_count > 0:
                if df[col].dtype in ['float64', 'int64']:
                    stats = df[col].describe()
                    print(f"  {col}: í‰ê· ={stats['mean']:.3f}, ìµœëŒ€={stats['max']:.3f}, ìµœì†Œ={stats['min']:.3f}")
                else:
                    print(f"  {col}: {non_null_count}ê°œ ê°’")
    
    # ê¸°ì¡´ ë°©ì‹ì˜ ì¶©ëŒ ì •ë³´ (í•˜ìœ„ í˜¸í™˜ì„±)
    if 'conflicts_count' in df.columns:
        conflict_stats = df['conflicts_count'].describe()
        print(f"\nê¸°ì¡´ ì¶©ëŒ ì •ë³´:")
        print(f"  í‰ê·  ì¶©ëŒ ìˆ˜: {conflict_stats['mean']:.2f}")
        print(f"  ìµœëŒ€ ì¶©ëŒ ìˆ˜: {conflict_stats['max']:.0f}")
        print(f"  ì¶©ëŒì´ ìˆëŠ” ë ˆì½”ë“œ: {(df['conflicts_count'] > 0).sum()}ê°œ")

def main_example():
    """Decision ë°ì´í„° ìƒì„± ì „ìš© - Parquet ì €ì¥ ë° ì¬ì‹œì‘ ì§€ì›"""
    print("\nğŸ“Š Decision ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    
    # 1. ê¸°ì¡´ ë°ì´í„° í™•ì¸
    if check_existing_decision_data():
        response = input("ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if response.lower() != 'y':
            print("ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            df = load_decisions_from_parquet()
            if df is not None:
                analyze_decision_data(df)
            return
    
    # 2. ì‹¤ì œ ETHUSDC ë°ì´í„° ë¡œë“œ (3ë¶„, 15ë¶„, 1ì‹œê°„ë´‰)
    price_data, price_data_15m, price_data_1h = load_ethusdc_data()
    
    if price_data is None:
        print("ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    print(f"ê°€ê²© ë°ì´í„° ì •ë³´:")
    print(f"   - ì´ ìº”ë“¤ ìˆ˜: {len(price_data)}ê°œ")
    print(f"   - ê°€ê²© ë²”ìœ„: ${price_data['close'].min():.2f} ~ ${price_data['close'].max():.2f}")
    
    # 3. CSV ë°ì´í„°ë¡œ ì‹¤ì œ ì§€í‘œ ì—…ë°ì´íŠ¸ ë° ì „ëµ ì‹¤í–‰ (ì¬ì‹œì‘ ì§€ì›)
    success = generate_signal_data_with_indicators(price_data, price_data_15m, price_data_1h, resume_from_progress=True)

    if success:
        print("\nâœ… Decision ë°ì´í„°ê°€ Parquet íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ì €ì¥ëœ ë°ì´í„° í™•ì¸ ë° ë¶„ì„
        df = load_decisions_from_parquet()
        if df is not None:
            print(f"\nì €ì¥ëœ ë°ì´í„° ìš”ì•½:")
            print(f"   - ì´ ë ˆì½”ë“œ ìˆ˜: {len(df)}")
            print(f"   - ì‹œê°„ ë²”ìœ„: {df['timestamp'].min()} ~ {df['timestamp'].max()}")
            print(f"   - ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
            
            # ìƒì„¸ ë¶„ì„
            analyze_decision_data(df)
            
            # Parquet êµ¬ì¡° í™•ì¸
            inspect_parquet_structure()
            
            print(f"\nğŸ¯ Decision ë°ì´í„° ìƒì„± ì™„ë£Œ!")
            print(f"   - íŒŒì¼ ìœ„ì¹˜: agent/decisions_data.parquet")
            print(f"   - ì´ ë°ì´í„°ë¥¼ ê°•í™”í•™ìŠµì´ë‚˜ ë‹¤ë¥¸ ë¶„ì„ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
    else:
        print("âŒ ë°ì´í„° ìƒì„±ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ì¬ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("   - ì§„í–‰ ìƒíƒœê°€ ì €ì¥ë˜ì–´ ìˆì–´ì„œ ë‹¤ìŒì— ì´ì–´ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    # ì˜ˆì‹œ ì‹¤í–‰
    main_example()