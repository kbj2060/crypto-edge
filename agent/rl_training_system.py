"""
58ì°¨ì› RL Decision ê¸°ë°˜ ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© AI í›ˆë ¨ ì‹œìŠ¤í…œ - Part 1
- ìƒˆë¡œìš´ RL Decision ìŠ¤í‚¤ë§ˆ í™œìš© (action_value, confidence_value ë“±)
- Conflict ì •ë³´ ë° ì‹œë„ˆì§€ ë©”íƒ€ë°ì´í„° í™œìš©
- ì¤‘ë³µ ê³„ì‚° ì œê±° ë° ì •ë³´ í™œìš© ê·¹ëŒ€í™”
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
import gymnasium as gym
import os

from collections import deque, namedtuple
from gymnasium import spaces
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path

# PyTorch í˜¸í™˜ì„± ì„¤ì •
def setup_pytorch_compatibility():
    """PyTorch ë²„ì „ í˜¸í™˜ì„± ì„¤ì •"""
    try:
        # NumPy 2.0 í˜¸í™˜ì„±ì„ ìœ„í•œ ì•ˆì „í•œ ê¸€ë¡œë²Œ ì„¤ì •
        safe_globals = [
            np.ndarray,
            np.dtype,
            np.float32,
            np.float64,
            np.int32,
            np.int64,
        ]
        
        # numpy.core ëŒ€ì‹  numpy._core ì‚¬ìš© (NumPy 2.0 í˜¸í™˜)
        try:
            import numpy._core.multiarray
            safe_globals.append(numpy._core.multiarray.scalar)
        except ImportError:
            # ì´ì „ ë²„ì „ í˜¸í™˜ì„±
            try:
                import numpy.core.multiarray
                safe_globals.append(numpy.core.multiarray.scalar)
            except ImportError:
                pass
        
        torch.serialization.add_safe_globals(safe_globals)
        print("PyTorch í˜¸í™˜ ì„¤ì • ì™„ë£Œ (NumPy 2.0 í˜¸í™˜)")
    except AttributeError:
        print("PyTorch ì´ì „ ë²„ì „ ê°ì§€ë¨")

setup_pytorch_compatibility()

Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])

class RewardCalculator:
    """ìŠ¹ë¥ ê³¼ ìˆ˜ìµì„±ì„ ìµœì í™”í•˜ëŠ” ë³´ìƒ ê³„ì‚°ê¸° (Signal ê¸°ë°˜)"""
    
    def __init__(self, max_trades_memory: int = 50):
        self.recent_trades = deque(maxlen=max_trades_memory)
        self.baseline_return = 0.0
        
    def calculate_reward(self, current_price: float, entry_price: float, position: float, 
                    action: str, holding_time: int, trade_pnl: Optional[float] = None) -> float:
        """ê°•í™”ëœ ë³´ìƒ ì‹œìŠ¤í…œ (100ë°° ì¦í­ + êµ¬ê°„ë³„ ì°¨ë“± ë³´ìƒ)"""
        reward = 0.0
        
        # ê±°ë˜ ì™„ë£Œ ì‹œ: ìˆ˜ìµë¥ ì„ 100ë°° ì¦í­ (ê°•í•œ í•™ìŠµ ì‹ í˜¸)
        if trade_pnl is not None:
            reward = trade_pnl * 100  # 100ë°° ì¦í­
            
            # êµ¬ê°„ë³„ ì°¨ë“± ë³´ìƒ
            if trade_pnl >= 0.02:  # 2% ì´ìƒ ìˆ˜ìµ
                reward += 50  # í° ë³´ë„ˆìŠ¤
            elif trade_pnl >= 0.01:  # 1% ì´ìƒ ìˆ˜ìµ
                reward += 20  # ì¤‘ê°„ ë³´ë„ˆìŠ¤
            elif trade_pnl > 0:  # ì–‘ì˜ ìˆ˜ìµ
                reward += 5   # ì‘ì€ ë³´ë„ˆìŠ¤
            elif trade_pnl <= -0.05:  # 5% ì´ìƒ ì†ì‹¤
                reward -= 30  # í° í˜ë„í‹°
            elif trade_pnl < 0:  # ì†ì‹¤
                reward -= 10  # ì‘ì€ í˜ë„í‹°
        
        # ê±°ë˜ ì™„ë£Œê°€ ì•„ë‹Œ ê²½ìš°: ë¯¸ì‹¤í˜„ ì†ìµ ê¸°ë°˜ ë³´ìƒ
        elif abs(position) > 0.0001 and entry_price > 0:  # ì„ê³„ê°’ ê°ì†Œ
            unrealized_pnl = self._calculate_unrealized_pnl(current_price, entry_price, position)
            reward = unrealized_pnl * 10  # 10ë°° ë³´ìƒ (í•™ìŠµ ì‹ í˜¸ ê°•í™”)
        
        return reward
    
    
    def _calculate_unrealized_pnl(self, current_price: float, entry_price: float, position: float) -> float:
        """ë¯¸ì‹¤í˜„ ì†ìµ ê³„ì‚°"""
        if entry_price <= 0:
            return 0.0
        
        price_change = (current_price - entry_price) / entry_price
        return position * price_change
    
    def _calculate_signal_reward(self, signal_data: Dict, position: float) -> float:
        """Signal ë°ì´í„° ê¸°ë°˜ ì¶”ê°€ ë³´ìƒ - ìƒˆë¡œìš´ RL ìŠ¤í‚¤ë§ˆ ê¸°ë°˜"""
        signal_reward = 0.0
        
        # ê° ì‹œê°„ëŒ€ë³„ ì‹ í˜¸ì™€ í¬ì§€ì…˜ ì¼ì¹˜ë„
        for timeframe in ['short_term', 'medium_term', 'long_term']:
            action_value = float(signal_data.get(f'{timeframe}_action', 0.0))
            net_score = float(signal_data.get(f'{timeframe}_net_score', 0.0))
            confidence_value = float(signal_data.get(f'{timeframe}_confidence', 0.0))
            
            # Action valueì™€ í¬ì§€ì…˜ ì¼ì¹˜ë„ (action_value: -1~1, position: -1~1)
            action_match = 1.0 - abs(action_value - position) / 2.0  # 0~1 ë²”ìœ„
            signal_reward += action_match * abs(net_score) * confidence_value * 0.3
        
        # Conflict ì •ë³´ í™œìš©
        conflict_penalty = float(signal_data.get('conflict_conflict_penalty', 0.0))
        conflict_consensus = float(signal_data.get('conflict_directional_consensus', 0.0))
        
        # Conflictê°€ ì ê³  consensusê°€ ë†’ì„ ë•Œ ë³´ìƒ
        if conflict_penalty == 0.0 and conflict_consensus > 0.5:
            signal_reward += 0.2
        
        return signal_reward



class DuelingDQN(nn.Module):
    """Dueling DQN (Value + Advantage ë¶„ë¦¬ë¡œ ì•ˆì •ì ì¸ í•™ìŠµ)"""
    
    def __init__(self, state_size: int, action_size: int = 3, hidden_size: int = 256, 
                 dropout: float = 0.1):
        super().__init__()
        
        self.state_size = state_size
        self.hidden_size = hidden_size
        
        # ê³µí†µ íŠ¹ì§• ì¶”ì¶œê¸° (Feature Extractor)
        self.feature_extractor = nn.Sequential(
            nn.Linear(state_size, hidden_size),
            nn.LayerNorm(hidden_size),
            nn.GELU(),
            nn.Dropout(dropout),
            
            nn.Linear(hidden_size, hidden_size),
            nn.LayerNorm(hidden_size),
            nn.GELU(),
            nn.Dropout(dropout),
            
            nn.Linear(hidden_size, hidden_size // 2),
            nn.LayerNorm(hidden_size // 2),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # Value Stream (ìƒíƒœì˜ ê°€ì¹˜)
        self.value_stream = nn.Sequential(
            nn.Linear(hidden_size // 2, hidden_size // 4),
            nn.LayerNorm(hidden_size // 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 4, 1)  # ë‹¨ì¼ Value ì¶œë ¥
        )
        
        # Advantage Stream (ì•¡ì…˜ë³„ ì¥ì )
        self.advantage_stream = nn.Sequential(
            nn.Linear(hidden_size // 2, hidden_size // 4),
            nn.LayerNorm(hidden_size // 4),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # ê° ì•¡ì…˜ ì°¨ì›ë³„ Advantage í—¤ë“œ
        self.position_advantage = nn.Linear(hidden_size // 4, 21)  # í¬ì§€ì…˜ -0.5~0.5 (21ê°œ êµ¬ê°„)
        self.leverage_advantage = nn.Linear(hidden_size // 4, 10)  # ë ˆë²„ë¦¬ì§€ 1~5 (10ê°œ êµ¬ê°„)
        self.holding_advantage = nn.Linear(hidden_size // 4, 20)   # í™€ë”© 10~60ë¶„ (20ê°œ êµ¬ê°„)
        
        # ìˆ˜ìµë¥  ì˜ˆì¸¡ (Value Stream í™œìš©)
        self.profit_predictor = nn.Sequential(
            nn.Linear(hidden_size // 2, hidden_size // 4),
            nn.LayerNorm(hidden_size // 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 4, 1)
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """Orthogonal ì´ˆê¸°í™” (DuelingDQNì— ë” ì í•©)"""
        if isinstance(module, nn.Linear):
            nn.init.orthogonal_(module.weight)
            nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        # ë°°ì¹˜ ì°¨ì› í™•ì¸
        if x.dim() == 1:
            x = x.unsqueeze(0)
            single_sample = True
        else:
            single_sample = False
        
        # ê³µí†µ íŠ¹ì§• ì¶”ì¶œ
        features = self.feature_extractor(x)  # [batch_size, hidden_size//2]
        
        # Value Stream (ìƒíƒœì˜ ê°€ì¹˜)
        value = self.value_stream(features)  # [batch_size, 1]
        
        # Advantage Stream (ì•¡ì…˜ë³„ ì¥ì )
        advantage_features = self.advantage_stream(features)  # [batch_size, hidden_size//4]
        
        # ê° ì•¡ì…˜ ì°¨ì›ë³„ Advantage ê³„ì‚°
        position_adv = self.position_advantage(advantage_features)  # [batch_size, 21]
        leverage_adv = self.leverage_advantage(advantage_features)  # [batch_size, 10]
        holding_adv = self.holding_advantage(advantage_features)    # [batch_size, 20]
        
        # Dueling êµ¬ì¡°: Q(s,a) = V(s) + A(s,a) - mean(A(s,a))
        # ê° ì•¡ì…˜ ì°¨ì›ë³„ë¡œ í‰ê·  Advantageë¥¼ ë¹¼ì„œ ì•ˆì •ì„± í™•ë³´
        position_q = value + position_adv - position_adv.mean(dim=1, keepdim=True)
        leverage_q = value + leverage_adv - leverage_adv.mean(dim=1, keepdim=True)
        holding_q = value + holding_adv - holding_adv.mean(dim=1, keepdim=True)
        
        # ìˆ˜ìµë¥  ì˜ˆì¸¡ (ê³µí†µ íŠ¹ì§• í™œìš©)
        profit_pred = self.profit_predictor(features)
        
        # ë‹¨ì¼ ìƒ˜í”Œì´ë©´ ë°°ì¹˜ ì°¨ì› ì œê±°
        if single_sample:
            position_q = position_q.squeeze(0)
            leverage_q = leverage_q.squeeze(0)
            holding_q = holding_q.squeeze(0)
            profit_pred = profit_pred.squeeze(0)
        
        return position_q, leverage_q, holding_q, profit_pred





class TradingEnvironment(gym.Env):
    """58ì°¨ì› RL Decision ê¸°ë°˜ ì•”í˜¸í™”í ê±°ë˜ ê°•í™”í•™ìŠµ í™˜ê²½ (Gymnasium í˜¸í™˜) - Open ê°€ê²© ê¸°ë°˜"""
    
    def __init__(self, price_data: pd.DataFrame, signal_data: List[Dict], 
                 initial_balance: float = 10000.0, max_position: float = 1.0):
        super().__init__()
        
        self.price_data = price_data
        self.signal_data = signal_data
        self.initial_balance = initial_balance
        self.max_position = max_position
        
        self.reward_calculator = RewardCalculator()
        
        # ì•¡ì…˜/ìƒíƒœ ìŠ¤í˜ì´ìŠ¤ ì •ì˜ (í¬ì§€ì…˜ -0.5~0.5 ë²”ìœ„ë¡œ ì¶•ì†Œ)
        self.action_space = spaces.Box(
            low=np.array([-0.5, 1.0, 10.0]),  # í¬ì§€ì…˜ -0.5~0.5 ë²”ìœ„ë¡œ ì¶•ì†Œ
            high=np.array([0.5, 5.0, 60.0]),  # ë ˆë²„ë¦¬ì§€ 1~5, í™€ë”© 10~60ë¶„
            dtype=np.float32
        )
        
        # ê±°ë˜ ì œí•œ ì„¤ì • (ë‹¨íƒ€ ìµœì í™”)
        self.min_trade_interval = 1  # ìµœì†Œ 1ìŠ¤í… ê°„ê²© (ë” ìì£¼ ê±°ë˜ í—ˆìš©)
        self.last_trade_step = -self.min_trade_interval  # ì´ˆê¸°ê°’
        self.trading_cost = 0.0001  # 0.01% ê±°ë˜ ë¹„ìš© (ì†Œì•¡ ê±°ë˜ì— ì í•©)
        
        # 58ì°¨ì› ìƒíƒœ ê³µê°„ (ê¸°ìˆ ì  ì§€í‘œ + í¬íŠ¸í´ë¦¬ì˜¤ + ì˜ì‚¬ê²°ì • íŠ¹ì„±)
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, 
            shape=(58,),  # 3 + 20 + 9 + 26 = 58ì°¨ì›
            dtype=np.float32
        )
        
        self.reset()
    
    def reset(self, seed=None, options=None):
        """í™˜ê²½ ì´ˆê¸°í™” (Gymnasium í˜¸í™˜)"""
        if seed is not None:
            np.random.seed(seed)
        
        self.current_step = 10
        self.balance = self.initial_balance
        self.current_position = 0.0
        self.current_leverage = 1.0
        self.entry_price = 0.0
        self.unrealized_pnl = 0.0
        self.total_trades = 0
        self.winning_trades = 0
        self.max_drawdown = 0.0
        self.peak_balance = self.initial_balance
        self.consecutive_losses = 0
        self.holding_time = 0
        self.in_position = False
        self.last_trade_pnl = None
        self.last_trade_step = -self.min_trade_interval  # ê±°ë˜ ê°„ê²© ì´ˆê¸°í™”
        
        observation = self._get_observation()
        info = self._create_info_dict()
        
        return observation, info
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """í™˜ê²½ ìŠ¤í… ì‹¤í–‰ (Gymnasium í˜¸í™˜) - Open ê°€ê²© ê¸°ë°˜"""
        if self.current_step >= min(len(self.price_data), len(self.signal_data)) - 1:
            return self._get_observation(), 0.0, True, False, {}
        
        position_change = np.clip(action[0], -0.5, 0.5)  # í¬ì§€ì…˜ -0.5~0.5 ë²”ìœ„
        leverage = np.clip(action[1], 1.0, 5.0)  # ë ˆë²„ë¦¬ì§€ 1~5
        target_holding_minutes = np.clip(action[2], 10.0, 60.0)  # 10ë¶„~60ë¶„
        
        # í˜„ì¬ ìŠ¤í…ì˜ open ê°€ê²© (ì‹¤ì œ ê±°ë˜ì—ì„œ ì•Œ ìˆ˜ ìˆëŠ” ê°€ê²©)
        current_price = self.price_data.iloc[self.current_step]['open']
        # ë‹¤ìŒ ìŠ¤í…ì˜ open ê°€ê²© (ë‹¤ìŒ ìº”ë“¤ ì‹œì‘ ì‹œì )
        next_price = self.price_data.iloc[self.current_step + 1]['open']
        
        # í¬ì§€ì…˜ ë° ê±°ë˜ ì²˜ë¦¬
        trade_completed, old_position = self._process_position_change(
            position_change, leverage, current_price, target_holding_minutes
        )
        
        # ê±°ë˜ ì™„ë£Œ ì‹œ ê±°ë˜ ìŠ¤í… ì—…ë°ì´íŠ¸
        if trade_completed:
            self.last_trade_step = self.current_step
        
        # Signal ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        current_signal = self.signal_data[self.current_step] if self.current_step < len(self.signal_data) else {}
        
        # ë³´ìƒ ê³„ì‚° (Signal ì •ë³´ í™œìš©) - Open ê°€ê²© ê¸°ë°˜
        if trade_completed:
            # ê±°ë˜ ì™„ë£Œ ì‹œ: ì‹¤ì œ ìˆ˜ìµë¥  ê¸°ë°˜ ë³´ìƒ (ì´ì „ ìº”ë“¤ì˜ close ê°€ê²© ì‚¬ìš©)
            if self.current_step > 0:
                # ì´ì „ ìº”ë“¤ì´ ì™„ì„±ëœ close ê°€ê²©ìœ¼ë¡œ ìˆ˜ìµë¥  ê³„ì‚°
                prev_close_price = self.price_data.iloc[self.current_step - 1]['close']
            reward = self.reward_calculator.calculate_reward(
                    current_price=prev_close_price,  # ì™„ì„±ëœ close ê°€ê²© ì‚¬ìš©
                entry_price=self.entry_price,
                position=old_position,  # ê±°ë˜ ì „ í¬ì§€ì…˜ ì‚¬ìš©
                action='TRADE',
                holding_time=self.holding_time,
                trade_pnl=self.last_trade_pnl
            )
        else:
            # ê±°ë˜ ì™„ë£Œê°€ ì•„ë‹Œ ê²½ìš°: ë¯¸ì‹¤í˜„ ì†ìµ ê¸°ë°˜ ë³´ìƒ (í˜„ì¬ open ê°€ê²© ì‚¬ìš©)
            reward = self.reward_calculator.calculate_reward(
                current_price=current_price,  # í˜„ì¬ open ê°€ê²© ì‚¬ìš©
                entry_price=self.entry_price,
                position=self.current_position,
                action='HOLD',
                holding_time=self.holding_time,
                trade_pnl=None
            )
        
        # ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ ì´ë™
        self.current_step += 1
        self.holding_time += 3
                
        done = (self.current_step >= min(len(self.price_data), len(self.signal_data)) - 1 or 
                self.balance <= self.initial_balance * 0.1)
        
        truncated = False  # Gymnasium í˜¸í™˜ì„ ìœ„í•œ truncated í”Œë˜ê·¸
        info = self._create_info_dict()
        
        return self._get_observation(), reward, done, truncated, info
    
    def _get_observation(self) -> np.ndarray:
        """58ì°¨ì› ìƒíƒœ ê´€ì°°ê°’ ë°˜í™˜ (ê¸°ìˆ ì  ì§€í‘œ + í¬íŠ¸í´ë¦¬ì˜¤ + ì˜ì‚¬ê²°ì • íŠ¹ì„±) - Open ê°€ê²© ê¸°ë°˜"""
        if self.current_step >= min(len(self.price_data), len(self.signal_data)):
            return np.zeros(58, dtype=np.float32)
        
        # í˜„ì¬ ê°€ê²©ê³¼ ì´ì „ ê°€ê²© ë¹„êµ (3ì°¨ì›) - Open ê°€ê²© ì‚¬ìš©
        current_price = self.price_data.iloc[self.current_step]['open']
        if self.current_step > 0:
            prev_price = self.price_data.iloc[self.current_step - 1]['open']
            price_change = (current_price - prev_price) / prev_price
        else:
            price_change = 0.0
        
        basic_observation = np.array([
            price_change,  # ê°€ê²© ë³€í™”ìœ¨
            self.current_position,  # í˜„ì¬ í¬ì§€ì…˜ (-1~1)
            self.balance / self.initial_balance  # ì”ê³  ë¹„ìœ¨
        ], dtype=np.float32)
        
        # Signal ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        current_signal = self.signal_data[self.current_step] if self.current_step < len(self.signal_data) else {}
        current_candle = self.price_data.iloc[self.current_step].to_dict()
        
        # ê° ì°¨ì›ë³„ íŠ¹ì„± ì¶”ì¶œ
        price_indicators = self._extract_price_indicators(current_signal, current_candle)  # 20ì°¨ì›
        portfolio_state = self._get_portfolio_state()  # 9ì°¨ì›
        decision_features = self._extract_decision_features(current_signal)  # 26ì°¨ì›
        
        # ëª¨ë“  ì°¨ì› ê²°í•© (3 + 20 + 9 + 26 = 58ì°¨ì›)
        observation = np.concatenate([
            basic_observation,      # 3ì°¨ì›
            price_indicators,       # 20ì°¨ì›
            portfolio_state,        # 9ì°¨ì›
            decision_features       # 26ì°¨ì›
        ], dtype=np.float32)
        
        return observation
    
    def _extract_price_indicators(self, signal_data: Dict, current_candle: Dict) -> np.ndarray:
        """Signalì˜ indicatorë“¤ì„ price featureë¡œ í™œìš© (20ì°¨ì›) - Open ê°€ê²© ê¸°ë°˜"""
        current_price = current_candle['open']  # Open ê°€ê²© ì‚¬ìš©
        
        # 1. ê°€ê²© ëŒ€ë¹„ ì§€í‘œ ìœ„ì¹˜
        vwap = signal_data.get('indicator_vwap')
        poc = signal_data.get('indicator_poc')  
        hvn = signal_data.get('indicator_hvn')
        lvn = signal_data.get('indicator_lvn')
        
        # 2. ë³€ë™ì„± ì§€í‘œë“¤
        atr = signal_data.get('indicator_atr', 0.0)
        vwap_std = signal_data.get('indicator_vwap_std', 0.0)
        
        # 3. ì¼ë³„ ê¸°ì¤€ì ë“¤
        prev_high = signal_data.get('indicator_prev_day_high')
        prev_low = signal_data.get('indicator_prev_day_low')
        or_high = signal_data.get('indicator_opening_range_high')
        or_low = signal_data.get('indicator_opening_range_low')
        
        prev_range = prev_high - prev_low
        prev_day_position = (current_price - prev_low) / prev_range if prev_range > 0 else 0.5
            
        or_range = or_high - or_low  
        or_position = (current_price - or_low) / or_range if or_range > 0 else 0.5
        
        # 4. í˜„ì¬ ìº”ë“¤ ì •ë³´
        high, low, close, open_price = current_candle['high'], current_candle['low'], current_candle['close'], current_candle['open']
        quote_volume = current_candle.get('quote_volume')
        
        return np.array([
            # ê°€ê²© ëŒ€ë¹„ ì§€í‘œ ìœ„ì¹˜ (4ê°œ)
            (current_price - vwap) / current_price if current_price > 0 else 0.0,
            (current_price - poc) / current_price if current_price > 0 else 0.0,   
            (current_price - hvn) / current_price if current_price > 0 else 0.0,   
            (current_price - lvn) / current_price if current_price > 0 else 0.0,
            
            # ë³€ë™ì„± ì§€í‘œë“¤ (2ê°œ)
            atr / current_price if current_price > 0 else 0.0,
            vwap_std / current_price if current_price > 0 else 0.0,
            
            # ì¼ë³„ ê¸°ì¤€ì ë“¤ (4ê°œ)
            prev_day_position,
            or_position,
            (current_price - prev_high) / current_price if current_price > 0 else 0.0,
            (prev_low - current_price) / current_price if current_price > 0 else 0.0,
            
            # í˜„ì¬ ìº”ë“¤ ì •ë³´ (8ê°œ)
            (close - open_price) / open_price if open_price > 0 else 0.0,
            (high - low) / close if close > 0 else 0.0,
            (high - close) / (high - low) if high > low else 0.5,
            (close - low) / (high - low) if high > low else 0.5,
            (close - open_price) / (high - low) if high > low else 0.0,
            min(quote_volume / 1000000, 2.0) if quote_volume > 0 else 0.0,
            1.0 if close > open_price else 0.0,
            (high - max(open_price, close)) / (high - low) if high > low else 0.0,
            
            # ì¶”ê°€ ìº”ë“¤ ì •ë³´ (2ê°œ)
            (low - min(open_price, close)) / (high - low) if high > low else 0.0,
            abs(close - open_price) / (high - low) if high > low else 0.0
        ], dtype=np.float32)
    
    def _get_portfolio_state(self) -> np.ndarray:
        """í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ì •ë³´ (9ì°¨ì›)"""
        return np.array([
            self.current_position,
            self.current_leverage / 20.0,
            (self.balance - self.initial_balance) / self.initial_balance,
            self.unrealized_pnl / self.initial_balance if self.initial_balance > 0 else 0.0,
            min(self.total_trades / 100.0, 1.0),
            self.winning_trades / max(self.total_trades, 1),
            self.max_drawdown,
            min(self.consecutive_losses / 10.0, 1.0),
            min(self.holding_time / 1440.0, 1.0)
        ], dtype=np.float32)
    
    def _extract_decision_features(self, signals: Dict) -> np.ndarray:
        """Decision íŠ¹ì„±ë“¤ (26ì°¨ì›) - ìƒˆë¡œìš´ RL ìŠ¤í‚¤ë§ˆ ê¸°ë°˜"""
        # ê° ì‹œê°„ëŒ€ë³„ íŠ¹ì„± (3 Ã— 6 = 18ê°œ)
        timeframe_features = []
        for timeframe in ['short_term', 'medium_term', 'long_term']:
            # ìƒˆë¡œìš´ RL ìŠ¤í‚¤ë§ˆ í•„ë“œë“¤ ì‚¬ìš©
            action_value = float(signals.get(f'{timeframe}_action', 0.0))
            net_score = float(signals.get(f'{timeframe}_net_score', 0.0))
            buy_score = float(signals.get(f'{timeframe}_buy_score', 0.0))
            sell_score = float(signals.get(f'{timeframe}_sell_score', 0.0))
            confidence_value = float(signals.get(f'{timeframe}_confidence', 0.0))
            market_context_value = float(signals.get(f'{timeframe}_market_context', 0.0))
            
            timeframe_features.extend([action_value, net_score, buy_score, sell_score, confidence_value, market_context_value])
        
        # ì¶”ê°€ ë©”íƒ€ ì •ë³´ (3ê°œ)
        signals_used = []
        for timeframe in ['short_term', 'medium_term', 'long_term']:
            used = signals.get(f'{timeframe}_signals_used', 0)
            
            signals_used.append(min(float(used) / 10.0, 1.0))
        
        # Conflict ì •ë³´ (3ê°œ)
        conflict_severity = float(signals.get('conflict_conflict_severity', 0.0))
        conflict_consensus = float(signals.get('conflict_directional_consensus', 0.0))
        conflict_penalty = float(signals.get('conflict_conflict_penalty', 0.0))
        
        # Long term íŠ¹í™” ì •ë³´ (2ê°œ)
        institutional_bias = float(signals.get('long_term_institutional_bias', 0.0))
        macro_trend_strength = float(signals.get('long_term_macro_trend_strength', 0.0))
        
        return np.array(
            timeframe_features + 
            signals_used + 
            [conflict_severity, conflict_consensus, conflict_penalty, institutional_bias, macro_trend_strength],
            dtype=np.float32
        )
    
    
    def _process_position_change(self, position_change: float, leverage: float, 
                                current_price: float, target_holding_minutes: float) -> Tuple[bool, float]:
        """í¬ì§€ì…˜ ë³€ê²½ ì²˜ë¦¬ (ìˆœìˆ˜ RL ì—ì´ì „íŠ¸ ê²°ì •) - Open ê°€ê²© ê¸°ë°˜"""
        old_position = self.current_position
        trade_completed = False
        
        # RL ì—ì´ì „íŠ¸ì˜ ê²°ì •ë§Œìœ¼ë¡œ í¬ì§€ì…˜ ë³€ê²½ (-0.5~0.5 ë²”ìœ„)
        target_position = np.clip(self.current_position + position_change, -0.5, 0.5)
        
        # í¬ì§€ì…˜ ë³€ê²½ì´ í•„ìš”í•œì§€ í™•ì¸ (ì„ê³„ê°’ ëŒ€í­ ê°ì†Œ)
        if abs(target_position - self.current_position) > 0.0001:
            # ê¸°ì¡´ í¬ì§€ì…˜ ì²­ì‚° (ì´ì „ ìº”ë“¤ì˜ close ê°€ê²©ìœ¼ë¡œ ì²­ì‚°)
            if abs(self.current_position) > 0.0001:
                trade_completed = True
                if self.current_step > 0:
                    # ì´ì „ ìº”ë“¤ì´ ì™„ì„±ëœ close ê°€ê²©ìœ¼ë¡œ ì²­ì‚°
                    prev_close_price = self.price_data.iloc[self.current_step - 1]['close']
                    self.last_trade_pnl = self._calculate_trade_pnl(prev_close_price, self.entry_price, old_position)
                    self._close_position(prev_close_price)
                else:
                    # ì²« ë²ˆì§¸ ìŠ¤í…ì—ì„œëŠ” í˜„ì¬ open ê°€ê²© ì‚¬ìš©
                    self.last_trade_pnl = self._calculate_trade_pnl(current_price, self.entry_price, old_position)
                    self._close_position(current_price)
            
            # ìƒˆ í¬ì§€ì…˜ ì§„ì… (í˜„ì¬ open ê°€ê²©ìœ¼ë¡œ ì§„ì…)
            if abs(target_position) > 0.0001:
                self.current_position = target_position
                self.current_leverage = leverage
                self.entry_price = current_price  # Open ê°€ê²©ìœ¼ë¡œ ì§„ì…
                self.holding_time = 0
                self.in_position = True
                
        
        return trade_completed, old_position
        
    def _calculate_trade_pnl(self, exit_price: float, entry_price: float, position: float) -> float:
        """ê±°ë˜ ì†ìµ ê³„ì‚°"""
        if entry_price <= 0:
            return 0.0
        
        price_change = (exit_price - entry_price) / entry_price
        return position * price_change
    
    def _close_position(self, exit_price: float):
        """í¬ì§€ì…˜ ì²­ì‚°"""
        if abs(self.current_position) < 0.01:
            return
        
        pnl = self._calculate_trade_pnl(exit_price, self.entry_price, self.current_position)
        # ë ˆë²„ë¦¬ì§€ëŠ” ê±°ë˜ëŸ‰ì—ë§Œ ì ìš©, ì†ìµì—ëŠ” ì ìš©í•˜ì§€ ì•ŠìŒ
        trade_volume = abs(self.current_position) * self.current_leverage * self.balance
        pnl_usd = pnl * trade_volume  # ì˜¬ë°”ë¥¸ ì†ìµ ê³„ì‚°
        
        # ê±°ë˜ ìˆ˜ìˆ˜ë£Œ ì°¨ê° (ê±°ë˜ëŸ‰ ê¸°ì¤€)
        fee = trade_volume * self.trading_cost  # 0.1% ê±°ë˜ ë¹„ìš©
        pnl_usd -= fee
        
        # ì”ê³  ë° í†µê³„ ì—…ë°ì´íŠ¸
        self.balance += pnl_usd
        self._update_trading_stats(pnl_usd)
        
        # í¬ì§€ì…˜ ì´ˆê¸°í™”
        self.current_position = 0.0
        self.unrealized_pnl = 0.0
        self.in_position = False
        self.holding_time = 0
        self.last_trade_pnl = pnl
    
    def _update_trading_stats(self, pnl_usd: float):
        """ê±°ë˜ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.total_trades += 1
        
        if pnl_usd > 0:
            self.winning_trades += 1
            self.consecutive_losses = 0
        else:
            self.consecutive_losses += 1
        
        # ìµœëŒ€ ë‚™í­ ì—…ë°ì´íŠ¸
        if self.balance > self.peak_balance:
            self.peak_balance = self.balance
        else:
            drawdown = (self.peak_balance - self.balance) / self.peak_balance
            self.max_drawdown = max(self.max_drawdown, drawdown)
    
    def _create_info_dict(self) -> Dict:
        """ì •ë³´ ë”•ì…”ë„ˆë¦¬ ìƒì„± - Open ê°€ê²© ê¸°ë°˜"""
        current_price = self.price_data.iloc[min(self.current_step, len(self.price_data)-1)]['open']
        
        return {
            'balance': self.balance,
            'position': self.current_position,
            'unrealized_pnl': self.unrealized_pnl,
            'total_trades': self.total_trades,
            'win_rate': self.winning_trades / max(self.total_trades, 1),
            'current_price': current_price,
            'entry_price': self.entry_price,
            'holding_time': self.holding_time,
            'max_drawdown': self.max_drawdown,
            'trade_completed': hasattr(self, 'last_trade_pnl') and self.last_trade_pnl is not None,
            'trade_pnl': self.last_trade_pnl if hasattr(self, 'last_trade_pnl') else None
        }

"""
58ì°¨ì› RL Decision ê¸°ë°˜ ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© AI í›ˆë ¨ ì‹œìŠ¤í…œ - Part 2
- RLAgent í´ë˜ìŠ¤ ë° í›ˆë ¨/í‰ê°€ ì‹œìŠ¤í…œ
- ìƒˆë¡œìš´ Decision ìŠ¤í‚¤ë§ˆ ë°ì´í„° ë¡œë” ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
"""

class RLAgent:
    """58ì°¨ì› RL Decision ê¸°ë°˜ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸"""
    
    def __init__(self, state_size: int = 3, learning_rate: float = 3e-4, 
                    gamma: float = 0.99, epsilon: float = 0.9, epsilon_decay: float = 0.995,
                    hidden_size: int = 128):
        
        self.state_size = state_size
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.hidden_size = hidden_size
        self.learning_rate = learning_rate  # learning_rate ì†ì„± ì¶”ê°€
        self.epsilon_min = 0.1  # 10%ë¡œ ì„¤ì • (ì ì ˆí•œ íƒí—˜)
        
        # Îµ ê°’ì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì´ˆê¸°í™”
        if self.epsilon < self.epsilon_min:
            self.epsilon = 0.8  # 80%ë¡œ ì´ˆê¸°í™” (ì¶©ë¶„í•œ íƒí—˜)
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° ë””ë°”ì´ìŠ¤ ì„¤ì •
        if torch.cuda.is_available():
            self.device = torch.device("cuda")
            print(f"ğŸš€ GPU ì‚¬ìš©: {torch.cuda.get_device_name(0)}")
            print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        else:
            self.device = torch.device("cpu")
            print("âš ï¸  GPU ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
            print("   GPU ì‚¬ìš©ì„ ì›í•˜ë©´ PyTorch CUDA ë²„ì „ì„ ì„¤ì¹˜í•˜ì„¸ìš”:")
            print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
        print(f"Using device: {self.device} for {state_size}ì°¨ì› ëª¨ë¸")
        
        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” (DuelingDQN ì‚¬ìš© - Value + Advantage ë¶„ë¦¬ë¡œ ì•ˆì •ì ì¸ í•™ìŠµ)
        print("ğŸš€ DuelingDQN ì•„í‚¤í…ì²˜ ì‚¬ìš© (Value + Advantage ë¶„ë¦¬ë¡œ ì•ˆì •ì ì¸ í•™ìŠµ)")
        self.q_network = DuelingDQN(state_size, 3, hidden_size).to(self.device)
        self.target_network = DuelingDQN(state_size, 3, hidden_size).to(self.device)
        
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # ê²½í—˜ ë¦¬í”Œë ˆì´ (ìˆ˜ìµë¥  í•™ìŠµ ìµœì í™”)
        self.memory = deque(maxlen=20000)  # ë©”ëª¨ë¦¬ í¬ê¸° ì¦ê°€ (ë” ë§ì€ ê²½í—˜)
        self.batch_size = 128  # ë°°ì¹˜ í¬ê¸° ì¦ê°€ (ë” ì•ˆì •ì ì¸ í•™ìŠµ)
        
        # í•™ìŠµ ì¶”ì 
        self.training_rewards = []
        self.losses = []
        self.win_rates = []
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ (ìˆ˜ìµë¥  í•™ìŠµ ìµœì í™”)
        self.target_update_freq = 100  # ì ì ˆí•œ ì—…ë°ì´íŠ¸ ë¹ˆë„
        self.update_count = 0
        
        
        # ì•¡ì…˜ ê³µê°„ ì„¤ì • (í™˜ê²½ì—ì„œ ê°€ì ¸ì˜´)
        self.action_space = None  # í™˜ê²½ì—ì„œ ì„¤ì •ë¨
    
    def remember(self, state, action, reward, next_state, done):
        """ê²½í—˜ ì €ì¥"""
        self.memory.append(Experience(state, action, reward, next_state, done))
    
    
    def adaptive_learning_rate(self, recent_rewards: List[float], recent_win_rates: List[float]):
        """ì ì‘í˜• í•™ìŠµë¥  ì¡°ì •"""
        if len(recent_rewards) < 10:
            return
        
        # ìµœê·¼ ì„±ëŠ¥ ë¶„ì„
        avg_reward = np.mean(recent_rewards[-10:])
        avg_win_rate = np.mean(recent_win_rates[-10:])
        
        # ì„±ëŠ¥ì´ ì¢‹ìœ¼ë©´ í•™ìŠµë¥  ê°ì†Œ (ì•ˆì •í™”)
        if avg_win_rate > 0.4 and avg_reward > 0:
            self.learning_rate *= 0.95
            self.learning_rate = max(self.learning_rate, 1e-5)  # ìµœì†Œê°’ ë³´ì¥
        # ì„±ëŠ¥ì´ ë‚˜ì˜ë©´ í•™ìŠµë¥  ì¦ê°€ (ë¹ ë¥¸ í•™ìŠµ)
        elif avg_win_rate < 0.2 or avg_reward < -100:
            self.learning_rate *= 1.05
            self.learning_rate = min(self.learning_rate, 5e-3)  # ìµœëŒ€ê°’ ì œí•œ
        
        # ì˜µí‹°ë§ˆì´ì € ì—…ë°ì´íŠ¸
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = self.learning_rate
    
    def act(self, state: np.ndarray) -> np.ndarray:
        """ì•¡ì…˜ ì„ íƒ - í–¥ìƒëœ íƒí—˜ ì „ëµ"""
        if np.random.random() <= self.epsilon:
            return self._get_smart_random_action(state)
        
        return self._get_greedy_action(state)
    
    def _get_smart_random_action(self, state: np.ndarray) -> np.ndarray:
        """ì§€ëŠ¥ì ì¸ ëœë¤ ì•¡ì…˜ - ìƒíƒœì— ê¸°ë°˜í•œ ì œí•œì  íƒí—˜"""
        # ê¸°ë³¸ ëœë¤ ì•¡ì…˜
        action = self._get_random_action()
        
        # ìƒíƒœ ê¸°ë°˜ ì•¡ì…˜ ì¡°ì •
        if len(self.memory) > 100:  # ì¶©ë¶„í•œ ê²½í—˜ì´ ìˆì„ ë•Œ
            recent_trades = [exp for exp in list(self.memory)[-50:] if exp.reward > 0]
            if recent_trades:
                # ìµœê·¼ ì„±ê³µí•œ ì•¡ì…˜ íŒ¨í„´ ë¶„ì„
                successful_actions = [exp.action for exp in recent_trades]
                if successful_actions:
                    # ì„±ê³µí•œ ì•¡ì…˜ê³¼ ìœ ì‚¬í•œ ë°©í–¥ìœ¼ë¡œ íƒí—˜
                    base_action = np.mean(successful_actions, axis=0)
                    noise = np.random.normal(0, 0.1, action.shape)
                    
                    # action_spaceê°€ ìˆì„ ë•Œë§Œ í´ë¦¬í•‘ ì ìš©
                    if self.action_space is not None:
                        action = np.clip(base_action + noise, 
                                       self.action_space.low, 
                                       self.action_space.high)
                    else:
                        # ê¸°ë³¸ í´ë¦¬í•‘ (position_change, leverage, holding_time)
                        action = np.clip(base_action + noise, 
                                       [-0.5, 1.0, 10.0], 
                                       [0.5, 5.0, 60.0])
        
        return action
    
    def _get_random_action(self) -> np.ndarray:
        """ë³´ìˆ˜ì ì¸ ëœë¤ ì•¡ì…˜"""
        return np.array([
            np.random.uniform(-0.5, 0.5),  # í¬ì§€ì…˜ ë²”ìœ„ ì¶•ì†Œ
            np.random.uniform(1.0, 5.0), # ë ˆë²„ë¦¬ì§€ ìµœëŒ€ 5
            np.random.uniform(10.0, 60.0) # í™€ë”© ì‹œê°„ ì¶•ì†Œ
        ])
    
    def _get_greedy_action(self, state: np.ndarray) -> np.ndarray:
        """Qê°’ ê¸°ë°˜ íƒìš•ì  ì•¡ì…˜ ì„ íƒ (ìˆ˜ìµë¥  ìµœì í™”)"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            position_q, leverage_q, holding_q, profit_pred = self.q_network(state_tensor)
            
            # ìˆ˜ìµë¥  ì˜ˆì¸¡ì„ ê³ ë ¤í•œ ì•¡ì…˜ ì„ íƒ
            position_idx = torch.argmax(position_q).item()
            leverage_idx = torch.argmax(leverage_q).item()
            holding_idx = torch.argmax(holding_q).item()
            
            # ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€í™˜ (-0.5~0.5 ë²”ìœ„)
            position = -0.5 + (position_idx * 0.05)   # -0.5~0.5 (21ê°œ êµ¬ê°„)
            leverage = 1.0 + (leverage_idx * 0.4)   # 1.0~5.0 (10ê°œ êµ¬ê°„)
            holding = 10.0 + (holding_idx * 2.5)    # 10~60ë¶„ (20ê°œ êµ¬ê°„)
            
            return np.array([position, leverage, holding])
    
    def replay(self):
        """ìš°ì„ ìˆœìœ„ ê²½í—˜ ë¦¬í”Œë ˆì´ í•™ìŠµ"""
        if len(self.memory) < self.batch_size * 2:
            return
        
        # ìš°ì„ ìˆœìœ„ ìƒ˜í”Œë§ (ê¸ì •ì  ê²½í—˜ 70%, ì¤‘ë¦½ 20%, ë¶€ì • 10%)
        positive_experiences = [exp for exp in self.memory if exp.reward > 10]
        neutral_experiences = [exp for exp in self.memory if -10 <= exp.reward <= 10]
        negative_experiences = [exp for exp in self.memory if exp.reward < -10]
        
        batch = []
        batch_size = self.batch_size
        
        # ê¸ì •ì  ê²½í—˜ 70%
        if positive_experiences:
            pos_count = int(batch_size * 0.7)
            batch.extend(random.sample(positive_experiences, min(pos_count, len(positive_experiences))))
        
        # ì¤‘ë¦½ ê²½í—˜ 20%
        if neutral_experiences:
            neutral_count = int(batch_size * 0.2)
            batch.extend(random.sample(neutral_experiences, min(neutral_count, len(neutral_experiences))))
        
        # ë¶€ì •ì  ê²½í—˜ 10%
        if negative_experiences:
            neg_count = batch_size - len(batch)
            batch.extend(random.sample(negative_experiences, min(neg_count, len(negative_experiences))))
        
        # ë¶€ì¡±í•œ ê²½ìš° ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ì±„ì›€
        if len(batch) < batch_size:
            remaining = batch_size - len(batch)
            batch.extend(random.sample(self.memory, remaining))
        
        loss = self._compute_loss(batch)
        
        # ì—­ì „íŒŒ
        self.optimizer.zero_grad()
        loss.backward()
        
        # ê³ ê¸‰ ì •ê·œí™” ê¸°ë²•ë“¤ (ê³¼ì í•© ë°©ì§€)
        
        # 1. ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ì ì‘ì )
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 0.5)
        
        # 2. L2 ì •ê·œí™” (ì ì‘ì )
        l2_lambda = 1e-3  # ë” ê°•í•œ L2 ì •ê·œí™”
        l2_reg = torch.tensor(0., device=self.device)
        for param in self.q_network.parameters():
            l2_reg += torch.norm(param)
        loss += l2_lambda * l2_reg
        
        # 3. ì—”íŠ¸ë¡œí”¼ ì •ê·œí™”ëŠ” _compute_loss í•¨ìˆ˜ì—ì„œ ì²˜ë¦¬ë¨
        
        # 4. Spectral Normalization íš¨ê³¼ (ê°€ì¤‘ì¹˜ ì •ê·œí™”)
        spectral_reg = torch.tensor(0., device=self.device)
        for module in self.q_network.modules():
            if isinstance(module, nn.Linear):
                # ê°€ì¤‘ì¹˜ì˜ ìŠ¤í™íŠ¸ëŸ´ ë…¸ë¦„ ì •ê·œí™”
                weight_norm = torch.norm(module.weight, p=2)
                spectral_reg += weight_norm
        loss += 0.001 * spectral_reg
        
        # 5. Dropout ì ì‘ì  ì¡°ì • (ê³¼ì í•© ê°ì§€ ì‹œ)
        if len(self.losses) > 10:
            recent_losses = self.losses[-10:]
            loss_variance = torch.var(torch.tensor(recent_losses))
            if loss_variance < 0.01:  # ì†ì‹¤ì´ ì•ˆì •ì ì´ë©´ ë“œë¡­ì•„ì›ƒ ì¦ê°€
                for module in self.q_network.modules():
                    if isinstance(module, nn.Dropout):
                        module.p = min(module.p + 0.005, 0.3)
        
        self.optimizer.step()
        
        
        self.losses.append(loss.item())
        
        # ì ì‘ì  ì—¡ì‹¤ë¡  ê°ì†Œ (ì„±ëŠ¥ì— ë”°ë¼ ì¡°ì •)
        if self.epsilon > self.epsilon_min:
            # ìµœê·¼ ì„±ëŠ¥ ê¸°ë°˜ ì ì‘ì  ê°ì†Œ
            if len(self.training_rewards) > 20:  # ë” ë¹ ë¥¸ ë°˜ì‘
                recent_rewards = self.training_rewards[-20:]
                avg_recent_reward = np.mean(recent_rewards)
                
                # ìˆ˜ìµë¥  ê¸°ë°˜ ê°ì†Œ (ìƒˆë¡œìš´ ë³´ìƒ ë²”ìœ„ì— ë§ì¶¤)
                if avg_recent_reward > 10.0:  # ë†’ì€ ìˆ˜ìµë¥  (100% ì´ìƒ)
                    self.epsilon *= 0.95  # ë¹ ë¥¸ ê°ì†Œ
                elif avg_recent_reward > 5.0:  # ì–‘ì˜ ìˆ˜ìµë¥  (50% ì´ìƒ)
                    self.epsilon *= 0.97  # ì¤‘ê°„ ê°ì†Œ
                elif avg_recent_reward > 0:  # ì•½ê°„ì˜ ìˆ˜ìµë¥ 
                    self.epsilon *= 0.99  # ëŠë¦° ê°ì†Œ
                else:  # ì†ì‹¤
                    self.epsilon *= 0.995  # ë§¤ìš° ëŠë¦° ê°ì†Œ
            else:
                self.epsilon *= 0.995  # ì´ˆê¸°ì—ëŠ” ë§¤ìš° ëŠë¦° ê°ì†Œ
        
        # ì ì‘ì  í•™ìŠµ ì „ëµ: ì„±ê³¼ ê°œì„ ì´ ì—†ìœ¼ë©´ íƒí—˜ë¥  ìë™ ì¦ê°€
        if len(self.training_rewards) > 50:
            recent_rewards = self.training_rewards[-50:]
            older_rewards = self.training_rewards[-100:-50] if len(self.training_rewards) >= 100 else self.training_rewards[:-50]
            
            if len(older_rewards) > 0:
                recent_avg = np.mean(recent_rewards)
                older_avg = np.mean(older_rewards)
                
                # ì„±ê³¼ ê°œì„ ì´ ì—†ìœ¼ë©´ íƒí—˜ë¥  ì¦ê°€
                if recent_avg <= older_avg:
                    self.epsilon = min(self.epsilon * 1.01, 0.9)  # ìµœëŒ€ 90%ê¹Œì§€ ì¦ê°€
                    print(f"ğŸ“ˆ ì„±ê³¼ ê°œì„  ì—†ìŒ: íƒí—˜ë¥  ì¦ê°€ {self.epsilon:.3f}")
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
        self.update_count += 1
        if self.update_count % self.target_update_freq == 0:
            self.update_target_network()
    
    def _compute_loss(self, batch) -> torch.Tensor:
        """ìˆ˜ìµë¥  ìµœì í™” ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°"""
        # íš¨ìœ¨ì ì¸ í…ì„œ ë³€í™˜ (numpy ë°°ì—´ì„ ë¨¼ì € ê²°í•©)
        states = torch.FloatTensor(np.array([e.state for e in batch])).to(self.device)
        actions = [e.action for e in batch]
        rewards = torch.FloatTensor(np.array([e.reward for e in batch])).to(self.device)
        next_states = torch.FloatTensor(np.array([e.next_state for e in batch])).to(self.device)
        dones = [bool(e.done) for e in batch]
        
        # í˜„ì¬ Qê°’ë“¤ê³¼ ìˆ˜ìµë¥  ì˜ˆì¸¡
        current_position_q, current_leverage_q, current_holding_q, current_profit_pred = self.q_network(states)
        
        # Double DQN: í˜„ì¬ ë„¤íŠ¸ì›Œí¬ë¡œ ì•¡ì…˜ ì„ íƒ, íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ë¡œ Qê°’ ê³„ì‚°
        with torch.no_grad():
            # í˜„ì¬ ë„¤íŠ¸ì›Œí¬ë¡œ ë‹¤ìŒ ìƒíƒœì˜ ì•¡ì…˜ ì„ íƒ
            next_position_q_current, next_leverage_q_current, next_holding_q_current, _ = self.q_network(next_states)
            
            # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ë¡œ Qê°’ ê³„ì‚°
            next_position_q_target, next_leverage_q_target, next_holding_q_target, _ = self.target_network(next_states)
            
            target_position_q = current_position_q.clone()
            target_leverage_q = current_leverage_q.clone()
            target_holding_q = current_holding_q.clone()
            
            for i, (action, reward, done) in enumerate(zip(actions, rewards, dones)):
                pos_idx = int(np.clip((action[0] + 0.5) / 0.05, 0, 20))   # -0.5~0.5 ë²”ìœ„
                lev_idx = int(np.clip((action[1] - 1.0) / 0.4, 0, 9))    # 1.0~5.0 ë²”ìœ„
                hold_idx = int(np.clip((action[2] - 10.0) / 2.5, 0, 19)) # 10~60ë¶„ ë²”ìœ„
                
                if not done:
                    # Double DQN: í˜„ì¬ ë„¤íŠ¸ì›Œí¬ë¡œ ì„ íƒí•œ ì•¡ì…˜ì˜ íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ Qê°’ ì‚¬ìš©
                    best_pos_action = torch.argmax(next_position_q_current[i])
                    best_lev_action = torch.argmax(next_leverage_q_current[i])
                    best_hold_action = torch.argmax(next_holding_q_current[i])
                    
                    target_q_pos = reward + self.gamma * next_position_q_target[i, best_pos_action]
                    target_q_lev = reward + self.gamma * next_leverage_q_target[i, best_lev_action]
                    target_q_hold = reward + self.gamma * next_holding_q_target[i, best_hold_action]
                    
                    target_position_q[i, pos_idx] = target_q_pos
                    target_leverage_q[i, lev_idx] = target_q_lev
                    target_holding_q[i, hold_idx] = target_q_hold
                else:
                    # ìµœì¢… ë³´ìƒ (ìˆ˜ìµë¥  ì¤‘ì‹¬)
                    target_position_q[i, pos_idx] = reward
                    target_leverage_q[i, lev_idx] = reward
                    target_holding_q[i, hold_idx] = reward
        
        # Q-learning ì†ì‹¤ (ìˆ˜ìµë¥  ê°€ì¤‘ì¹˜ ì ìš©)
        pos_loss = F.smooth_l1_loss(current_position_q, target_position_q)
        lev_loss = F.smooth_l1_loss(current_leverage_q, target_leverage_q)
        hold_loss = F.smooth_l1_loss(current_holding_q, target_holding_q)
        
        # ìˆ˜ìµë¥  ì˜ˆì¸¡ ì†ì‹¤ (ë³´ì¡° í•™ìŠµ)
        profit_targets = rewards.unsqueeze(1)  # ì‹¤ì œ ìˆ˜ìµë¥ ì„ íƒ€ê²Ÿìœ¼ë¡œ
        profit_loss = F.mse_loss(current_profit_pred, profit_targets)
        
        # ì—”íŠ¸ë¡œí”¼ ì •ê·œí™” (ê³¼ì í•© ë°©ì§€)
        position_entropy = -torch.sum(F.softmax(current_position_q, dim=1) * 
                                    F.log_softmax(current_position_q, dim=1), dim=1).mean()
        leverage_entropy = -torch.sum(F.softmax(current_leverage_q, dim=1) * 
                                    F.log_softmax(current_leverage_q, dim=1), dim=1).mean()
        holding_entropy = -torch.sum(F.softmax(current_holding_q, dim=1) * 
                                   F.log_softmax(current_holding_q, dim=1), dim=1).mean()
        
        entropy_reg = 0.01 * (position_entropy + leverage_entropy + holding_entropy)
        
        # ìˆ˜ìµë¥  ì¤‘ì‹¬ ê°€ì¤‘ì¹˜ (ìˆ˜ìµë¥  ì˜ˆì¸¡ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        total_loss = (pos_loss + lev_loss + hold_loss) + 2.0 * profit_loss + entropy_reg
        
        return total_loss
    
    
    def update_target_network(self):
        """íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def save_model(self, filepath: str) -> bool:
        """ëª¨ë¸ ì €ì¥"""
        try:
            os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)
            
            save_dict = {
                'q_network': self.q_network.state_dict(),
                'target_network': self.target_network.state_dict(),
                'optimizer': self.optimizer.state_dict(),
                'epsilon': float(self.epsilon),
                'training_rewards': [float(r) for r in self.training_rewards],
                'losses': [float(l) for l in self.losses],
                'win_rates': [float(w) for w in self.win_rates],
                'update_count': int(self.update_count),
                'state_size': int(self.state_size)
            }
            
            torch.save(save_dict, filepath)
            print(f"58ì°¨ì› ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")
            return True
            
        except Exception as e:
            print(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def load_model(self, filepath: str) -> bool:
        """ëª¨ë¸ ë¡œë“œ"""
        if not os.path.exists(filepath):
            print(f"ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {filepath}")
            return False
        
        try:
            try:
                checkpoint = torch.load(filepath, map_location=self.device, weights_only=True)
            except:
                checkpoint = torch.load(filepath, map_location=self.device, weights_only=False)
            
            # ìƒíƒœ í¬ê¸° í™•ì¸
            model_state_size = checkpoint.get('state_size', 60)
            if model_state_size != self.state_size:
                print(f"âŒ ëª¨ë¸ ì°¨ì› ë¶ˆì¼ì¹˜: ê¸°ëŒ€ {self.state_size}, ì‹¤ì œ {model_state_size}")
                return False
            
            self.q_network.load_state_dict(checkpoint['q_network'])
            self.target_network.load_state_dict(checkpoint['target_network'])
            self.optimizer.load_state_dict(checkpoint['optimizer'])
            self.epsilon = checkpoint.get('epsilon', self.epsilon)
            self.training_rewards = checkpoint.get('training_rewards', [])
            self.losses = checkpoint.get('losses', [])
            self.win_rates = checkpoint.get('win_rates', [])
            self.update_count = checkpoint.get('update_count', 0)
            
            print(f"âœ… 58ì°¨ì› ëª¨ë¸ ë¡œë“œ ì„±ê³µ! ì—¡ì‹¤ë¡ : {self.epsilon:.3f}")
            return True
            
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def load_model_with_compatibility(self, filepath: str) -> bool:
        """í˜¸í™˜ì„±ì„ ê³ ë ¤í•œ ëª¨ë¸ ë¡œë“œ (êµ¬ì¡° ì°¨ì´ ë¬´ì‹œ)"""
        if not os.path.exists(filepath):
            print(f"ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {filepath}")
            return False
        
        try:
            checkpoint = torch.load(filepath, map_location=self.device, weights_only=False)
            
            # ìƒíƒœ í¬ê¸° í™•ì¸
            model_state_size = checkpoint.get('state_size', 60)
            print(f"ê¸°ì¡´ ëª¨ë¸ ì°¨ì›: {model_state_size}ì°¨ì›")
            
            # ê¸°ì¡´ ëª¨ë¸ê³¼ í˜„ì¬ ëª¨ë¸ì˜ êµ¬ì¡°ê°€ ì™„ì „íˆ ë‹¤ë¥´ë¯€ë¡œ
            # í˜¸í™˜ ê°€ëŠ¥í•œ ë¶€ë¶„ë§Œ ë¡œë“œí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ìƒˆë¡œ ì´ˆê¸°í™”
            print("âš ï¸ ëª¨ë¸ êµ¬ì¡°ê°€ ë‹¤ë¦…ë‹ˆë‹¤. í˜¸í™˜ ê°€ëŠ¥í•œ ë¶€ë¶„ë§Œ ë¡œë“œí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ìƒˆë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            
            # í˜„ì¬ ëª¨ë¸ì˜ state_dict
            current_state_dict = self.q_network.state_dict()
            loaded_state_dict = checkpoint['q_network']
            
            # í˜¸í™˜ ê°€ëŠ¥í•œ ë ˆì´ì–´ë§Œ ë¡œë“œ
            compatible_state_dict = {}
            loaded_count = 0
            initialized_count = 0
            
            for key in current_state_dict.keys():
                if key in loaded_state_dict:
                    # í¬ê¸°ê°€ ê°™ì€ ê²½ìš°ë§Œ ë¡œë“œ
                    if current_state_dict[key].shape == loaded_state_dict[key].shape:
                        compatible_state_dict[key] = loaded_state_dict[key]
                        print(f"   âœ… {key}: ë¡œë“œë¨")
                        loaded_count += 1
                    else:
                        compatible_state_dict[key] = current_state_dict[key]
                        print(f"   âš ï¸ {key}: í¬ê¸° ë¶ˆì¼ì¹˜ ({loaded_state_dict[key].shape} â†’ {current_state_dict[key].shape}), ìƒˆë¡œ ì´ˆê¸°í™”")
                        initialized_count += 1
                else:
                    compatible_state_dict[key] = current_state_dict[key]
                    print(f"   âŒ {key}: ëˆ„ë½, ìƒˆë¡œ ì´ˆê¸°í™”")
                    initialized_count += 1
            
            # ëˆ„ë½ëœ ë ˆì´ì–´ë“¤ í™•ì¸
            missing_in_current = set(loaded_state_dict.keys()) - set(current_state_dict.keys())
            if missing_in_current:
                print(f"   ğŸ“ í˜„ì¬ ëª¨ë¸ì— ì—†ëŠ” ë ˆì´ì–´ë“¤: {len(missing_in_current)}ê°œ")
                for key in sorted(missing_in_current):
                    print(f"      - {key}: {loaded_state_dict[key].shape}")
            
            # í˜¸í™˜ ê°€ëŠ¥í•œ state_dictë¡œ ë¡œë“œ
            try:
                self.q_network.load_state_dict(compatible_state_dict)
                self.target_network.load_state_dict(compatible_state_dict)
                print("   âœ… ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"   âŒ ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
                # ë¶€ë¶„ì  ë¡œë“œë¥¼ ì‹œë„
                try:
                    self.q_network.load_state_dict(compatible_state_dict, strict=False)
                    self.target_network.load_state_dict(compatible_state_dict, strict=False)
                    print("   âš ï¸ ë¶€ë¶„ì  ë¡œë“œë¡œ ë³µêµ¬ë¨")
                except Exception as e2:
                    print(f"   âŒ ë¶€ë¶„ì  ë¡œë“œë„ ì‹¤íŒ¨: {e2}")
                    return False
            
            # ì˜µí‹°ë§ˆì´ì € ë¡œë“œ (í˜¸í™˜ì„± í™•ì¸)
            try:
                if 'optimizer' in checkpoint:
                    self.optimizer.load_state_dict(checkpoint['optimizer'])
                    print("   âœ… ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¡œë“œë¨")
                else:
                    print("   âš ï¸ ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
            except Exception as e:
                print(f"   âš ï¸ ì˜µí‹°ë§ˆì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
            
            # ê¸°íƒ€ íŒŒë¼ë¯¸í„°ë“¤
            self.epsilon = checkpoint.get('epsilon', self.epsilon)
            self.training_rewards = checkpoint.get('training_rewards', [])
            self.losses = checkpoint.get('losses', [])
            self.win_rates = checkpoint.get('win_rates', [])
            self.update_count = checkpoint.get('update_count', 0)
            
            print(f"âœ… {model_state_size}ì°¨ì› â†’ 58ì°¨ì› í˜¸í™˜ì„± ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
            print(f"   - ë¡œë“œëœ ë ˆì´ì–´: {loaded_count}ê°œ")
            print(f"   - ìƒˆë¡œ ì´ˆê¸°í™”ëœ ë ˆì´ì–´: {initialized_count}ê°œ")
            
            # ëª¨ë¸ ê²€ì¦
            try:
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¡œ ëª¨ë¸ì´ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
                test_input = torch.randn(1, self.state_size).to(self.device)
                with torch.no_grad():
                    _ = self.q_network(test_input)
                print("   âœ… ëª¨ë¸ ê²€ì¦ ì™„ë£Œ - ì •ìƒ ì‘ë™")
            except Exception as e:
                print(f"   âš ï¸ ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
                print("   ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆì§€ë§Œ ì˜ˆìƒê³¼ ë‹¤ë¥´ê²Œ ì‘ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            return True
            
        except Exception as e:
            print(f"âŒ í˜¸í™˜ì„± ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"   ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            import traceback
            print(f"   ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False

    def create_compatible_model(self, old_model_path: str) -> bool:
        """ê¸°ì¡´ ëª¨ë¸ì„ SimpleDQN ì•„í‚¤í…ì²˜ë¡œ ë³€í™˜"""
        try:
            print(f"ğŸ”„ ê¸°ì¡´ ëª¨ë¸ì„ SimpleDQN ì•„í‚¤í…ì²˜ë¡œ ë³€í™˜ ì¤‘...")
            
            # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ
            checkpoint = torch.load(old_model_path, map_location=self.device, weights_only=False)
            old_state_dict = checkpoint['q_network']
            
            # ìƒˆë¡œìš´ DuelingDQN ëª¨ë¸ ìƒì„±
            new_model = DuelingDQN(self.state_size, 3, self.hidden_size).to(self.device)
            new_state_dict = new_model.state_dict()
            
            # í˜¸í™˜ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë§Œ ë³µì‚¬
            compatible_weights = {}
            for key in new_state_dict.keys():
                if key in old_state_dict and new_state_dict[key].shape == old_state_dict[key].shape:
                    compatible_weights[key] = old_state_dict[key]
                    print(f"   âœ… {key}: ë³€í™˜ë¨")
                else:
                    compatible_weights[key] = new_state_dict[key]
                    print(f"   âŒ {key}: ìƒˆë¡œ ì´ˆê¸°í™”")
            
            # ìƒˆë¡œìš´ ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ë¡œë“œ
            new_model.load_state_dict(compatible_weights)
            
            # í˜„ì¬ ì—ì´ì „íŠ¸ì˜ ë„¤íŠ¸ì›Œí¬ êµì²´
            self.q_network = new_model
            self.target_network = DuelingDQN(self.state_size, 3, self.hidden_size).to(self.device)
            self.target_network.load_state_dict(compatible_weights)
            
            print(f"âœ… DuelingDQN ì•„í‚¤í…ì²˜ë¡œ ë³€í™˜ ì™„ë£Œ!")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return False

    def diagnose_model_compatibility(self, model_path: str) -> Dict:
        """ëª¨ë¸ í˜¸í™˜ì„± ì§„ë‹¨"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
            old_state_dict = checkpoint['q_network']
            current_state_dict = self.q_network.state_dict()
            
            diagnosis = {
                'total_old_layers': len(old_state_dict),
                'total_new_layers': len(current_state_dict),
                'compatible_layers': 0,
                'incompatible_layers': 0,
                'missing_layers': 0,
                'compatibility_rate': 0.0
            }
            
            for key in current_state_dict.keys():
                if key in old_state_dict:
                    if current_state_dict[key].shape == old_state_dict[key].shape:
                        diagnosis['compatible_layers'] += 1
                    else:
                        diagnosis['incompatible_layers'] += 1
                else:
                    diagnosis['missing_layers'] += 1
            
            diagnosis['compatibility_rate'] = diagnosis['compatible_layers'] / len(current_state_dict)
            
            return diagnosis
            
        except Exception as e:
            return {'error': str(e)}

class DataLoader:
    """58ì°¨ì› RL Decision ê¸°ë°˜ ë°ì´í„° ë¡œë”© í´ë˜ìŠ¤"""
    
    @staticmethod
    def load_price_data(file_path: str = 'data/ETHUSDC_3m_historical_data.csv') -> Optional[pd.DataFrame]:
        """ê°€ê²© ë°ì´í„° ë¡œë“œ"""
        try:
            required_columns = ['open', 'high', 'low', 'close', 'volume', 'quote_volume']
            
            df = pd.read_csv(file_path)
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.set_index('timestamp')
            df = df[required_columns]
            
            price_data = df.reset_index()
            print(f"ê°€ê²© ë°ì´í„° ë¡œë“œ: {len(price_data):,}ê°œ ìº”ë“¤")
            return price_data
            
        except Exception as e:
            print(f"ê°€ê²© ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    @staticmethod
    def load_signal_data(agent_folder: str = "agent") -> Optional[List[Dict]]:
        """58ì°¨ì›ìš© RL Decision ë°ì´í„° ë¡œë“œ"""
        parquet_files = []
        
        if Path(agent_folder).exists():
            parquet_files = list(Path(agent_folder).glob("*.parquet"))
        
        if parquet_files:
            try:
                print(f"Signal ë°ì´í„° ë¡œë“œ ì¤‘: {parquet_files[0].name}")
                signal_df = pd.read_parquet(parquet_files[0])
                print(f"Signal ë°ì´í„° ë¡œë“œ: {len(signal_df):,}ê°œ ë ˆì½”ë“œ")
                
                return DataLoader._convert_parquet_to_signal_dicts(signal_df)
                
            except Exception as e:
                print(f"Parquet ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print("Parquet íŒŒì¼ì´ ì—†ì–´ ê¸°ë³¸ Signalì„ ìƒì„±í•©ë‹ˆë‹¤.")
        return None
    
    @staticmethod
    def _convert_parquet_to_signal_dicts(signal_df: pd.DataFrame) -> List[Dict]:
        """Parquetì„ Signal Dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (58ì°¨ì›ìš©) - ìƒˆë¡œìš´ RL ìŠ¤í‚¤ë§ˆ"""
        signal_data = []
        
        print("58ì°¨ì›ìš© RL ìŠ¤í‚¤ë§ˆ Signal ë°ì´í„° ë³€í™˜ ì¤‘...")
        
        for idx, row in signal_df.iterrows():
            # ê° í–‰ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ìƒˆë¡œìš´ RL ìŠ¤í‚¤ë§ˆ í˜•íƒœ ìœ ì§€)
            signal_dict = {}
            
            for col, value in row.items():
                if pd.notna(value):
                    # ìˆ˜ì¹˜ ë°ì´í„°ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                    signal_dict[col] = value
                else:
                    # ê¸°ë³¸ê°’ ì„¤ì • (ìƒˆë¡œìš´ RL ìŠ¤í‚¤ë§ˆì— ë§ê²Œ)
                    if 'action_value' in col or 'net_score' in col or 'buy_score' in col or 'sell_score' in col:
                        signal_dict[col] = 0.0
                    elif 'confidence_value' in col or 'market_context_value' in col:
                        signal_dict[col] = 0.0
                    elif 'conflict_' in col:
                        signal_dict[col] = 0.0
                    elif 'leverage' in col or 'signals_used' in col or 'strategies_count' in col:
                        signal_dict[col] = 0
                    elif 'max_holding_minutes' in col:
                        signal_dict[col] = 0
                    else:
                        signal_dict[col] = 0.0
            
            signal_data.append(signal_dict)
            
            if (idx + 1) % 5000 == 0:
                print(f"   ë³€í™˜ ì§„í–‰: {idx + 1:,}/{len(signal_df):,}")
        
        print(f"58ì°¨ì›ìš© RL ìŠ¤í‚¤ë§ˆ Signal ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(signal_data):,}ê°œ")
        return signal_data
    

class PerformanceAnalyzer:
    """58ì°¨ì› RL Decision ê¸°ë°˜ ì„±ëŠ¥ ë¶„ì„ í´ë˜ìŠ¤"""
    
    @staticmethod
    def evaluate_agent(agent: RLAgent, env: TradingEnvironment, num_episodes: int = 10) -> Tuple[List[Dict], Dict]:
        """58ì°¨ì› ì—ì´ì „íŠ¸ ì„±ëŠ¥ í‰ê°€"""
        print(f"58ì°¨ì› ì—ì´ì „íŠ¸ ì„±ëŠ¥ í‰ê°€ ì¤‘ ({num_episodes} ì—í”¼ì†Œë“œ)...")
        
        original_epsilon = agent.epsilon
        agent.epsilon = 0.01  # í…ŒìŠ¤íŠ¸ì—ì„œë„ ì•½ê°„ì˜ íƒí—˜ í—ˆìš©
        
        results = []
        all_trades = []
        
        for episode in range(num_episodes):
            # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì™„ì „ ì´ˆê¸°í™”
            state, _ = env.reset()
            episode_reward = 0
            episode_trades = []
            episode_balance = env.initial_balance
            
            # í…ŒìŠ¤íŠ¸ í™˜ê²½ ìƒíƒœ í™•ì¸
            print(f"   ì—í”¼ì†Œë“œ {episode+1}: ì´ˆê¸° ì”ê³  ${episode_balance:.0f}")
            
            for step in range(500):
                action = agent.act(state)
                next_state, reward, done, truncated, info = env.step(action)
                
                episode_reward += reward
                episode_balance = info['balance']
                
                if info.get('trade_completed', False):
                    trade_pnl = info.get('trade_pnl', 0.0)
                    episode_trades.append(1 if trade_pnl > 0 else 0)
                
                state = next_state
                if done:
                    break
            
            episode_return = (episode_balance - env.initial_balance) / env.initial_balance
            win_rate = np.mean(episode_trades) if episode_trades else 0.0
            
            # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
            print(f"   ì—í”¼ì†Œë“œ {episode+1}: ê±°ë˜ {len(episode_trades)}ê°œ, ìŠ¹ë¥  {win_rate:.1%}, ìˆ˜ìµë¥  {episode_return:.1%}, ì”ê³  ${episode_balance:.0f}")
            
            results.append({
                'episode': episode,
                'total_reward': episode_reward,
                'final_balance': episode_balance,
                'return': episode_return,
                'win_rate': win_rate,
                'total_trades': len(episode_trades),
                'max_drawdown': info.get('max_drawdown', 0.0)
            })
            
            all_trades.extend(episode_trades)
        
        agent.epsilon = original_epsilon
        
        overall_stats = {
            'avg_return': np.mean([r['return'] for r in results]),
            'avg_reward': np.mean([r['total_reward'] for r in results]),
            'overall_win_rate': np.mean(all_trades) if all_trades else 0.0,
            'avg_trades_per_episode': np.mean([r['total_trades'] for r in results]),
            'avg_max_drawdown': np.mean([r['max_drawdown'] for r in results]),
            'consistency': 1.0 - np.std([r['return'] for r in results]) if len(results) > 1 else 1.0,
            'total_trades': len(all_trades),
            'model_dimension': agent.state_size
        }
        
        return results, overall_stats
    
    @staticmethod
    def print_performance_report(results: List[Dict], stats: Dict):
        """58ì°¨ì› ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        print("\n" + "="*60)
        print(f"58ì°¨ì› RL Decision ê¸°ë°˜ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼")
        print("="*60)
        print(f"ëª¨ë¸ ì°¨ì›: {stats['model_dimension']}ì°¨ì›")
        print(f"ì „ì²´ ìŠ¹ë¥ : {stats['overall_win_rate']:.3f}")
        print(f"í‰ê·  ìˆ˜ìµë¥ : {stats['avg_return']:.3f} ({stats['avg_return']*100:.1f}%)")
        print(f"í‰ê·  ë¦¬ì›Œë“œ: {stats['avg_reward']:.1f}")
        print(f"ì—í”¼ì†Œë“œë‹¹ í‰ê·  ê±°ë˜ ìˆ˜: {stats['avg_trades_per_episode']:.1f}")
        print(f"í‰ê·  ìµœëŒ€ ë‚™í­: {stats['avg_max_drawdown']:.3f}")
        print(f"ì„±ê³¼ ì¼ê´€ì„±: {stats['consistency']:.3f}")
        print(f"ì´ ê±°ë˜ ìˆ˜: {stats['total_trades']}")
        
        grade = PerformanceAnalyzer._get_performance_grade(stats)
        print(f"\nì„±ëŠ¥ ë“±ê¸‰: {grade}")
        
        recommendations = PerformanceAnalyzer._get_recommendations(stats)
        print("\nê°œì„  ì œì•ˆ:")
        for rec in recommendations:
            print(f"  â€¢ {rec}")
    
    @staticmethod
    def _get_performance_grade(stats: Dict) -> str:
        """ì„±ëŠ¥ ë“±ê¸‰ ì‚°ì¶œ (ìˆ˜ìµë¥  ì¤‘ì‹¬)"""
        avg_return = stats['avg_return']
        consistency = stats['consistency']
        win_rate = stats['overall_win_rate']  # ë³´ì¡° ì§€í‘œ
        
        score = 0
        
        # ìˆ˜ìµë¥ ì´ ì£¼ìš” í‰ê°€ ê¸°ì¤€ (ê°€ì¤‘ì¹˜ ë†’ìŒ)
        if avg_return >= 0.20: score += 5  # 20% ì´ìƒ
        elif avg_return >= 0.15: score += 4  # 15% ì´ìƒ
        elif avg_return >= 0.10: score += 3  # 10% ì´ìƒ
        elif avg_return >= 0.05: score += 2  # 5% ì´ìƒ
        elif avg_return >= 0.0: score += 1   # 0% ì´ìƒ
        
        # ì¼ê´€ì„± (ì¤‘ìš”í•˜ì§€ë§Œ ìˆ˜ìµë¥ ë³´ë‹¤ ë‚®ì€ ê°€ì¤‘ì¹˜)
        if consistency >= 0.8: score += 2
        elif consistency >= 0.6: score += 1
        
        # ìŠ¹ë¥  (ë³´ì¡° ì§€í‘œ)
        if win_rate >= 0.6: score += 1
        
        grades = {8: "A+ (ìš°ìˆ˜)", 7: "A (ì¢‹ìŒ)", 6: "B+ (ì–‘í˜¸)", 5: "B (ë³´í†µ)", 
                 4: "C+ (ë¯¸í¡)", 3: "C (ê°œì„ í•„ìš”)", 2: "D (ë‚˜ì¨)", 1: "F (ë§¤ìš°ë‚˜ì¨)", 0: "F (ì‹¤íŒ¨)"}
        
        return grades.get(score, "F (ì‹¤íŒ¨)")
    
    @staticmethod
    def _get_recommendations(stats: Dict) -> List[str]:
        """ì„±ëŠ¥ ê¸°ë°˜ ê°œì„  ì œì•ˆ (ìˆ˜ìµë¥  ì¤‘ì‹¬)"""
        recommendations = []
        
        # ìˆ˜ìµë¥ ì´ ì£¼ìš” ê¸°ì¤€
        if stats['avg_return'] < 0.05:
            recommendations.append("ìˆ˜ìµë¥ ì´ 5% ë¯¸ë§Œì…ë‹ˆë‹¤. ìˆ˜ìµë¥  ì¤‘ì‹¬ ë³´ìƒ í•¨ìˆ˜ë¥¼ ë” ê°•í™”í•˜ì„¸ìš”.")
        
        if stats['avg_return'] < 0.10:
            recommendations.append("ìˆ˜ìµë¥ ì´ 10% ë¯¸ë§Œì…ë‹ˆë‹¤. 58ì°¨ì› ìƒíƒœ ê³µê°„ì˜ ìˆ˜ìµë¥  ìµœì í™”ë¥¼ ë” í™œìš©í•˜ì„¸ìš”.")
        
        if stats['avg_return'] < 0.15:
            recommendations.append("ìˆ˜ìµë¥ ì´ 15% ë¯¸ë§Œì…ë‹ˆë‹¤. Signal ê¸°ë°˜ ìˆ˜ìµë¥  ì˜ˆì¸¡ì„ ê°œì„ í•˜ì„¸ìš”.")
        
        # ë¦¬ìŠ¤í¬ ê´€ë¦¬
        if stats['avg_max_drawdown'] > 0.2:
            recommendations.append("ìµœëŒ€ ë‚™í­ì´ í½ë‹ˆë‹¤. ìˆ˜ìµë¥ ê³¼ ë¦¬ìŠ¤í¬ì˜ ê· í˜•ì„ ë§ì¶”ì„¸ìš”.")
        
        # ì¼ê´€ì„±
        if stats['consistency'] < 0.5:
            recommendations.append("ì„±ê³¼ ì¼ê´€ì„±ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤. ìˆ˜ìµë¥  ì•ˆì •ì„±ì„ ìœ„í•œ ë” ë§ì€ í›ˆë ¨ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ê±°ë˜ ë¹ˆë„
        if stats['avg_trades_per_episode'] < 3:
            recommendations.append("ê±°ë˜ ë¹ˆë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ìˆ˜ìµë¥  ê¸°íšŒë¥¼ ë†“ì¹˜ì§€ ì•Šë„ë¡ Signal ê°ë„ë¥¼ ì¡°ì •í•˜ì„¸ìš”.")
        
        # ìŠ¹ë¥ ì€ ë³´ì¡° ì§€í‘œ
        if stats['overall_win_rate'] < 0.4:
            recommendations.append("ìŠ¹ë¥ ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤. ìˆ˜ìµë¥ ê³¼ ìŠ¹ë¥ ì˜ ê· í˜•ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        if not recommendations:
            recommendations.append("58ì°¨ì› RL Decision ê¸°ë°˜ ìˆ˜ìµë¥  ì¤‘ì‹¬ ì‹œìŠ¤í…œì´ ì˜ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤!")
        
        return recommendations

class TrainingManager:
    """58ì°¨ì› RL Decision ê¸°ë°˜ í›ˆë ¨ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def train_agent(agent: RLAgent, train_env: TradingEnvironment, 
                   episodes: int = 1000, save_interval: int = 100, 
                   test_env: TradingEnvironment = None) -> Tuple[RLAgent, List[float], List[float]]:
        """58ì°¨ì› RL Decision ê¸°ë°˜ ì—ì´ì „íŠ¸ í›ˆë ¨ (í…ŒìŠ¤íŠ¸ í™˜ê²½ ëª¨ë‹ˆí„°ë§ í¬í•¨)"""
        print(f"58ì°¨ì› RL Decision ê¸°ë°˜ ê°•í™”í•™ìŠµ í›ˆë ¨ ì‹œì‘ ({episodes} ì—í”¼ì†Œë“œ)")
        print(f"ìƒíƒœ ê³µê°„: {train_env.observation_space.shape[0]}ì°¨ì›")
        if test_env:
            print(f"í…ŒìŠ¤íŠ¸ í™˜ê²½ ëª¨ë‹ˆí„°ë§: í™œì„±í™”")
        
        # ì—ì´ì „íŠ¸ì— ì•¡ì…˜ ê³µê°„ ì„¤ì •
        agent.action_space = train_env.action_space
        
        episode_rewards = []
        episode_win_rates = []
        episode_returns = []  # í›ˆë ¨ ë°ì´í„° ìˆ˜ìµë¥  ì¶”ì 
        test_win_rates = []  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìŠ¹ë¥  ì¶”ì 
        best_win_rate = 0.0
        best_test_win_rate = 0.0
        
        for episode in range(episodes):
            state, _ = train_env.reset()
            total_reward = 0
            episode_trades = []
            steps = 0
            
            while steps < 1000:
                action = agent.act(state)
                next_state, reward, done, truncated, info = train_env.step(action)
                
                agent.remember(state, action, reward, next_state, done)
                
                if info.get('trade_completed', False):
                    trade_pnl = info.get('trade_pnl', 0.0)
                    episode_trades.append(1 if trade_pnl > 0 else 0)
                
                state = next_state
                total_reward += reward
                steps += 1
                
                if len(agent.memory) > agent.batch_size:
                    agent.replay()
                
                if done:
                    break
            
            episode_rewards.append(total_reward)
            episode_win_rate = np.mean(episode_trades) if episode_trades else 0.0
            episode_win_rates.append(episode_win_rate)
            
            # í›ˆë ¨ ë°ì´í„° ìˆ˜ìµë¥  ê³„ì‚° (ì”ê³  ë³€í™” ê¸°ë°˜)
            initial_balance = train_env.initial_balance  # í™˜ê²½ì˜ ì‹¤ì œ ì´ˆê¸° ì”ê³  ì‚¬ìš©
            final_balance = info.get('balance', initial_balance)
            episode_return = (final_balance - initial_balance) / initial_balance
            episode_returns.append(episode_return)
            
            agent.training_rewards.append(total_reward)
            agent.win_rates.append(episode_win_rate)
            
            
            # ì ì‘í˜• í•™ìŠµë¥  ì—…ë°ì´íŠ¸ (10ì—í”¼ì†Œë“œë§ˆë‹¤)
            if episode % 10 == 0 and episode > 0:
                recent_rewards = episode_rewards[-20:] if len(episode_rewards) >= 20 else episode_rewards
                recent_win_rates = episode_win_rates[-20:] if len(episode_win_rates) >= 20 else episode_win_rates
                agent.adaptive_learning_rate(recent_rewards, recent_win_rates)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€ (ê³¼ì í•© ë°©ì§€ ê°•í™”)
            if test_env and episode % 5 == 0 and episode > 0:  # ë” ìì£¼ í‰ê°€
                print(f"\nğŸ“Š Episode {episode}: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
                test_results, test_stats = PerformanceAnalyzer.evaluate_agent(agent, test_env, num_episodes=5)  # ë” ë§ì€ ì—í”¼ì†Œë“œë¡œ í‰ê°€
                test_return = test_stats['avg_return']
                test_win_rates.append(test_stats['overall_win_rate'])
                
                print(f"   í…ŒìŠ¤íŠ¸ ìˆ˜ìµë¥ : {test_return:.3f} ({test_return*100:.1f}%) (ì´ì „ ìµœê³ : {best_test_win_rate:.3f})")
                
                # ê³¼ì í•© ê°ì§€: í›ˆë ¨ ìˆ˜ìµë¥ ê³¼ í…ŒìŠ¤íŠ¸ ìˆ˜ìµë¥  ì°¨ì´ í™•ì¸
                recent_train_return = np.mean(episode_returns[-10:]) if len(episode_returns) >= 10 else 0.0
                overfitting_gap = recent_train_return - (test_return if test_return > 0 else -test_return)
                
                if overfitting_gap > 0.1:  # í›ˆë ¨ ìˆ˜ìµë¥ ì´ í…ŒìŠ¤íŠ¸ ìˆ˜ìµë¥ ë³´ë‹¤ 10% ì´ìƒ ë†’ìœ¼ë©´ ê³¼ì í•© ì˜ì‹¬
                    print(f"âš ï¸ ê³¼ì í•© ê°ì§€: í›ˆë ¨ ìˆ˜ìµë¥ ({recent_train_return:.3f}) - í…ŒìŠ¤íŠ¸ ìˆ˜ìµë¥ ({test_return:.3f}) = {overfitting_gap:.3f}")
                    # í•™ìŠµë¥  ê°ì†Œ
                    for param_group in agent.optimizer.param_groups:
                        param_group['lr'] *= 0.9
                    print(f"   í•™ìŠµë¥  ê°ì†Œ: {agent.optimizer.param_groups[0]['lr']:.2e}")
                
                if test_return > best_test_win_rate:
                    best_test_win_rate = test_return
                    # ì—í”¼ì†Œë“œë³„ ëª¨ë¸ ì €ì¥ (ìˆ˜ìµë¥  ê¸°ì¤€)
                    agent.save_model(f'best_test_model_ep{episode}_return{test_return:.3f}.pth')
                    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì—…ë°ì´íŠ¸ (ìˆ˜ìµë¥  ê¸°ì¤€)
                    agent.save_model('agent/best_test_performance_model_return{:.3f}.pth'.format(test_return))
                    print(f"ğŸ¯ ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìµœê³  ìˆ˜ìµë¥ ! ìˆ˜ìµë¥ : {test_return:.3f} ({test_return*100:.1f}%)")
                    print(f"   ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì—…ë°ì´íŠ¸: best_test_performance_model_return{test_return:.3f}.pth")
                print()  # ë¹ˆ ì¤„ ì¶”ê°€
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥ (ë” ìì£¼)
            if episode % 5 == 0 or episode < 10:
                recent_rewards = episode_rewards[-50:] if len(episode_rewards) >= 50 else episode_rewards
                recent_win_rates = episode_win_rates[-50:] if len(episode_win_rates) >= 50 else episode_win_rates
                recent_returns = episode_returns[-50:] if len(episode_returns) >= 50 else episode_returns
                
                avg_reward = np.mean(recent_rewards)
                avg_win_rate = np.mean(recent_win_rates)
                avg_return = np.mean(recent_returns)
                
                # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ë„ í•¨ê»˜ í‘œì‹œ
                test_info = ""
                if test_win_rates:
                    recent_test_win_rate = np.mean(test_win_rates[-5:]) if len(test_win_rates) >= 5 else test_win_rates[-1]
                    test_info = f" | í…ŒìŠ¤íŠ¸: {recent_test_win_rate:.3f}"
                
                # ìˆ˜ìµë¥ ê³¼ ë¦¬ì›Œë“œ ì¼ì¹˜ì„± í™•ì¸
                reward_return_ratio = avg_reward / (avg_return * 100) if avg_return != 0 else 0
                
                print(f"Episode {episode:4d} | "
                        f"í›ˆë ¨ìŠ¹ë¥ : {avg_win_rate:.3f} | "
                        f"í›ˆë ¨ìˆ˜ìµë¥ : {avg_return:.3f} ({avg_return*100:+.1f}%){test_info} | "
                        f"ë¦¬ì›Œë“œ: {avg_reward:7.1f} | "
                        f"ì”ê³ : ${info['balance']:7.0f} | "
                        f"ê±°ë˜: {info.get('total_trades', 0):3d}ê°œ | "
                        f"Îµ: {agent.epsilon:.3f} | "
                        f"LR: {agent.learning_rate:.2e} | "
                        f"58D")
            
            # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥ (í›ˆë ¨ ë°ì´í„° ê¸°ì¤€ - ìˆ˜ìµë¥  ì¤‘ì‹¬)
            if episode % save_interval == 0 and episode > 0:
                # ìµœê·¼ 100 ì—í”¼ì†Œë“œì˜ í‰ê·  ìˆ˜ìµë¥  ê³„ì‚°
                recent_returns = []
                for i in range(max(0, len(episode_rewards)-100), len(episode_rewards)):
                    if i < len(episode_rewards):
                        # ê°„ë‹¨í•œ ìˆ˜ìµë¥  ì¶”ì • (ë¦¬ì›Œë“œ ê¸°ë°˜)
                        estimated_return = episode_rewards[i] / 1000.0  # ë¦¬ì›Œë“œë¥¼ ìˆ˜ìµë¥ ë¡œ ê·¼ì‚¬
                        recent_returns.append(estimated_return)
                
                current_avg_return = np.mean(recent_returns) if recent_returns else 0.0
                
                if current_avg_return > best_win_rate:  # ë³€ìˆ˜ëª…ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì§€ë§Œ ìˆ˜ìµë¥ ë¡œ ì‚¬ìš©
                    best_win_rate = current_avg_return
                    agent.save_model(f'best_train_model_ep{episode}_return{current_avg_return:.3f}.pth')
                    print(f"ğŸ¯ ìƒˆë¡œìš´ í›ˆë ¨ ë°ì´í„°ì…‹ ìµœê³  ìˆ˜ìµë¥ ! ìˆ˜ìµë¥ : {current_avg_return:.3f} ({current_avg_return*100:.1f}%)")
            
            # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ (ê³¼ì í•© ë°©ì§€ ê°•í™”)
            if episode > 500 and test_win_rates:
                # ìµœê·¼ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë“¤ì˜ ìˆ˜ìµë¥  í™•ì¸
                recent_test_returns = []
                for i in range(max(0, len(test_win_rates)-5), len(test_win_rates)):
                    if i < len(test_win_rates):
                        # í…ŒìŠ¤íŠ¸ ìˆ˜ìµë¥  ì¶”ì • (ìŠ¹ë¥ ì„ ìˆ˜ìµë¥ ë¡œ ê·¼ì‚¬)
                        estimated_return = test_win_rates[i] * 0.1  # ìŠ¹ë¥  65% = ìˆ˜ìµë¥  6.5%ë¡œ ê·¼ì‚¬
                        recent_test_returns.append(estimated_return)
                
                recent_test_return = np.mean(recent_test_returns) if recent_test_returns else 0.0
                
                # ê³¼ì í•© ê°ì§€ ì‹œ ì¡°ê¸° ì¢…ë£Œ
                if len(episode_returns) >= 20:
                    recent_train_return = np.mean(episode_returns[-20:])
                    overfitting_gap = recent_train_return - recent_test_return
                    
                    if overfitting_gap > 0.15:  # ê³¼ì í•©ì´ ì‹¬í•˜ë©´ ì¡°ê¸° ì¢…ë£Œ
                        print(f"ğŸ›‘ ê³¼ì í•©ìœ¼ë¡œ ì¸í•œ ì¡°ê¸° ì¢…ë£Œ: í›ˆë ¨ ìˆ˜ìµë¥ ({recent_train_return:.3f}) - í…ŒìŠ¤íŠ¸ ìˆ˜ìµë¥ ({recent_test_return:.3f}) = {overfitting_gap:.3f}")
                        agent.save_model('agent/early_stop_model.pth')
                        break
                
                if recent_test_return >= 0.20:  # ìˆ˜ìµë¥  20% ì´ìƒ ë‹¬ì„±
                    print(f"ğŸ† 58ì°¨ì› ëª©í‘œ ë‹¬ì„±! í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìˆ˜ìµë¥  {recent_test_return:.3f} ({recent_test_return*100:.1f}%) ë„ë‹¬")
                    agent.save_model('agent/final_optimized_model_58d.pth')
                    break
        
        
        print(f"\n58ì°¨ì› RL Decision ê¸°ë°˜ í›ˆë ¨ ì™„ë£Œ!")
        print(f"   ì´ ì—í”¼ì†Œë“œ: {episode + 1}")
        print(f"   í›ˆë ¨ ë°ì´í„° ìµœê³  ìŠ¹ë¥ : {best_win_rate:.3f}")
        print(f"   í›ˆë ¨ ë°ì´í„° ìµœì¢… ìŠ¹ë¥ : {np.mean(episode_win_rates[-50:]) if episode_win_rates else 0:.3f}")
        if test_win_rates:
            print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„° ìµœê³  ìŠ¹ë¥ : {best_test_win_rate:.3f}")
            print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„° ìµœì¢… ìŠ¹ë¥ : {test_win_rates[-1]:.3f}")
        print(f"   ìƒíƒœ ì°¨ì›: 58ì°¨ì› (RL Decision ê¸°ë°˜)")
        print(f"   ì•„í‚¤í…ì²˜: DuelingDQN (Value + Advantage ë¶„ë¦¬)")
        print(f"   ì •ê·œí™” ê¸°ë²•: ì—”íŠ¸ë¡œí”¼ ì •ê·œí™”, Spectral Normalization, ì ì‘ì  ë“œë¡­ì•„ì›ƒ")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if test_win_rates and best_test_win_rate > 0:
            best_test_model_path = f'agent/best_test_performance_model_wr{best_test_win_rate:.3f}.pth'
            agent.save_model(best_test_model_path)
            print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {best_test_model_path}")
        
        return agent, episode_rewards, episode_win_rates

def synchronize_data_by_timestamp(price_data: pd.DataFrame, signal_data: List[Dict]) -> Tuple[pd.DataFrame, List[Dict]]:
    """íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ìœ¼ë¡œ Priceì™€ Signal ë°ì´í„° ë™ê¸°í™”"""
    print("íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ ë°ì´í„° ë™ê¸°í™” ì¤‘...")
    
    # Signal ë°ì´í„°ì˜ ì‹œì‘ê³¼ ë íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ìœ¼ë¡œ Price ë°ì´í„° ìŠ¬ë¼ì´ì‹±
    if not signal_data or 'timestamp' not in signal_data[0]:
        print("Signal ë°ì´í„°ì— íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ë™ê¸°í™”í•©ë‹ˆë‹¤.")
        min_length = len(signal_data)
        price_data = price_data.iloc[-min_length:].reset_index(drop=True)
        signal_data = signal_data[-min_length:]
        print(f"ê¸¸ì´ ê¸°ì¤€ ë™ê¸°í™” ì™„ë£Œ: {min_length:,}ê°œ")
        return price_data, signal_data
    
    signal_start_time = signal_data[0]['timestamp']
    signal_end_time = signal_data[-1]['timestamp']
        
    if hasattr(signal_end_time, 'timestamp'):
        signal_end_timestamp = signal_end_time.timestamp()
    elif isinstance(signal_end_time, str):
        signal_end_timestamp = pd.to_datetime(signal_end_time).timestamp()
    else:
        signal_end_timestamp = float(signal_end_time)
    
    # Price ë°ì´í„°ì—ì„œ Signal ì‹œì‘/ë ì‹œê°„ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì¸ë±ìŠ¤ ì°¾ê¸°
    price_data['timestamp'] = pd.to_datetime(price_data['timestamp'])
    
    # ì •í™•í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ë§¤ì¹­
    start_matches = price_data[price_data['timestamp'] == signal_start_time]
    end_matches = price_data[price_data['timestamp'] == signal_end_time]
    
    if len(start_matches) == 0 or len(end_matches) == 0:
        print("âŒ ì •í™•í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ë§¤ì¹­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None, None
    
    # Signal ë°ì´í„°ì˜ ì •í™•í•œ íƒ€ì„ìŠ¤íƒ¬í”„ì— ë§ì¶° Price ë°ì´í„° í•„í„°ë§
    # Signal ë°ì´í„°ì— ìˆëŠ” íƒ€ì„ìŠ¤íƒ¬í”„ë§Œ Price ë°ì´í„°ì—ì„œ ì„ íƒ
    signal_timestamps = set(signal['timestamp'] for signal in signal_data)
    price_data = price_data[price_data['timestamp'].isin(signal_timestamps)].reset_index(drop=True)
    
    # ë™ê¸°í™” ê²€ì¦
    price_start = price_data.iloc[0]['timestamp']
    price_end = price_data.iloc[-1]['timestamp']
    signal_start = signal_data[0]['timestamp']
    signal_end = signal_data[-1]['timestamp']
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ì •í™•í•œ ì¼ì¹˜ í™•ì¸
    start_time_match = (price_start == signal_start)
    end_time_match = (price_end == signal_end)
    length_match = (len(price_data) == len(signal_data))
    
    print(f"âœ… ì‹œì‘ ì‹œê°„ ë™ê¸°í™”: {'ì„±ê³µ' if start_time_match else 'ì‹¤íŒ¨'}")
    print(f"âœ… ë ì‹œê°„ ë™ê¸°í™”: {'ì„±ê³µ' if end_time_match else 'ì‹¤íŒ¨'}")
    print(f"   Price: {price_start} ~ {price_end}")
    print(f"   Signal: {signal_start} ~ {signal_end}")
    print(f"âœ… ê¸¸ì´ ë™ê¸°í™”: {'ì„±ê³µ' if length_match else 'ì‹¤íŒ¨'}")
    print(f"   Price: {len(price_data):,}ê°œ, Signal: {len(signal_data):,}ê°œ")
    
    if not (start_time_match and end_time_match and length_match):
        print("âŒ ë™ê¸°í™” ì‹¤íŒ¨! ë°ì´í„°ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.")
        return None, None
    
    return price_data, signal_data

def split_data(price_data: pd.DataFrame, signal_data: List[Dict], 
               train_ratio: float = 0.8, test_ratio: float = 0.2) -> Tuple[pd.DataFrame, List[Dict], pd.DataFrame, List[Dict]]:
    """ë°ì´í„°ë¥¼ í›ˆë ¨ìš©ê³¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¶„í• """
    total_length = min(len(price_data), len(signal_data))
    train_size = int(total_length * train_ratio)
    
    # í›ˆë ¨ ë°ì´í„°
    train_price = price_data.iloc[:train_size].reset_index(drop=True)
    train_signal = signal_data[:train_size]
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_price = price_data.iloc[train_size:].reset_index(drop=True)
    test_signal = signal_data[train_size:]
    
    print(f"ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
    print(f"  - í›ˆë ¨ ë°ì´í„°: {len(train_price):,}ê°œ ({train_ratio*100:.1f}%)")
    print(f"  - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_price):,}ê°œ ({test_ratio*100:.1f}%)")
    
    return train_price, train_signal, test_price, test_signal

def main():
    """58ì°¨ì› RL Decision ê¸°ë°˜ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("58ì°¨ì› RL Decision ê¸°ë°˜ ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œ")
    print("=" * 80)
    
    try:
        # 1. ë°ì´í„° ë¡œë”©
        print("\n1ï¸âƒ£ 58ì°¨ì›ìš© ë°ì´í„° ë¡œë”©...")
        price_data = DataLoader.load_price_data()
        if price_data is None:
            print("ê°€ê²© ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
            return
        
        signal_data = DataLoader.load_signal_data()
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë™ê¸°í™”
        price_data, signal_data = synchronize_data_by_timestamp(price_data, signal_data)
        if price_data is None or signal_data is None:
            print("ë°ì´í„° ë™ê¸°í™” ì‹¤íŒ¨ë¡œ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        
        # 2. ë°ì´í„° ë¶„í•  (í›ˆë ¨ 80%, í…ŒìŠ¤íŠ¸ 20%)
        print("\n2ï¸âƒ£ ë°ì´í„° ë¶„í• ...")
        train_price, train_signal, test_price, test_signal = split_data(price_data, signal_data, 0.8, 0.2)
        
        # 3. í™˜ê²½ ë° ì—ì´ì „íŠ¸ ìƒì„±
        print("\n3ï¸âƒ£ 58ì°¨ì› í™˜ê²½ ë° ì—ì´ì „íŠ¸ ìƒì„±...")
        train_env = TradingEnvironment(train_price, train_signal)
        test_env = TradingEnvironment(test_price, test_signal)
        agent = RLAgent(train_env.observation_space.shape[0])  # 58ì°¨ì›
        
        print(f"ìƒíƒœ ê³µê°„: {train_env.observation_space.shape[0]}ì°¨ì›")
        print("Signalì˜ ëª¨ë“  indicatorì™€ raw score í™œìš©")
        
        model_loaded = False
        
        # 1. í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ëª¨ë¸ ìš°ì„  ë¡œë“œ
        import glob
        test_model_files = glob.glob('agent/best_test_performance_model_return*.pth')
        if test_model_files:
            # ê°€ì¥ ë†’ì€ ìŠ¹ë¥ ì˜ í…ŒìŠ¤íŠ¸ ëª¨ë¸ ì„ íƒ
            best_test_model = max(test_model_files, key=lambda x: float(x.split('return')[1].split('.pth')[0]))
            if agent.load_model(best_test_model):
                model_loaded = True
                print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ: {best_test_model}")
        
        # 2. í˜¸í™˜ì„± ëª¨ë“œë¡œ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹œë„
        if not model_loaded:
            for model_file in ['agent/final_optimized_model_58d.pth', 'agent/best_model_58d.pth']:
                if os.path.exists(model_file):
                    print(f"ğŸ”„ í˜¸í™˜ì„± ëª¨ë“œë¡œ {model_file} ë¡œë“œ ì‹œë„...")
                    
                    # ëª¨ë¸ í˜¸í™˜ì„± ì§„ë‹¨
                    diagnosis = agent.diagnose_model_compatibility(model_file)
                    if 'error' not in diagnosis:
                        print(f"   ğŸ“Š í˜¸í™˜ì„± ì§„ë‹¨: {diagnosis['compatibility_rate']:.1%} ({diagnosis['compatible_layers']}/{diagnosis['total_new_layers']} ë ˆì´ì–´)")
                    
                    if agent.load_model_with_compatibility(model_file):
                        model_loaded = True
                        print(f"âœ… í˜¸í™˜ì„± ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_file}")
                        break
        
        # 3. ëª¨ë¸ ë³€í™˜ ì‹œë„ (ê¸°ì¡´ ëª¨ë¸ì„ AdvancedProfitDQN ì•„í‚¤í…ì²˜ë¡œ)
        if not model_loaded:
            for model_file in ['agent/final_optimized_model_58d.pth', 'agent/best_model_58d.pth']:
                if os.path.exists(model_file):
                    print(f"ğŸ”„ ëª¨ë¸ ë³€í™˜ ì‹œë„: {model_file}")
                    if agent.create_compatible_model(model_file):
                        model_loaded = True
                        print(f"âœ… ëª¨ë¸ ë³€í™˜ ì„±ê³µ: {model_file}")
                        break
        
        if not model_loaded:
            print("ìƒˆë¡œìš´ 58ì°¨ì› ëª¨ë¸ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        # 4. í›ˆë ¨ ì „ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€ (ë² ì´ìŠ¤ë¼ì¸)
        print("\n4ï¸âƒ£ í›ˆë ¨ ì „ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€...")
        baseline_results, baseline_stats = PerformanceAnalyzer.evaluate_agent(agent, test_env, num_episodes=5)
        print("=== í›ˆë ¨ ì „ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ ===")
        PerformanceAnalyzer.print_performance_report(baseline_results, baseline_stats)
        
        # 5. í›ˆë ¨ ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨
        print(f"\n5ï¸âƒ£ í›ˆë ¨ ë°ì´í„°ì…‹ìœ¼ë¡œ 58ì°¨ì› RL Decision ê¸°ë°˜ í›ˆë ¨ ì‹œì‘...")
        print(f"   í›ˆë ¨ ë°ì´í„°: {len(train_price):,}ê°œ")
        print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_price):,}ê°œ")
        print(f"   ëª©í‘œ ìˆ˜ìµë¥ : 5%+ (ìˆ˜ìµë¥  ì¤‘ì‹¬)")
        print(f"   Signal íŠ¹ì„± í™œìš©: ìˆ˜ìµë¥  ìµœì í™”")
        
        # í›ˆë ¨ ì‹¤í–‰ (ê³¼ì í•© ë°©ì§€ ê°•í™”)
        trained_agent, rewards, win_rates = TrainingManager.train_agent(agent, train_env, episodes=1000, test_env=test_env)
        
        # 6. í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€
        print("\n6ï¸âƒ£ í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€...")
        final_results, final_stats = PerformanceAnalyzer.evaluate_agent(trained_agent, test_env, num_episodes=10)
        print("=== í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ ===")
        PerformanceAnalyzer.print_performance_report(final_results, final_stats)
        
        # 7. ì„±ëŠ¥ ê°œì„ ë„ ë¶„ì„
        improvement = final_stats['overall_win_rate'] - baseline_stats['overall_win_rate']
        print(f"\nğŸš€ 58ì°¨ì› RL Decision ê¸°ë°˜ ì„±ëŠ¥ ê°œì„ ë„ (í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê¸°ì¤€):")
        print(f"   ìŠ¹ë¥ : {baseline_stats['overall_win_rate']:.3f} â†’ {final_stats['overall_win_rate']:.3f} ({improvement:+.3f})")
        print(f"   í‰ê·  ìˆ˜ìµë¥ : {baseline_stats['avg_return']:.3f} â†’ {final_stats['avg_return']:.3f}")
        print(f"   Signal í™œìš©ë„: ìµœëŒ€í™”ë¨")
        
        # 8. ìµœì¢… ëª¨ë¸ ì €ì¥
        trained_agent.save_model('agent/final_optimized_model_58d.pth')
        print(f"\nâœ… ìµœì¢… ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: agent/final_optimized_model_58d.pth")
        
        # 9. ì¶”ê°€ í›ˆë ¨ ì—¬ë¶€ í™•ì¸ (ìˆ˜ìµë¥  ê¸°ì¤€)
        if final_stats['avg_return'] < 0.05:  # ìˆ˜ìµë¥  5% ë¯¸ë§Œ
            user_input = input("\nìˆ˜ìµë¥ ì´ ëª©í‘œ(5%)ì— ë¯¸ë‹¬í•©ë‹ˆë‹¤. ì¶”ê°€ í›ˆë ¨ì„ ì›í•˜ì‹œë‚˜ìš”? (y/n): ")
            if user_input.lower() == 'y':
                print("58ì°¨ì› ìˆ˜ìµë¥  ì¤‘ì‹¬ ì¶”ê°€ í›ˆë ¨ ì‹œì‘...")
                TrainingManager.train_agent(trained_agent, train_env, episodes=1000, test_env=test_env)
                
                # ì¶”ê°€ í›ˆë ¨ í›„ ì¬í‰ê°€
                print("\nì¶”ê°€ í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€...")
                additional_results, additional_stats = PerformanceAnalyzer.evaluate_agent(trained_agent, test_env, num_episodes=10)
                print("=== ì¶”ê°€ í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ ===")
                PerformanceAnalyzer.print_performance_report(additional_results, additional_stats)
        else:
            print(f"\nğŸ‰ ëª©í‘œ ìˆ˜ìµë¥  ë‹¬ì„±! (í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìˆ˜ìµë¥ : {final_stats['avg_return']:.3f} ({final_stats['avg_return']*100:.1f}%))")
    
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()