"""
66ì°¨ì› RL Decision ê¸°ë°˜ ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© AI í›ˆë ¨ ì‹œìŠ¤í…œ - Part 1
- ìƒˆë¡œìš´ RL Decision ìŠ¤í‚¤ë§ˆ í™œìš© (action_value, confidence_value ë“±)
- Conflict ì •ë³´ ë° ì‹œë„ˆì§€ ë©”íƒ€ë°ì´í„° í™œìš©
- ì¤‘ë³µ ê³„ì‚° ì œê±° ë° ì •ë³´ í™œìš© ê·¹ëŒ€í™”
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
import gymnasium as gym
import os

from collections import deque, namedtuple
from gymnasium import spaces
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path

# PyTorch í˜¸í™˜ì„± ì„¤ì •
def setup_pytorch_compatibility():
    """PyTorch ë²„ì „ í˜¸í™˜ì„± ì„¤ì •"""
    try:
        # NumPy 2.0 í˜¸í™˜ì„±ì„ ìœ„í•œ ì•ˆì „í•œ ê¸€ë¡œë²Œ ì„¤ì •
        safe_globals = [
            np.ndarray,
            np.dtype,
            np.float32,
            np.float64,
            np.int32,
            np.int64,
        ]
        
        # numpy.core ëŒ€ì‹  numpy._core ì‚¬ìš© (NumPy 2.0 í˜¸í™˜)
        try:
            import numpy._core.multiarray
            safe_globals.append(numpy._core.multiarray.scalar)
        except ImportError:
            # ì´ì „ ë²„ì „ í˜¸í™˜ì„±
            try:
                import numpy.core.multiarray
                safe_globals.append(numpy.core.multiarray.scalar)
            except ImportError:
                pass
        
        torch.serialization.add_safe_globals(safe_globals)
        print("PyTorch í˜¸í™˜ ì„¤ì • ì™„ë£Œ (NumPy 2.0 í˜¸í™˜)")
    except AttributeError:
        print("PyTorch ì´ì „ ë²„ì „ ê°ì§€ë¨")

setup_pytorch_compatibility()

Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])

class RewardCalculator:
    """í¬í…ì…œ ê¸°ë°˜ ë³´ìƒ ê³„ì‚°ê¸° - Sparse Reward ë¬¸ì œ í•´ê²°"""
    
    def __init__(self, max_trades_memory: int = 50):
        self.recent_trades = deque(maxlen=max_trades_memory)
        self.recent_returns = deque(maxlen=20)
        self.target_return_per_trade = 0.005  # ê±°ë˜ë‹¹ 0.5% ëª©í‘œ
        
    def calculate_reward(self, current_price: float, entry_price: float, position: float, 
                    holding_time: int, trade_pnl: Optional[float] = None) -> float:
        """í¬í…ì…œ ê¸°ë°˜ ë³´ìƒ ê³„ì‚° - Hold ì•¡ì…˜ë„ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ"""
        reward = 0.0
        
        # ê±°ë˜ ì™„ë£Œ ì‹œ
        if trade_pnl is not None:
            # ì‹¤ì œ ìˆ˜ìµë¥  ê¸°ë°˜ ë³´ìƒ (ë” ê°•í•œ ì‹ í˜¸)
            reward = trade_pnl * 1000  # 1% = 10.0 ë¦¬ì›Œë“œ
            
            # ë³´ìœ  ì‹œê°„ ì¸ì„¼í‹°ë¸Œ (ì§§ì€ ê±°ë˜ ì–µì œ)
            if holding_time >= 10:  # 30ë¶„ ì´ìƒ
                reward += 0.5
            else:
                reward -= 1.0  # ë¹ˆë²ˆí•œ ê±°ë˜ í˜ë„í‹°
            
            # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ì— ë”°ë¥¸ ì¶”ê°€ ë³´ìƒ
            if trade_pnl > self.target_return_per_trade:
                reward += 5.0  # ëª©í‘œ ë‹¬ì„± ë³´ë„ˆìŠ¤
            elif trade_pnl < -self.target_return_per_trade:
                reward -= 5.0  # í° ì†ì‹¤ í˜ë„í‹°
            
            # ìƒ¤í”„ ë¹„ìœ¨ ê¸°ë°˜ ë³´ìƒ (ë¦¬ìŠ¤í¬ ëŒ€ë¹„ ìˆ˜ìµ ê³ ë ¤)
            if len(self.recent_returns) > 5:
                returns_std = np.std(list(self.recent_returns))
                if returns_std > 0:
                    sharpe_bonus = (trade_pnl / returns_std) * 10
                    reward += sharpe_bonus
            
            # ê±°ë˜ ê¸°ë¡ ì €ì¥
            self.recent_trades.append({
                'pnl': trade_pnl,
                'holding_time': holding_time
            })
            self.recent_returns.append(trade_pnl)
        
        # Hold ì¤‘ì¼ ë•Œ - ë¯¸ì‹¤í˜„ ì†ìµ ê¸°ë°˜ ê°•í•œ ì‹ í˜¸
        elif position != 0:
            unrealized_pnl = self._calculate_unrealized_pnl(current_price, entry_price, position)
            reward = unrealized_pnl * 100  # ê°•í•œ ì‹ í˜¸ (1% = 1.0 ë¦¬ì›Œë“œ)
            
            # ë„ˆë¬´ ì˜¤ë˜ ë³´ìœ  ì‹œ í˜ë„í‹°
            if holding_time > 50:  # 150ë¶„ ì´ìƒ
                reward -= 0.2
        
        # ì™„ì „íˆ í¬ì§€ì…˜ ì—†ì„ ë•ŒëŠ” 0 (ë¬¸ì œì—†ìŒ)
        
        return reward
    
    def _calculate_unrealized_pnl(self, current_price: float, entry_price: float, position: float) -> float:
        """ë¯¸ì‹¤í˜„ ì†ìµ ê³„ì‚°"""
        if entry_price <= 0:
            return 0.0
        
        price_change = (current_price - entry_price) / entry_price
        return position * price_change
    



class DuelingDQN(nn.Module):
    """ê°•í™”ëœ Dueling DQN (Value + Advantage ë¶„ë¦¬ë¡œ ì•ˆì •ì ì¸ í•™ìŠµ)"""
    
    def __init__(self, state_size: int, hidden_size: int = 256, dropout: float = 0.1):
        super().__init__()
        
        self.state_size = state_size
        self.hidden_size = hidden_size
        
        # ë” ê¹Šì€ ê³µí†µ íŠ¹ì§• ì¶”ì¶œê¸° (4ì¸µ)
        self.feature_extractor = nn.Sequential(
            nn.Linear(state_size, 512),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64),
            nn.ReLU()
        )
        
        # Value Stream (ìƒíƒœì˜ ê°€ì¹˜) - ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬
        self.value_stream = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )
        
        # Advantage Stream (ì•¡ì…˜ë³„ ì¥ì ) - ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬
        self.advantage_stream = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 3)
        )
                
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """Orthogonal ì´ˆê¸°í™” (DuelingDQNì— ë” ì í•©)"""
        if isinstance(module, nn.Linear):
            nn.init.orthogonal_(module.weight)
            nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        # ë°°ì¹˜ ì°¨ì› í™•ì¸
        if x.dim() == 1:
            x = x.unsqueeze(0)
            single_sample = True
        else:
            single_sample = False
        
        # ê³µí†µ íŠ¹ì§• ì¶”ì¶œ
        features = self.feature_extractor(x)  # [batch_size, 64]
        
        # Value Stream (ìƒíƒœì˜ ê°€ì¹˜)
        value = self.value_stream(features)  # [batch_size, 1]
        
        # Advantage Stream (ì•¡ì…˜ë³„ ì¥ì )
        position_adv = self.advantage_stream(features)  # [batch_size, 3]
        
        # Dueling êµ¬ì¡°: Q(s,a) = V(s) + A(s,a) - mean(A(s,a))
        position_q = value + position_adv - position_adv.mean(dim=1, keepdim=True)
        
        # ë‹¨ì¼ ìƒ˜í”Œì´ë©´ ë°°ì¹˜ ì°¨ì› ì œê±°
        if single_sample:
            position_q = position_q.squeeze(0)
        
        return position_q


class TradingEnvironment(gym.Env):
    """111ì°¨ì› RL Decision ê¸°ë°˜ ì•”í˜¸í™”í ê±°ë˜ ê°•í™”í•™ìŠµ í™˜ê²½ (Gymnasium í˜¸í™˜) - OHLC í¬í•¨"""
    
    def __init__(self, signal_data: List[Dict], initial_balance: float = 10000.0):
        super().__init__()
        
        self.signal_data = signal_data
        self.initial_balance = initial_balance
        
        self.reward_calculator = RewardCalculator()
        
        # ì•¡ì…˜/ìƒíƒœ ìŠ¤í˜ì´ìŠ¤ ì •ì˜ (ë‹¨íƒ€ì— ì í•©í•œ ë‹¨ìˆœí•œ ì•¡ì…˜)
        self.action_space = spaces.Discrete(3)  # 0: Hold, 1: Buy, 2: Sell
        
        # ê±°ë˜ ì œí•œ ì„¤ì • (ê±°ë˜ ê°„ê²© ì™„ì „ ì œê±°)
        self.min_trade_interval = 1  # ê±°ë˜ ê°„ê²© ìµœì†Œí™”
        self.last_trade_step = -1  # ì´ˆê¸°ê°’
        self.trading_cost = 0.0  # í›ˆë ¨ìš© ìˆ˜ìˆ˜ë£Œ ì œê±° 
        
        # 111ì°¨ì› ìƒíƒœ ê³µê°„ (ê¸°ìˆ ì  ì§€í‘œ + í¬íŠ¸í´ë¦¬ì˜¤ + ì˜ì‚¬ê²°ì • íŠ¹ì„±)
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, 
            shape=(111,),  # 3 + 20 + 8 + 80 = 111ì°¨ì›
            dtype=np.float32
        )
        
        self.reset()
    
    def reset(self, seed=None, options=None):
        """í™˜ê²½ ì´ˆê¸°í™” (Gymnasium í˜¸í™˜)"""
        if seed is not None:
            np.random.seed(seed)
        
        self.current_step = 0
        self.balance = self.initial_balance
        self.current_position = 0.0
        self.entry_price = 0.0
        self.entry_balance = 0.0  # ì§„ì… ì‹œì ì˜ ì”ê³  ì¶”ì 
        self.unrealized_pnl = 0.0
        self.total_trades = 0
        self.winning_trades = 0
        self.max_drawdown = 0.0
        self.peak_balance = self.initial_balance
        self.consecutive_losses = 0
        self.holding_time = 0
        self.in_position = False
        self.last_trade_pnl = None
        self.last_trade_step = -self.min_trade_interval - 1  # ê±°ë˜ ê°„ê²© ì´ˆê¸°í™” (ë” ì•ˆì „í•˜ê²Œ)
        
        observation = self._get_observation()
        info = self._create_info_dict()
        
        return observation, info
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """í™˜ê²½ ìŠ¤í… ì‹¤í–‰ (Gymnasium í˜¸í™˜) - OHLC í¬í•¨"""
        if self.current_step >= len(self.signal_data) - 1:
            return self._get_observation(), 0.0, True, False, {}
        
        # ì´ì‚°ì ì¸ ì•¡ì…˜ ì²˜ë¦¬ (positionë§Œ) - ê±°ë˜ ê°„ê²© ì²´í¬ ì œê±°
        # 0: Hold, 1: Buy, 2: Sell
        if action == 0:  # Hold
            position_change = 0.0
        elif action == 1:  # Buy
            position_change = 1.0  # ê±°ë˜ ê°„ê²© ì²´í¬ ì œê±°
        elif action == 2:  # Sell
            position_change = -1.0  # ê±°ë˜ ê°„ê²© ì²´í¬ ì œê±°
        else:
            position_change = 0.0  # ê¸°ë³¸ê°’ì€ Hold
        
        # í˜„ì¬ ì‹ í˜¸ ë°ì´í„°ì—ì„œ OHLC ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        current_signal = self.signal_data[self.current_step]
        current_close_price = current_signal.get('close')
        
        # í¬ì§€ì…˜ ë° ê±°ë˜ ì²˜ë¦¬ (í˜„ì¬ close ê°€ê²©ìœ¼ë¡œ ì‹¤í–‰)
        trade_completed, old_position = self._process_position_change(
            position_change, current_close_price
        )
        
        # ê±°ë˜ ì™„ë£Œ ì‹œ ê±°ë˜ ìŠ¤í… ì—…ë°ì´íŠ¸
        if trade_completed:
            self.last_trade_step = self.current_step
        
        # ë³´ìƒ ê³„ì‚° (í¬í…ì…œ ê¸°ë°˜ - Hold ì•¡ì…˜ë„ í•™ìŠµ ê°€ëŠ¥)
        if trade_completed:
            # ê±°ë˜ ì™„ë£Œ ì‹œ: ìˆ˜ìˆ˜ë£Œ ì°¨ê° í›„ ì‹¤ì œ ìˆ˜ìµë¥  ê¸°ë°˜ ë³´ìƒ
            reward = self.reward_calculator.calculate_reward(
                current_price=current_close_price,  # í˜„ì¬ close ê°€ê²© ì‚¬ìš©
                entry_price=self.entry_price,
                position=old_position,  # ê±°ë˜ ì „ í¬ì§€ì…˜ ì‚¬ìš©
                holding_time=self.holding_time,
                trade_pnl=self.last_trade_pnl  # ì´ë¯¸ ìˆ˜ìˆ˜ë£Œ ì°¨ê°ëœ ìˆ˜ìµë¥ 
            )
        else:
            # Hold ì•¡ì…˜ì´ë‚˜ ê±°ë˜ ê°„ê²© ë¯¸ì¶©ì¡± ì‹œ: ë¯¸ì‹¤í˜„ ì†ìµ ê¸°ë°˜ ì•½í•œ ì‹ í˜¸
            reward = self.reward_calculator.calculate_reward(
                current_price=current_close_price,
                entry_price=self.entry_price,
                position=self.current_position,  # í˜„ì¬ í¬ì§€ì…˜ ì‚¬ìš©
                holding_time=self.holding_time,
                trade_pnl=None  # ê±°ë˜ ì™„ë£Œê°€ ì•„ë‹˜
            )
        
        # ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ ì´ë™
        self.current_step += 1
        self.holding_time += 3
        
        done = (self.current_step >= len(self.signal_data) - 1 or 
                self.balance <= self.initial_balance * 0.1)
        
        # ì—í”¼ì†Œë“œ ë³´ë„ˆìŠ¤ ì œê±° - Sparse Reward ë¬¸ì œ í•´ê²°
        
        truncated = False  # Gymnasium í˜¸í™˜ì„ ìœ„í•œ truncated í”Œë˜ê·¸
        info = self._create_info_dict()
        
        return self._get_observation(), reward, done, truncated, info
    
    def _get_observation(self) -> np.ndarray:
        """111ì°¨ì› ìƒíƒœ ê´€ì°°ê°’ ë°˜í™˜ (ê¸°ìˆ ì  ì§€í‘œ + í¬íŠ¸í´ë¦¬ì˜¤ + ì˜ì‚¬ê²°ì • íŠ¹ì„±) - OHLC í¬í•¨"""
        if self.current_step >= len(self.signal_data):
            return np.zeros(111, dtype=np.float32)
        
        # í˜„ì¬ ì‹ í˜¸ ë°ì´í„°ì—ì„œ OHLC ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        current_signal = self.signal_data[self.current_step]
        current_price = current_signal.get('close')
        
        # ì´ì „ ê°€ê²©ê³¼ ë¹„êµí•˜ì—¬ ê°€ê²© ë³€í™”ìœ¨ ê³„ì‚°
        if self.current_step > 0:
            prev_signal = self.signal_data[self.current_step - 1]
            prev_price = prev_signal.get('close', current_price)
            price_change = (current_price - prev_price) / prev_price if prev_price > 0 else 0.0
        else:
            price_change = 0.0
        
        basic_observation = np.array([
            price_change,  # ê°€ê²© ë³€í™”ìœ¨
            self.current_position,  # í˜„ì¬ í¬ì§€ì…˜ (-1~1)
            self.balance / self.initial_balance  # ì”ê³  ë¹„ìœ¨
        ], dtype=np.float32)
        
        # Signal ë°ì´í„°ëŠ” ì´ë¯¸ ê°€ì ¸ì™”ìŒ
        
        # ê° ì°¨ì›ë³„ íŠ¹ì„± ì¶”ì¶œ
        price_indicators = self._extract_price_indicators(current_signal)  # 20ì°¨ì›
        portfolio_state = self._get_portfolio_state()  # 8ì°¨ì›
        decision_features = self._extract_decision_features(current_signal)  # 80ì°¨ì›
        
        # ëª¨ë“  ì°¨ì› ê²°í•© (3 + 20 + 8 + 80 = 111ì°¨ì›)
        observation = np.concatenate([
            basic_observation,      # 3ì°¨ì›
            price_indicators,       # 20ì°¨ì›
            portfolio_state,        # 8ì°¨ì›
            decision_features       # 80ì°¨ì›
        ], dtype=np.float32)
        
        return observation
    
    def _extract_price_indicators(self, signal_data: Dict) -> np.ndarray:
        """Signalì˜ indicatorë“¤ì„ price featureë¡œ í™œìš© (20ì°¨ì›) - ì‹¤ì œ ë°ì´í„° êµ¬ì¡° ê¸°ë°˜"""
        current_price = signal_data.get('close')
        
        # 1. ì§€í‘œ ê°’ë“¤ ê·¸ëŒ€ë¡œ ì‚¬ìš© (10ê°œ)
        vwap = signal_data.get('indicator_vwap', 0.0)
        poc = signal_data.get('indicator_poc', 0.0)
        hvn = signal_data.get('indicator_hvn', 0.0)
        lvn = signal_data.get('indicator_lvn', 0.0)
        atr = signal_data.get('indicator_atr', 0.0)
        vwap_std = signal_data.get('indicator_vwap_std', 0.0)
        prev_high = signal_data.get('indicator_prev_day_high', 0.0)
        prev_low = signal_data.get('indicator_prev_day_low', 0.0)
        or_high = signal_data.get('indicator_opening_range_high', 0.0)
        or_low = signal_data.get('indicator_opening_range_low', 0.0)
        
        # 2. í˜„ì¬ ìº”ë“¤ ì •ë³´ (10ê°œ)
        high = signal_data.get('high', 0.0)
        low = signal_data.get('low', 0.0)
        close = signal_data.get('close', 0.0)
        open_price = signal_data.get('open', 0.0)
        quote_volume = signal_data.get('quote_volume', 0.0)
        
        # ì•ˆì „í•œ ìº”ë“¤ ê³„ì‚°
        body_size = abs(close - open_price) if open_price > 0 else 0.0
        candle_range = high - low if high > low else 1.0
        upper_shadow = high - max(open_price, close) if high > low else 0.0
        lower_shadow = min(open_price, close) - low if high > low else 0.0
        
        return np.array([
            # ì§€í‘œ ê°’ë“¤ ê·¸ëŒ€ë¡œ ì‚¬ìš© (10ê°œ)
            vwap,
            poc,
            hvn,
            lvn,
            atr,
            vwap_std,
            prev_high,
            prev_low,
            or_high,
            or_low,
            
            # í˜„ì¬ ìº”ë“¤ ì •ë³´ (10ê°œ)
            body_size / open_price if open_price > 0 else 0.0,  # ëª¸í†µ í¬ê¸°
            candle_range / current_price if current_price > 0 else 0.0,  # ì „ì²´ ë²”ìœ„
            upper_shadow / candle_range if candle_range > 0 else 0.0,  # ìœ„ê¼¬ë¦¬ ë¹„ìœ¨
            lower_shadow / candle_range if candle_range > 0 else 0.0,  # ì•„ë˜ê¼¬ë¦¬ ë¹„ìœ¨
            body_size / candle_range if candle_range > 0 else 0.0,  # ëª¸í†µ ë¹„ìœ¨
            min(quote_volume / 1000000, 2.0) if quote_volume > 0 else 0.0,  # ê±°ë˜ëŸ‰
            1.0 if close > open_price else 0.0,  # ìƒìŠ¹/í•˜ë½
            upper_shadow / current_price if current_price > 0 else 0.0,  # ìœ„ê¼¬ë¦¬ í¬ê¸°
            lower_shadow / current_price if current_price > 0 else 0.0,  # ì•„ë˜ê¼¬ë¦¬ í¬ê¸°
            body_size / current_price if current_price > 0 else 0.0  # ëª¸í†µ í¬ê¸°
        ], dtype=np.float32)
    
    def _get_portfolio_state(self) -> np.ndarray:
        """í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ì •ë³´ (8ì°¨ì›)"""
        return np.array([
            self.current_position,
            (self.balance - self.initial_balance) / self.initial_balance,
            self.unrealized_pnl / self.initial_balance if self.initial_balance > 0 else 0.0,
            self.total_trades / 100.0,
            self.winning_trades / max(self.total_trades, 1),
            self.max_drawdown,
            self.consecutive_losses / 10.0,
            self.holding_time / 1440.0
        ], dtype=np.float32)
    
    def _extract_decision_features(self, signals: Dict) -> np.ndarray:
        """Decision íŠ¹ì„±ë“¤ (80ì°¨ì›) - ëª¨ë“  ì „ëµ íŠ¹ì„± ì‚¬ìš©"""
        # ì „ëµë³„ íŠ¹ì„± ì¶”ì¶œ (ì‹¤ì œ ë°ì´í„°ì— ìˆëŠ” ì „ëµë“¤)
        strategy_names = [
            'session', 'vpvr', 'bollinger_squeeze', 'orderflow_cvd', 'ichimoku', 
            'vwap_pinball', 'vol_spike', 'liquidity_grab', 'vpvr_micro', 
            'zscore_mean_reversion', 'htf_trend', 'oi_delta', 'funding_rate', 
            'multi_timeframe', 'support_resistance', 'ema_confluence'
        ]
        
        # ëª¨ë“  ì „ëµì˜ ëª¨ë“  íŠ¹ì„± ì‚¬ìš© (16ê°œ ì „ëµ Ã— 5ê°œ íŠ¹ì„± = 80ì°¨ì›)
        all_features = []
        for strategy in strategy_names:
            # Actionì„ ìˆ«ìë¡œ ë³€í™˜ (HOLD=0, BUY=1, SELL=-1)
            action_str = signals.get(f'{strategy}_action', 'HOLD')
            if action_str == 'BUY':
                action_value = 1.0
            elif action_str == 'SELL':
                action_value = -1.0
            else:  # HOLD ë˜ëŠ” None
                action_value = 0.0
            
            # Scoreì™€ Confidence (Noneì¸ ê²½ìš° 0.0ìœ¼ë¡œ ì²˜ë¦¬)
            score = float(signals.get(f'{strategy}_score', 0.0))
            confidence_str = signals.get(f'{strategy}_confidence')
            if confidence_str == 'HIGH':
                confidence = 1.0
            elif confidence_str == 'MEDIUM':
                confidence = 0.5
            elif confidence_str == 'LOW':
                confidence = 0.2
            else:
                confidence = 0.0
            
            # Entryì™€ Stop (Noneì¸ ê²½ìš° 0.0ìœ¼ë¡œ ì²˜ë¦¬)
            entry = float(signals.get(f'{strategy}_entry', 0.0))
            stop = float(signals.get(f'{strategy}_stop', 0.0))
            
            # ëª¨ë“  íŠ¹ì„± ì¶”ê°€: action, score, confidence, entry, stop
            all_features.extend([action_value, score, confidence, entry, stop])

        return np.array(all_features, dtype=np.float32)
    
    def _process_position_change(self, position_change: float, current_price: float) -> Tuple[bool, float]:
        """í¬ì§€ì…˜ ë³€ê²½ ì²˜ë¦¬ (positionë§Œ) - 3ë¶„ë´‰ ê¸°ë°˜"""
        old_position = self.current_position
        trade_completed = False
        
        # ë‹¨ìˆœí•œ ì•¡ì…˜ ì²˜ë¦¬: ì „ì²´ í¬ì§€ì…˜ì„ ì¦‰ì‹œ ë³€ê²½
        target_position = position_change  # -1.0, 0.0, 1.0 ì¤‘ í•˜ë‚˜
        
        # í¬ì§€ì…˜ ë³€ê²½ì´ í•„ìš”í•œì§€ í™•ì¸
        if abs(target_position - self.current_position) > 0.0001:
            # ê¸°ì¡´ í¬ì§€ì…˜ ì²­ì‚° (í˜„ì¬ close ê°€ê²©ìœ¼ë¡œ ì²­ì‚°)
            if abs(self.current_position) > 0.0001:
                trade_completed = True
                # ì²­ì‚° ì²˜ë¦¬ (ìˆ˜ìˆ˜ë£Œ í¬í•¨)
                self._close_position(current_price)
            
            # ìƒˆ í¬ì§€ì…˜ ì§„ì… (í˜„ì¬ close ê°€ê²©ìœ¼ë¡œ ì§„ì…)
            if abs(target_position) > 0.0001:
                self.current_position = target_position
                self.entry_price = current_price  # í˜„ì¬ close ê°€ê²©ìœ¼ë¡œ ì§„ì…
                self.entry_balance = self.balance  # ì§„ì… ì‹œì ì˜ ì”ê³  ì €ì¥
                
                # ì§„ì… ì‹œ ìˆ˜ìˆ˜ë£Œ ì°¨ê°
                entry_fee = abs(target_position) * self.entry_balance * self.trading_cost
                self.balance -= entry_fee
                
                self.holding_time = 0
                self.in_position = True
                trade_completed = True  # ìƒˆ í¬ì§€ì…˜ ì§„ì…ë„ ê±°ë˜ ì™„ë£Œë¡œ ê°„ì£¼
                
        
        return trade_completed, old_position
    
    def _calculate_trade_pnl(self, exit_price: float, entry_price: float, position: float) -> float:
        """ê±°ë˜ ì†ìµ ê³„ì‚°"""
        if entry_price <= 0:
            return 0.0
        
        price_change = (exit_price - entry_price) / entry_price
        return position * price_change
    
    def _close_position(self, exit_price: float):
        """í¬ì§€ì…˜ ì²­ì‚°"""
        if abs(self.current_position) < 0.01:
            return
        
        # ìˆœìˆ˜ ê°€ê²© ë³€í™”ìœ¨ ê³„ì‚° (ìˆ˜ìˆ˜ë£Œ ë¯¸ê³ ë ¤)
        price_change_rate = self._calculate_trade_pnl(exit_price, self.entry_price, self.current_position)
        
        # ì‹¤ì œ ê±°ë˜ ê¸ˆì•¡ ê³„ì‚° (ì§„ì… ì‹œì ì˜ ì”ê³  ê¸°ì¤€)
        trade_volume = abs(self.current_position) * self.entry_balance
        gross_pnl_usd = price_change_rate * trade_volume  # ìˆ˜ìˆ˜ë£Œ ì°¨ê° ì „ ì†ìµ
        
        # ì²­ì‚° ì‹œ ìˆ˜ìˆ˜ë£Œ ê³„ì‚° (ì§„ì… ì‹œì ì˜ ì”ê³  ê¸°ì¤€)
        exit_trade_amount = abs(self.current_position) * self.entry_balance
        exit_fee = exit_trade_amount * self.trading_cost
        
        # ì²­ì‚° ìˆ˜ìˆ˜ë£Œ ì°¨ê° í›„ ìˆœì†ìµ
        net_pnl_usd = gross_pnl_usd - exit_fee
        
        # ì”ê³  ì—…ë°ì´íŠ¸
        self.balance += net_pnl_usd
        
        # í†µê³„ ì—…ë°ì´íŠ¸ (ìˆœì†ìµ ê¸°ì¤€)
        self._update_trading_stats(net_pnl_usd)
        
        # ìˆ˜ìˆ˜ë£Œ ì°¨ê° í›„ ì‹¤ì œ ìˆ˜ìµë¥  ê³„ì‚° (ë¹„ìœ¨)
        # ì§„ì… ìˆ˜ìˆ˜ë£Œì™€ ì²­ì‚° ìˆ˜ìˆ˜ë£Œë¥¼ ëª¨ë‘ ê³ ë ¤í•œ ì´ ìˆ˜ìˆ˜ë£Œ
        total_fee = exit_trade_amount * self.trading_cost * 2  # ì§„ì… + ì²­ì‚°
        total_net_pnl = gross_pnl_usd - total_fee
        
        if trade_volume > 0:
            self.last_trade_pnl = total_net_pnl / trade_volume
        else:
            self.last_trade_pnl = 0.0
        
        # í¬ì§€ì…˜ ì´ˆê¸°í™”
        self.current_position = 0.0
        self.entry_balance = 0.0  # ì§„ì… ì”ê³  ì´ˆê¸°í™”
        self.unrealized_pnl = 0.0
        self.in_position = False
        self.holding_time = 0
    
    def _update_trading_stats(self, pnl_usd: float):
        """ê±°ë˜ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.total_trades += 1
        
        if pnl_usd > 0:
            self.winning_trades += 1
            self.consecutive_losses = 0
        else:
            self.consecutive_losses += 1
        
        # ìµœëŒ€ ë‚™í­ ì—…ë°ì´íŠ¸
        if self.balance > self.peak_balance:
            self.peak_balance = self.balance
        else:
            drawdown = (self.peak_balance - self.balance) / self.peak_balance
            self.max_drawdown = max(self.max_drawdown, drawdown)
    
    def _create_info_dict(self) -> Dict:
        """ì •ë³´ ë”•ì…”ë„ˆë¦¬ ìƒì„± - OHLC í¬í•¨"""
        if self.current_step < len(self.signal_data):
            current_signal = self.signal_data[self.current_step]
            current_price = current_signal.get('close')
        else:
            current_price = 0.0
        
        return {
            'balance': self.balance,
            'position': self.current_position,
            'unrealized_pnl': self.unrealized_pnl,
            'total_trades': self.total_trades,
            'win_rate': self.winning_trades / max(self.total_trades, 1),
            'current_price': current_price,
            'entry_price': self.entry_price,
            'holding_time': self.holding_time,
            'max_drawdown': self.max_drawdown,
            'trade_completed': hasattr(self, 'last_trade_pnl') and self.last_trade_pnl is not None,
            'trade_pnl': self.last_trade_pnl if hasattr(self, 'last_trade_pnl') else None
        }

"""
111ì°¨ì›ì› RL Decision ê¸°ë°˜ ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© AI í›ˆë ¨ ì‹œìŠ¤í…œ - Part 2
- RLAgent í´ë˜ìŠ¤ ë° í›ˆë ¨/í‰ê°€ ì‹œìŠ¤í…œ
- ìƒˆë¡œìš´ Decision ìŠ¤í‚¤ë§ˆ ë°ì´í„° ë¡œë” ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
"""

class RLAgent:
    """111ì°¨ì› RL Decision ê¸°ë°˜ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸"""
    
    def __init__(self, state_size: int = 3, learning_rate: float = 1e-4,  # ì ì ˆí•œ í•™ìŠµë¥ 
                    gamma: float = 0.99, epsilon: float = 0.9, epsilon_decay: float = 0.9995,  # 0.995 â†’ 0.9995ë¡œ ë³€ê²½
                    hidden_size: int = 256):
        
        self.state_size = state_size
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.hidden_size = hidden_size
        self.learning_rate = learning_rate  # learning_rate ì†ì„± ì¶”ê°€
        self.epsilon_min = 0.2  # 20%ë¡œ ì„¤ì • (ë” ì•ˆì •ì ì¸ íƒí—˜)
        
        # Îµ ê°’ì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì´ˆê¸°í™”
        if self.epsilon < self.epsilon_min:
            self.epsilon = 0.8  # 80%ë¡œ ì´ˆê¸°í™” (ì¶©ë¶„í•œ íƒí—˜)
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° ë””ë°”ì´ìŠ¤ ì„¤ì •
        if torch.cuda.is_available():
            self.device = torch.device("cuda")
            print(f"ğŸš€ GPU ì‚¬ìš©: {torch.cuda.get_device_name(0)}")
            print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        else:
            self.device = torch.device("cpu")
            print("âš ï¸  GPU ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
            print("   GPU ì‚¬ìš©ì„ ì›í•˜ë©´ PyTorch CUDA ë²„ì „ì„ ì„¤ì¹˜í•˜ì„¸ìš”:")
            print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
        print(f"Using device: {self.device} for {state_size}ì°¨ì› ëª¨ë¸")
        
        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” (DuelingDQN ì‚¬ìš© - Value + Advantage ë¶„ë¦¬ë¡œ ì•ˆì •ì ì¸ í•™ìŠµ)
        print("ğŸš€ DuelingDQN ì•„í‚¤í…ì²˜ ì‚¬ìš© (Value + Advantage ë¶„ë¦¬ë¡œ ì•ˆì •ì ì¸ í•™ìŠµ)")
        self.q_network = DuelingDQN(state_size, hidden_size).to(self.device)
        self.target_network = DuelingDQN(state_size, hidden_size).to(self.device)
        
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # ê²½í—˜ ë¦¬í”Œë ˆì´ (ìˆ˜ìµë¥  í•™ìŠµ ìµœì í™”)
        self.memory = deque(maxlen=5000)  # ë©”ëª¨ë¦¬ í¬ê¸° ì¦ê°€ (ë” ë§ì€ ê²½í—˜ ì €ì¥)
        self.batch_size = 512  # ë°°ì¹˜ í¬ê¸° ì¦ê°€ (ë” ì•ˆì •ì ì¸ í•™ìŠµ)
        
        # í•™ìŠµ ì¶”ì 
        self.training_rewards = []
        self.losses = []
        self.return_rates = []  # ìˆ˜ìµë¥  ì¶”ì 
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ (ìˆ˜ìµë¥  í•™ìŠµ ìµœì í™”)
        self.target_update_freq = 25  # ë” ìì£¼ ì—…ë°ì´íŠ¸ (ë¹ ë¥¸ í•™ìŠµ)
        self.update_count = 0
    
        
        # ì•¡ì…˜ ê³µê°„ ì„¤ì • (í™˜ê²½ì—ì„œ ê°€ì ¸ì˜´)
        self.action_space = None  # í™˜ê²½ì—ì„œ ì„¤ì •ë¨
    
    def remember(self, state, action: int, reward, next_state, done):
        """ê²½í—˜ ì €ì¥ (ë‹¨ìˆœ ì•¡ì…˜)"""
        self.memory.append(Experience(state, action, reward, next_state, done))
    
    
    def adaptive_learning_rate(self, recent_rewards: List[float], recent_return_rates: List[float]):
        """ì ì‘í˜• í•™ìŠµë¥  ì¡°ì •"""
        if len(recent_rewards) < 10:
            return
        
        # ìµœê·¼ ì„±ëŠ¥ ë¶„ì„
        avg_reward = np.mean(recent_rewards[-10:])
        avg_return_rate = np.mean(recent_return_rates[-10:])
        
        # ì„±ëŠ¥ì´ ì¢‹ìœ¼ë©´ í•™ìŠµë¥  ê°ì†Œ (ë§¤ìš° ë³´ìˆ˜ì ìœ¼ë¡œ)
        if avg_return_rate > 0.05 and avg_reward > 0:  # ìˆ˜ìµë¥  5% ì´ìƒ
            self.learning_rate *= 0.995  # ë§¤ìš° ëŠë¦° ê°ì†Œ
            self.learning_rate = max(self.learning_rate, 1e-7)  # ë” ë‚®ì€ ìµœì†Œê°’
        # ì„±ëŠ¥ì´ ë‚˜ì˜ë©´ í•™ìŠµë¥  ì¦ê°€ (ë§¤ìš° ë³´ìˆ˜ì ìœ¼ë¡œ)
        elif avg_return_rate < 0.02 or avg_reward < -10:  # ìˆ˜ìµë¥  2% ë¯¸ë§Œ ë˜ëŠ” ì†ì‹¤
            self.learning_rate *= 1.01  # ë§¤ìš° ëŠë¦° ì¦ê°€
            self.learning_rate = min(self.learning_rate, 1e-4)  # ë” ë‚®ì€ ìµœëŒ€ê°’
        
        # ì˜µí‹°ë§ˆì´ì € ì—…ë°ì´íŠ¸
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = self.learning_rate
    
    def act(self, state: np.ndarray) -> int:
        """ì•¡ì…˜ ì„ íƒ - ë‹¨ìˆœí•œ epsilon-greedy"""
        if np.random.random() <= self.epsilon:
            return self._get_random_action()
        
        return self._get_greedy_action(state)
    
    def _get_random_action(self) -> int:
        """ëœë¤ ì•¡ì…˜ (ë‹¨ìˆœ ì•¡ì…˜)"""
        return np.random.randint(0, 3)  # 0: Hold, 1: Buy, 2: Sell
    
    def _get_greedy_action(self, state: np.ndarray) -> int:
        """Qê°’ ê¸°ë°˜ ì•¡ì…˜ ì„ íƒ - ì„ê³„ê°’ ì ìš©"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            position_q = self.q_network(state_tensor)
            
            # Qê°’ ê¸°ë°˜ ì•¡ì…˜ ì„ íƒ (ì„ê³„ê°’ ì œê±°)
            q_values = position_q[0].cpu().numpy()
            action = np.argmax(q_values)  # ë‹¨ìˆœíˆ ìµœëŒ€ Qê°’ ì„ íƒ
            
            return action
    
    def replay(self):
        """ìš°ì„ ìˆœìœ„ ê²½í—˜ ë¦¬í”Œë ˆì´ í•™ìŠµ"""
        if len(self.memory) < self.batch_size * 2:
            return
        
        # ë‹¨ìˆœí•œ ëœë¤ ìƒ˜í”Œë§
        batch = random.sample(self.memory, min(self.batch_size, len(self.memory)))
        
        loss = self._compute_loss(batch)
        
        # ì—­ì „íŒŒ
        self.optimizer.zero_grad()
        loss.backward()
        
        # ê°•í™”ëœ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ì•ˆì •ì„± í–¥ìƒ)
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 0.5)
        
        self.optimizer.step()
        
        self.losses.append(loss.item())
        
        # ë‹¨ìˆœí•œ ì—¡ì‹¤ë¡  ê°ì†Œ
        if self.epsilon > self.epsilon_min:
            self.epsilon *= 0.999  # ë§¤ìš° ëŠë¦° ê°ì†Œ
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
        self.update_count += 1
        if self.update_count % self.target_update_freq == 0:
            self.update_target_network()
    
    def _compute_loss(self, batch) -> torch.Tensor:
        """ìˆ˜ìµë¥  ìµœì í™” ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°"""
        # íš¨ìœ¨ì ì¸ í…ì„œ ë³€í™˜ (numpy ë°°ì—´ì„ ë¨¼ì € ê²°í•©)
        states = torch.FloatTensor(np.array([e.state for e in batch])).to(self.device)
        actions = [e.action for e in batch]
        rewards = torch.FloatTensor(np.array([e.reward for e in batch])).to(self.device)
        next_states = torch.FloatTensor(np.array([e.next_state for e in batch])).to(self.device)
        dones = [bool(e.done) for e in batch]
        
        # í˜„ì¬ Qê°’ë“¤
        current_position_q = self.q_network(states)
        
        # Double DQN: í˜„ì¬ ë„¤íŠ¸ì›Œí¬ë¡œ ì•¡ì…˜ ì„ íƒ, íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ë¡œ Qê°’ ê³„ì‚°
        with torch.no_grad():
            # í˜„ì¬ ë„¤íŠ¸ì›Œí¬ë¡œ ë‹¤ìŒ ìƒíƒœì˜ ì•¡ì…˜ ì„ íƒ
            next_position_q_current = self.q_network(next_states)
            
            # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ë¡œ Qê°’ ê³„ì‚°
            next_position_q_target = self.target_network(next_states)
            
            target_position_q = current_position_q.clone()
            
            for i, (action, reward, done) in enumerate(zip(actions, rewards, dones)):
                # ë‹¨ìˆœ ì•¡ì…˜ ì²˜ë¦¬: actionì€ 0, 1, 2 ì¤‘ í•˜ë‚˜
                if isinstance(action, (list, np.ndarray)):
                    # ê¸°ì¡´ ì—°ì† ì•¡ì…˜ì—ì„œ ë‹¨ìˆœ ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜
                    if len(action) > 0:
                        if action[0] > 0.3:  # Buy
                            action_idx = 1
                        elif action[0] < -0.3:  # Sell
                            action_idx = 2
                        else:  # Hold
                            action_idx = 0
                    else:
                        action_idx = 0  # ê¸°ë³¸ê°’ Hold
                else:
                    # ì´ë¯¸ ë‹¨ìˆœ ì•¡ì…˜ì¸ ê²½ìš°
                    action_idx = int(action) if 0 <= action <= 2 else 0
                
                if not done:
                    # Double DQN: í˜„ì¬ ë„¤íŠ¸ì›Œí¬ë¡œ ì„ íƒí•œ ì•¡ì…˜ì˜ íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ Qê°’ ì‚¬ìš©
                    best_action = torch.argmax(next_position_q_current[i])
                    target_q = reward + self.gamma * next_position_q_target[i, best_action]
                    target_position_q[i, action_idx] = target_q
                else:
                    # ìµœì¢… ë³´ìƒ (ìˆ˜ìµë¥  ì¤‘ì‹¬)
                    target_position_q[i, action_idx] = reward
        
        # Q-learning ì†ì‹¤ (ìˆœìˆ˜ DuelingDQN)
        pos_loss = F.smooth_l1_loss(current_position_q, target_position_q)
        
        # ë‹¨ìˆœí•œ DuelingDQN ì†ì‹¤
        total_loss = pos_loss
        
        return total_loss
    
    
    def update_target_network(self):
        """íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def save_model(self, filepath: str) -> bool:
        """ëª¨ë¸ ì €ì¥"""
        try:
            os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)
            
            save_dict = {
                'q_network': self.q_network.state_dict(),
                'target_network': self.target_network.state_dict(),
                'optimizer': self.optimizer.state_dict(),
                'epsilon': float(self.epsilon),
                'training_rewards': [float(r) for r in self.training_rewards],
                'losses': [float(l) for l in self.losses],
                'return_rates': [float(r) for r in self.return_rates],
                'update_count': int(self.update_count),
                'state_size': int(self.state_size)
            }
            
            torch.save(save_dict, filepath)
            print(f"111ì°¨ì› ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")
            return True
            
        except Exception as e:
            print(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def load_model(self, filepath: str) -> bool:
        """ëª¨ë¸ ë¡œë“œ"""
        if not os.path.exists(filepath):
            print(f"ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {filepath}")
            return False
        
        try:
            try:
                checkpoint = torch.load(filepath, map_location=self.device, weights_only=True)
            except:
                checkpoint = torch.load(filepath, map_location=self.device, weights_only=False)
            
            # ìƒíƒœ í¬ê¸° í™•ì¸
            model_state_size = checkpoint.get('state_size', 60)
            if model_state_size != self.state_size:
                print(f"âŒ ëª¨ë¸ ì°¨ì› ë¶ˆì¼ì¹˜: ê¸°ëŒ€ {self.state_size}, ì‹¤ì œ {model_state_size}")
                return False
            
            self.q_network.load_state_dict(checkpoint['q_network'])
            self.target_network.load_state_dict(checkpoint['target_network'])
            self.optimizer.load_state_dict(checkpoint['optimizer'])
            self.epsilon = checkpoint.get('epsilon', self.epsilon)
            self.training_rewards = checkpoint.get('training_rewards', [])
            self.losses = checkpoint.get('losses', [])
            self.return_rates = checkpoint.get('return_rates', [])
            self.update_count = checkpoint.get('update_count', 0)
            
            print(f"âœ… 111ì°¨ì› ëª¨ë¸ ë¡œë“œ ì„±ê³µ! ì—¡ì‹¤ë¡ : {self.epsilon:.3f}")
            return True
            
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def load_model_with_compatibility(self, filepath: str) -> bool:
        """í˜¸í™˜ì„±ì„ ê³ ë ¤í•œ ëª¨ë¸ ë¡œë“œ (êµ¬ì¡° ì°¨ì´ ë¬´ì‹œ)"""
        if not os.path.exists(filepath):
            print(f"ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {filepath}")
            return False
        
        try:
            checkpoint = torch.load(filepath, map_location=self.device, weights_only=False)
            
            # ìƒíƒœ í¬ê¸° í™•ì¸
            model_state_size = checkpoint.get('state_size', 60)
            print(f"ê¸°ì¡´ ëª¨ë¸ ì°¨ì›: {model_state_size}ì°¨ì›")
            
            # ê¸°ì¡´ ëª¨ë¸ê³¼ í˜„ì¬ ëª¨ë¸ì˜ êµ¬ì¡°ê°€ ì™„ì „íˆ ë‹¤ë¥´ë¯€ë¡œ
            # í˜¸í™˜ ê°€ëŠ¥í•œ ë¶€ë¶„ë§Œ ë¡œë“œí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ìƒˆë¡œ ì´ˆê¸°í™”
            print("âš ï¸ ëª¨ë¸ êµ¬ì¡°ê°€ ë‹¤ë¦…ë‹ˆë‹¤. í˜¸í™˜ ê°€ëŠ¥í•œ ë¶€ë¶„ë§Œ ë¡œë“œí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ìƒˆë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            
            # í˜„ì¬ ëª¨ë¸ì˜ state_dict
            current_state_dict = self.q_network.state_dict()
            loaded_state_dict = checkpoint['q_network']
            
            # í˜¸í™˜ ê°€ëŠ¥í•œ ë ˆì´ì–´ë§Œ ë¡œë“œ
            compatible_state_dict = {}
            loaded_count = 0
            initialized_count = 0
            
            for key in current_state_dict.keys():
                if key in loaded_state_dict:
                    # í¬ê¸°ê°€ ê°™ì€ ê²½ìš°ë§Œ ë¡œë“œ
                    if current_state_dict[key].shape == loaded_state_dict[key].shape:
                        compatible_state_dict[key] = loaded_state_dict[key]
                        print(f"   âœ… {key}: ë¡œë“œë¨")
                        loaded_count += 1
                    else:
                        compatible_state_dict[key] = current_state_dict[key]
                        print(f"   âš ï¸ {key}: í¬ê¸° ë¶ˆì¼ì¹˜ ({loaded_state_dict[key].shape} â†’ {current_state_dict[key].shape}), ìƒˆë¡œ ì´ˆê¸°í™”")
                        initialized_count += 1
                else:
                    compatible_state_dict[key] = current_state_dict[key]
                    print(f"   âŒ {key}: ëˆ„ë½, ìƒˆë¡œ ì´ˆê¸°í™”")
                    initialized_count += 1
            
            # ëˆ„ë½ëœ ë ˆì´ì–´ë“¤ í™•ì¸
            missing_in_current = set(loaded_state_dict.keys()) - set(current_state_dict.keys())
            if missing_in_current:
                print(f"   ğŸ“ í˜„ì¬ ëª¨ë¸ì— ì—†ëŠ” ë ˆì´ì–´ë“¤: {len(missing_in_current)}ê°œ")
                for key in sorted(missing_in_current):
                    print(f"      - {key}: {loaded_state_dict[key].shape}")
            
            # í˜¸í™˜ ê°€ëŠ¥í•œ state_dictë¡œ ë¡œë“œ
            try:
                self.q_network.load_state_dict(compatible_state_dict)
                self.target_network.load_state_dict(compatible_state_dict)
                print("   âœ… ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"   âŒ ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
                # ë¶€ë¶„ì  ë¡œë“œë¥¼ ì‹œë„
                try:
                    self.q_network.load_state_dict(compatible_state_dict, strict=False)
                    self.target_network.load_state_dict(compatible_state_dict, strict=False)
                    print("   âš ï¸ ë¶€ë¶„ì  ë¡œë“œë¡œ ë³µêµ¬ë¨")
                except Exception as e2:
                    print(f"   âŒ ë¶€ë¶„ì  ë¡œë“œë„ ì‹¤íŒ¨: {e2}")
                    return False
            
            # ì˜µí‹°ë§ˆì´ì € ë¡œë“œ (í˜¸í™˜ì„± í™•ì¸)
            try:
                if 'optimizer' in checkpoint:
                    self.optimizer.load_state_dict(checkpoint['optimizer'])
                    print("   âœ… ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¡œë“œë¨")
                else:
                    print("   âš ï¸ ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
            except Exception as e:
                print(f"   âš ï¸ ì˜µí‹°ë§ˆì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
            
            # ê¸°íƒ€ íŒŒë¼ë¯¸í„°ë“¤
            self.epsilon = checkpoint.get('epsilon', self.epsilon)
            self.training_rewards = checkpoint.get('training_rewards', [])
            self.losses = checkpoint.get('losses', [])
            self.return_rates = checkpoint.get('return_rates', [])
            self.update_count = checkpoint.get('update_count', 0)
            
            print(f"âœ… {model_state_size}ì°¨ì› â†’ 111ì°¨ì› í˜¸í™˜ì„± ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
            print(f"   - ë¡œë“œëœ ë ˆì´ì–´: {loaded_count}ê°œ")
            print(f"   - ìƒˆë¡œ ì´ˆê¸°í™”ëœ ë ˆì´ì–´: {initialized_count}ê°œ")
            
            # ëª¨ë¸ ê²€ì¦
            try:
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¡œ ëª¨ë¸ì´ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
                test_input = torch.randn(1, self.state_size).to(self.device)
                with torch.no_grad():
                    _ = self.q_network(test_input)
                print("   âœ… ëª¨ë¸ ê²€ì¦ ì™„ë£Œ - ì •ìƒ ì‘ë™")
            except Exception as e:
                print(f"   âš ï¸ ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
                print("   ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆì§€ë§Œ ì˜ˆìƒê³¼ ë‹¤ë¥´ê²Œ ì‘ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            return True
            
        except Exception as e:
            print(f"âŒ í˜¸í™˜ì„± ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"   ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            import traceback
            print(f"   ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False

    def create_compatible_model(self, old_model_path: str) -> bool:
        """ê¸°ì¡´ ëª¨ë¸ì„ SimpleDQN ì•„í‚¤í…ì²˜ë¡œ ë³€í™˜"""
        try:
            print(f"ğŸ”„ ê¸°ì¡´ ëª¨ë¸ì„ SimpleDQN ì•„í‚¤í…ì²˜ë¡œ ë³€í™˜ ì¤‘...")
            
            # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ
            checkpoint = torch.load(old_model_path, map_location=self.device, weights_only=False)
            old_state_dict = checkpoint['q_network']
            
            # ìƒˆë¡œìš´ DuelingDQN ëª¨ë¸ ìƒì„±
            new_model = DuelingDQN(self.state_size, self.hidden_size).to(self.device)
            new_state_dict = new_model.state_dict()
            
            # í˜¸í™˜ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë§Œ ë³µì‚¬
            compatible_weights = {}
            for key in new_state_dict.keys():
                if key in old_state_dict and new_state_dict[key].shape == old_state_dict[key].shape:
                    compatible_weights[key] = old_state_dict[key]
                    print(f"   âœ… {key}: ë³€í™˜ë¨")
                else:
                    compatible_weights[key] = new_state_dict[key]
                    print(f"   âŒ {key}: ìƒˆë¡œ ì´ˆê¸°í™”")
            
            # ìƒˆë¡œìš´ ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ë¡œë“œ
            new_model.load_state_dict(compatible_weights)
            
            # í˜„ì¬ ì—ì´ì „íŠ¸ì˜ ë„¤íŠ¸ì›Œí¬ êµì²´
            self.q_network = new_model
            self.target_network = DuelingDQN(self.state_size, self.hidden_size).to(self.device)
            self.target_network.load_state_dict(compatible_weights)
            
            print(f"âœ… DuelingDQN ì•„í‚¤í…ì²˜ë¡œ ë³€í™˜ ì™„ë£Œ!")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return False

    def diagnose_model_compatibility(self, model_path: str) -> Dict:
        """ëª¨ë¸ í˜¸í™˜ì„± ì§„ë‹¨"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
            old_state_dict = checkpoint['q_network']
            current_state_dict = self.q_network.state_dict()
            
            diagnosis = {
                'total_old_layers': len(old_state_dict),
                'total_new_layers': len(current_state_dict),
                'compatible_layers': 0,
                'incompatible_layers': 0,
                'missing_layers': 0,
                'compatibility_rate': 0.0
            }
            
            for key in current_state_dict.keys():
                if key in old_state_dict:
                    if current_state_dict[key].shape == old_state_dict[key].shape:
                        diagnosis['compatible_layers'] += 1
                    else:
                        diagnosis['incompatible_layers'] += 1
                else:
                    diagnosis['missing_layers'] += 1
            
            diagnosis['compatibility_rate'] = diagnosis['compatible_layers'] / len(current_state_dict)
            
            return diagnosis
            
        except Exception as e:
            return {'error': str(e)}

class DataLoader:
    """111ì°¨ì› RL Decision ê¸°ë°˜ ë°ì´í„° ë¡œë”© í´ë˜ìŠ¤"""
    
    
    @staticmethod
    def load_signal_data(agent_folder: str = "agent") -> Optional[List[Dict]]:
        """111ì°¨ì›ìš© RL Decision ë°ì´í„° ë¡œë“œ"""
        parquet_files = []
        
        if Path(agent_folder).exists():
            parquet_files = list(Path(agent_folder).glob("*.parquet"))
        
        if parquet_files:
            try:
                print(f"Signal ë°ì´í„° ë¡œë“œ ì¤‘: {parquet_files[0].name}")
                signal_df = pd.read_parquet(parquet_files[0])
                print(f"Signal ë°ì´í„° ë¡œë“œ: {len(signal_df):,}ê°œ ë ˆì½”ë“œ")
                
                return DataLoader._convert_parquet_to_signal_dicts(signal_df)
            
            except Exception as e:
                print(f"Parquet ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print("Parquet íŒŒì¼ì´ ì—†ì–´ ê¸°ë³¸ Signalì„ ìƒì„±í•©ë‹ˆë‹¤.")
        return None
    
    @staticmethod
    def load_signal_data_for_test(agent_folder: str = "agent", max_records: int = 50000) -> Optional[List[Dict]]:
        """í…ŒìŠ¤íŠ¸ìš© Signal ë°ì´í„° ë¡œë“œ (50,000ê°œ ì œí•œ)"""
        parquet_files = []
        
        if Path(agent_folder).exists():
            parquet_files = list(Path(agent_folder).glob("*.parquet"))
        
        if parquet_files:
            try:
                print(f"í…ŒìŠ¤íŠ¸ìš© Signal ë°ì´í„° ë¡œë“œ ì¤‘: {parquet_files[0].name}")
                signal_df = pd.read_parquet(parquet_files[0])
                
                # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ìµœëŒ€ 50,000ê°œë§Œ ì‚¬ìš©
                if len(signal_df) > max_records:
                    signal_df = signal_df.tail(max_records)  # ìµœê·¼ 50,000ê°œ ì‚¬ìš©
                    print(f"í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ì œí•œ: {len(signal_df):,}ê°œ ë ˆì½”ë“œ (ì „ì²´ {len(pd.read_parquet(parquet_files[0])):,}ê°œ ì¤‘)")
                else:
                    print(f"Signal ë°ì´í„° ë¡œë“œ: {len(signal_df):,}ê°œ ë ˆì½”ë“œ")
                
                return DataLoader._convert_parquet_to_signal_dicts(signal_df)
                
            except Exception as e:
                print(f"Parquet ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print("Parquet íŒŒì¼ì´ ì—†ì–´ ê¸°ë³¸ Signalì„ ìƒì„±í•©ë‹ˆë‹¤.")
        return None
    
    @staticmethod
    def _convert_parquet_to_signal_dicts(signal_df: pd.DataFrame) -> List[Dict]:
        """Parquetì„ Signal Dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (111ì°¨ì›ìš©) - ìƒˆë¡œìš´ RL ìŠ¤í‚¤ë§ˆ"""
        signal_data = []
        
        print("111ì°¨ì›ìš© RL ìŠ¤í‚¤ë§ˆ Signal ë°ì´í„° ë³€í™˜ ì¤‘...")
        
        for idx, row in signal_df.iterrows():
            # ê° í–‰ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ìƒˆë¡œìš´ RL ìŠ¤í‚¤ë§ˆ í˜•íƒœ ìœ ì§€)
            signal_dict = {}
            
            for col, value in row.items():
                if pd.notna(value):
                    # ìˆ˜ì¹˜ ë°ì´í„°ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                    signal_dict[col] = value
        
            
            signal_data.append(signal_dict)
            
            if (idx + 1) % 5000 == 0:
                print(f"   ë³€í™˜ ì§„í–‰: {idx + 1:,}/{len(signal_df):,}")
        
        print(f"111ì°¨ì›ìš© RL ìŠ¤í‚¤ë§ˆ Signal ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(signal_data):,}ê°œ")
        return signal_data
    

class PerformanceAnalyzer:
    """111ì°¨ì› RL Decision ê¸°ë°˜ ì„±ëŠ¥ ë¶„ì„ í´ë˜ìŠ¤"""
    
    @staticmethod
    def evaluate_agent(agent: RLAgent, env: TradingEnvironment, num_episodes: int = 10) -> Tuple[List[Dict], Dict]:
        """111ì°¨ì› ì—ì´ì „íŠ¸ ì„±ëŠ¥ í‰ê°€"""
        print(f"111ì°¨ì› ì—ì´ì „íŠ¸ ì„±ëŠ¥ í‰ê°€ ì¤‘ ({num_episodes} ì—í”¼ì†Œë“œ)...")
        
        original_epsilon = agent.epsilon
        agent.epsilon = 0.1  # í…ŒìŠ¤íŠ¸ì—ì„œë„ ì ì ˆí•œ íƒí—˜ í—ˆìš© (í›ˆë ¨ê³¼ ìœ ì‚¬)
        
        results = []
        all_trades = []
        
        for episode in range(num_episodes):
            # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì™„ì „ ì´ˆê¸°í™”
            state, _ = env.reset()
            episode_reward = 0
            episode_trades = []
            episode_balance = env.initial_balance
            
            # í…ŒìŠ¤íŠ¸ í™˜ê²½ ìƒíƒœ í™•ì¸
            print(f"   ì—í”¼ì†Œë“œ {episode+1}: ì´ˆê¸° ì”ê³  ${episode_balance:.0f}")
            
            for step in range(200):  # í›ˆë ¨ê³¼ ë™ì¼í•œ ìŠ¤í… ìˆ˜
                action = agent.act(state)
                next_state, reward, done, truncated, info = env.step(action)
                
                episode_reward += reward
                episode_balance = info['balance']
                
                if info.get('trade_completed', False):
                    trade_pnl = info.get('trade_pnl', 0.0)
                    # trade_pnlì€ ì´ë¯¸ ìˆ˜ìˆ˜ë£Œê°€ ì°¨ê°ëœ ì‹¤ì œ ìˆ˜ìµë¥ 
                    episode_trades.append(1 if trade_pnl > 0 else 0)
                
                state = next_state
                if done:
                    break
            
            episode_return = (episode_balance - env.initial_balance) / env.initial_balance
            win_rate = np.mean(episode_trades) if episode_trades else 0.0
            
            # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
            print(f"   ì—í”¼ì†Œë“œ {episode+1}: ê±°ë˜ {len(episode_trades)}ê°œ, ìŠ¹ë¥  {win_rate:.1%}, ìˆ˜ìµë¥  {episode_return:.1%}, ì”ê³  ${episode_balance:.0f}")
            
            results.append({
                'episode': episode,
                'total_reward': episode_reward,
                'final_balance': episode_balance,
                'return': episode_return,
                'win_rate': win_rate,
                'total_trades': len(episode_trades),
                'max_drawdown': info.get('max_drawdown', 0.0)
            })
            
            all_trades.extend(episode_trades)
        
        agent.epsilon = original_epsilon
        
        overall_stats = {
            'avg_return': np.mean([r['return'] for r in results]),
            'avg_reward': np.mean([r['total_reward'] for r in results]),
            'overall_win_rate': np.mean(all_trades) if all_trades else 0.0,
            'avg_trades_per_episode': np.mean([r['total_trades'] for r in results]),
            'avg_max_drawdown': np.mean([r['max_drawdown'] for r in results]),
            'consistency': 1.0 - np.std([r['return'] for r in results]) if len(results) > 1 else 1.0,
            'total_trades': len(all_trades),
            'model_dimension': agent.state_size
        }
        
        return results, overall_stats
    
    @staticmethod
    def print_performance_report(results: List[Dict], stats: Dict):
        """111ì°¨ì› ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        print("\n" + "="*60)
        print(f"111ì°¨ì› RL Decision ê¸°ë°˜ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼")
        print("="*60)
        print(f"ëª¨ë¸ ì°¨ì›: {stats['model_dimension']}ì°¨ì›")
        print(f"ì „ì²´ ìŠ¹ë¥ : {stats['overall_win_rate']:.3f}")
        print(f"í‰ê·  ìˆ˜ìµë¥ : {stats['avg_return']:.3f} ({stats['avg_return']*100:.1f}%)")
        print(f"í‰ê·  ë¦¬ì›Œë“œ: {stats['avg_reward']:.1f}")
        print(f"ì—í”¼ì†Œë“œë‹¹ í‰ê·  ê±°ë˜ ìˆ˜: {stats['avg_trades_per_episode']:.1f}")
        print(f"í‰ê·  ìµœëŒ€ ë‚™í­: {stats['avg_max_drawdown']:.3f}")
        print(f"ì„±ê³¼ ì¼ê´€ì„±: {stats['consistency']:.3f}")
        print(f"ì´ ê±°ë˜ ìˆ˜: {stats['total_trades']}")
        
        grade = PerformanceAnalyzer._get_performance_grade(stats)
        print(f"\nì„±ëŠ¥ ë“±ê¸‰: {grade}")
        
        recommendations = PerformanceAnalyzer._get_recommendations(stats)
        print("\nê°œì„  ì œì•ˆ:")
        for rec in recommendations:
            print(f"  â€¢ {rec}")
    
    @staticmethod
    def _get_performance_grade(stats: Dict) -> str:
        """ì„±ëŠ¥ ë“±ê¸‰ ì‚°ì¶œ (ìˆ˜ìµë¥  ì¤‘ì‹¬)"""
        avg_return = stats['avg_return']
        consistency = stats['consistency']
        win_rate = stats['overall_win_rate']  # ë³´ì¡° ì§€í‘œ
        
        score = 0
        
        # ìˆ˜ìµë¥ ì´ ì£¼ìš” í‰ê°€ ê¸°ì¤€ (ê°€ì¤‘ì¹˜ ë†’ìŒ)
        if avg_return >= 0.20: score += 5  # 20% ì´ìƒ
        elif avg_return >= 0.15: score += 4  # 15% ì´ìƒ
        elif avg_return >= 0.10: score += 3  # 10% ì´ìƒ
        elif avg_return >= 0.05: score += 2  # 5% ì´ìƒ
        elif avg_return >= 0.0: score += 1   # 0% ì´ìƒ
        
        # ì¼ê´€ì„± (ì¤‘ìš”í•˜ì§€ë§Œ ìˆ˜ìµë¥ ë³´ë‹¤ ë‚®ì€ ê°€ì¤‘ì¹˜)
        if consistency >= 0.8: score += 2
        elif consistency >= 0.6: score += 1
        
        # ìŠ¹ë¥  (ë³´ì¡° ì§€í‘œ)
        if win_rate >= 0.6: score += 1
        
        grades = {8: "A+ (ìš°ìˆ˜)", 7: "A (ì¢‹ìŒ)", 6: "B+ (ì–‘í˜¸)", 5: "B (ë³´í†µ)", 
                 4: "C+ (ë¯¸í¡)", 3: "C (ê°œì„ í•„ìš”)", 2: "D (ë‚˜ì¨)", 1: "F (ë§¤ìš°ë‚˜ì¨)", 0: "F (ì‹¤íŒ¨)"}
        
        return grades.get(score, "F (ì‹¤íŒ¨)")
    
    @staticmethod
    def _get_recommendations(stats: Dict) -> List[str]:
        """ì„±ëŠ¥ ê¸°ë°˜ ê°œì„  ì œì•ˆ (ìˆ˜ìµë¥  ì¤‘ì‹¬)"""
        recommendations = []
        
        # ìˆ˜ìµë¥ ì´ ì£¼ìš” ê¸°ì¤€
        if stats['avg_return'] < 0.05:
            recommendations.append("ìˆ˜ìµë¥ ì´ 5% ë¯¸ë§Œì…ë‹ˆë‹¤. ìˆ˜ìµë¥  ì¤‘ì‹¬ ë³´ìƒ í•¨ìˆ˜ë¥¼ ë” ê°•í™”í•˜ì„¸ìš”.")
        
        if stats['avg_return'] < 0.10:
            recommendations.append("ìˆ˜ìµë¥ ì´ 10% ë¯¸ë§Œì…ë‹ˆë‹¤. 111ì°¨ì› ìƒíƒœ ê³µê°„ì˜ ìˆ˜ìµë¥  ìµœì í™”ë¥¼ ë” í™œìš©í•˜ì„¸ìš”.")
        
        if stats['avg_return'] < 0.15:
            recommendations.append("ìˆ˜ìµë¥ ì´ 15% ë¯¸ë§Œì…ë‹ˆë‹¤. Signal ê¸°ë°˜ ìˆ˜ìµë¥  ì˜ˆì¸¡ì„ ê°œì„ í•˜ì„¸ìš”.")
        
        # ë¦¬ìŠ¤í¬ ê´€ë¦¬
        if stats['avg_max_drawdown'] > 0.2:
            recommendations.append("ìµœëŒ€ ë‚™í­ì´ í½ë‹ˆë‹¤. ìˆ˜ìµë¥ ê³¼ ë¦¬ìŠ¤í¬ì˜ ê· í˜•ì„ ë§ì¶”ì„¸ìš”.")
        
        # ì¼ê´€ì„±
        if stats['consistency'] < 0.5:
            recommendations.append("ì„±ê³¼ ì¼ê´€ì„±ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤. ìˆ˜ìµë¥  ì•ˆì •ì„±ì„ ìœ„í•œ ë” ë§ì€ í›ˆë ¨ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ê±°ë˜ ë¹ˆë„
        if stats['avg_trades_per_episode'] < 3:
            recommendations.append("ê±°ë˜ ë¹ˆë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ìˆ˜ìµë¥  ê¸°íšŒë¥¼ ë†“ì¹˜ì§€ ì•Šë„ë¡ Signal ê°ë„ë¥¼ ì¡°ì •í•˜ì„¸ìš”.")
        
        # ìŠ¹ë¥ ì€ ë³´ì¡° ì§€í‘œ
        if stats['overall_win_rate'] < 0.4:
            recommendations.append("ìŠ¹ë¥ ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤. ìˆ˜ìµë¥ ê³¼ ìŠ¹ë¥ ì˜ ê· í˜•ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        if not recommendations:
            recommendations.append("111ì°¨ì› RL Decision ê¸°ë°˜ ìˆ˜ìµë¥  ì¤‘ì‹¬ ì‹œìŠ¤í…œì´ ì˜ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤!")
        
        return recommendations

class TrainingManager:
    """111ì°¨ì› RL Decision ê¸°ë°˜ í›ˆë ¨ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def train_agent(agent: RLAgent, train_env: TradingEnvironment, 
                   episodes: int = 1000, save_interval: int = 100, 
                   test_env: TradingEnvironment = None) -> Tuple[RLAgent, List[float], List[float]]:
        """111ì°¨ì› RL Decision ê¸°ë°˜ ì—ì´ì „íŠ¸ í›ˆë ¨ (í…ŒìŠ¤íŠ¸ í™˜ê²½ ëª¨ë‹ˆí„°ë§ í¬í•¨)"""
        print(f"111ì°¨ì› RL Decision ê¸°ë°˜ ê°•í™”í•™ìŠµ í›ˆë ¨ ì‹œì‘ ({episodes} ì—í”¼ì†Œë“œ)")
        print(f"ìƒíƒœ ê³µê°„: {train_env.observation_space.shape[0]}ì°¨ì›")
        if test_env:
            print(f"í…ŒìŠ¤íŠ¸ í™˜ê²½ ëª¨ë‹ˆí„°ë§: í™œì„±í™”")
        
        # ì—ì´ì „íŠ¸ì— ì•¡ì…˜ ê³µê°„ ì„¤ì •
        agent.action_space = train_env.action_space
        
        episode_rewards = []
        episode_win_rates = []
        episode_returns = []  # í›ˆë ¨ ë°ì´í„° ìˆ˜ìµë¥  ì¶”ì 
        test_return_rates = []  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìˆ˜ìµë¥  ì¶”ì 
        best_return_rate = 0.0
        best_test_return_rate = 0.0
        
        for episode in range(episodes):
            
            state, _ = train_env.reset()
            episode_start_balance = train_env.balance  # ì—í”¼ì†Œë“œ ì‹œì‘ ì”ê³  ì¶”ì 
            total_reward = 0
            episode_trades = []
            steps = 0
            
            while steps < 200:
                action = agent.act(state)
                next_state, reward, done, truncated, info = train_env.step(action)
                
                agent.remember(state, action, reward, next_state, done)
                
                if info.get('trade_completed', False):
                    trade_pnl = info.get('trade_pnl')
                    # trade_pnlì€ ì´ë¯¸ ìˆ˜ìˆ˜ë£Œê°€ ì°¨ê°ëœ ì‹¤ì œ ìˆ˜ìµë¥ 
                    episode_trades.append(1 if trade_pnl > 0 else 0)
                
                state = next_state
                total_reward += reward
                steps += 1
                
                if len(agent.memory) > agent.batch_size:
                    agent.replay()
                
                if done:
                    break
            
            episode_rewards.append(total_reward)
            episode_win_rate = np.mean(episode_trades) if episode_trades else 0.0
            episode_win_rates.append(episode_win_rate)
            
            # í›ˆë ¨ ë°ì´í„° ìˆ˜ìµë¥  ê³„ì‚° (ì”ê³  ë³€í™” ê¸°ë°˜)
            final_balance = info.get('balance', episode_start_balance)
            episode_return = (final_balance - episode_start_balance) / episode_start_balance
            episode_returns.append(episode_return)
            
            # ë³´ìƒ ì •ë³´ ì¶œë ¥ (ì²˜ìŒ 5ê°œ ì—í”¼ì†Œë“œë§Œ)
            if episode < 5:
                print(f"\nğŸ” Episode {episode} ë³´ìƒ ë¶„ì„:")
                print(f"   ì´ ë¦¬ì›Œë“œ: {total_reward:.2f}")
                print(f"   ìˆ˜ìµë¥ : {episode_return:.4f} ({episode_return*100:+.2f}%)")
                print(f"   ê±°ë˜ ìˆ˜: {len(episode_trades)}ê°œ")
            
            
            agent.training_rewards.append(total_reward)
            agent.return_rates.append(episode_return)
            
            
            # ì ì‘í˜• í•™ìŠµë¥  ì—…ë°ì´íŠ¸ (10ì—í”¼ì†Œë“œë§ˆë‹¤)
            if episode % 10 == 0 and episode > 0:
                recent_rewards = episode_rewards[-20:] if len(episode_rewards) >= 20 else episode_rewards
                recent_return_rates = episode_returns[-20:] if len(episode_returns) >= 20 else episode_returns
                agent.adaptive_learning_rate(recent_rewards, recent_return_rates)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€ (ê³¼ì í•© ë°©ì§€ ê°•í™”)
            if test_env and episode % 20 == 0 and episode > 0:  # 20ì—í”¼ì†Œë“œë§ˆë‹¤ í‰ê°€
                print(f"\nğŸ“Š Episode {episode}: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
                test_results, test_stats = PerformanceAnalyzer.evaluate_agent(agent, test_env, num_episodes=5)  # ë” ë§ì€ ì—í”¼ì†Œë“œë¡œ í‰ê°€
                test_return = test_stats['avg_return']
                test_return_rates.append(test_return)
                
                print(f"   í…ŒìŠ¤íŠ¸ ìˆ˜ìµë¥ : {test_return:.3f} ({test_return*100:.1f}%) (ì´ì „ ìµœê³ : {best_test_return_rate:.3f})")
                
                # ê³¼ì í•© ê°ì§€: í›ˆë ¨ ìˆ˜ìµë¥ ê³¼ í…ŒìŠ¤íŠ¸ ìˆ˜ìµë¥  ì°¨ì´ í™•ì¸
                recent_train_return = np.mean(episode_returns[-10:]) if len(episode_returns) >= 10 else 0.0
                overfitting_gap = abs(recent_train_return - test_return)
                
                # ê³¼ì í•© ê°ì§€: í›ˆë ¨ì´ í…ŒìŠ¤íŠ¸ë³´ë‹¤ í˜„ì €íˆ ì¢‹ì„ ë•Œ
                if overfitting_gap > 0.15:
                    print(f"âš ï¸ ê³¼ì í•© ê°ì§€: í›ˆë ¨ ìˆ˜ìµë¥ ({recent_train_return:.3f}) vs í…ŒìŠ¤íŠ¸ ìˆ˜ìµë¥ ({test_return:.3f}) = ì°¨ì´ {overfitting_gap:.3f}")
                    # í•™ìŠµë¥  ê°ì†Œ ë° ì—¡ì‹¤ë¡  ì¦ê°€
                    for param_group in agent.optimizer.param_groups:
                        param_group['lr'] *= 0.8
                    agent.epsilon = min(agent.epsilon * 1.2, 0.8)  # ë” ë§ì€ íƒí—˜
                    print(f"   í•™ìŠµë¥  ê°ì†Œ: {agent.optimizer.param_groups[0]['lr']:.2e}, ì—¡ì‹¤ë¡  ì¦ê°€: {agent.epsilon:.3f}")
                
                # í•™ìŠµ ë¶€ì¡± ê°ì§€: í›ˆë ¨ ìˆ˜ìµë¥ ì´ í…ŒìŠ¤íŠ¸ë³´ë‹¤ í˜„ì €íˆ ë‚®ì„ ë•Œ
                elif recent_train_return < test_return - 0.1:
                    print(f"âš ï¸ í•™ìŠµ ë¶€ì¡±: í›ˆë ¨ ìˆ˜ìµë¥ ({recent_train_return:.3f}) < í…ŒìŠ¤íŠ¸ ìˆ˜ìµë¥ ({test_return:.3f})")
                    # í•™ìŠµë¥  ì¦ê°€ ë° ì—¡ì‹¤ë¡  ì¡°ì •
                    for param_group in agent.optimizer.param_groups:
                        param_group['lr'] *= 1.1
                    agent.epsilon = max(agent.epsilon * 0.95, 0.2)  # ì ì ˆí•œ íƒí—˜
                    print(f"   í•™ìŠµë¥  ì¦ê°€: {agent.optimizer.param_groups[0]['lr']:.2e}, ì—¡ì‹¤ë¡  ì¡°ì •: {agent.epsilon:.3f}")
                
                # ì •ìƒì ì¸ í•™ìŠµ ê³¼ì •
                else:
                    print(f"â„¹ï¸ ì •ìƒ í•™ìŠµ: í›ˆë ¨({recent_train_return:.3f}) vs í…ŒìŠ¤íŠ¸({test_return:.3f}) - ì°¨ì´ {overfitting_gap:.3f}")
                
                if test_return > best_test_return_rate:
                    best_test_return_rate = test_return
                    # ì—í”¼ì†Œë“œë³„ ëª¨ë¸ ì €ì¥ (ìˆ˜ìµë¥  ê¸°ì¤€)
                    # agent.save_model(f'best_test_model_ep{episode}_return{test_return:.3f}.pth')
                    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì—…ë°ì´íŠ¸ (ìˆ˜ìµë¥  ê¸°ì¤€)
                    agent.save_model('agent/best_test_performance_model_return{:.3f}.pth'.format(test_return))
                    print(f"ğŸ¯ ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìµœê³  ìˆ˜ìµë¥ ! ìˆ˜ìµë¥ : {test_return:.3f} ({test_return*100:.1f}%)")
                    print(f"   ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì—…ë°ì´íŠ¸: best_test_performance_model_return{test_return:.3f}.pth")
                print()  # ë¹ˆ ì¤„ ì¶”ê°€
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥ (20ì—í”¼ì†Œë“œë§ˆë‹¤)
            if episode % 20 == 0 or episode < 10:
                recent_rewards = episode_rewards[-50:] if len(episode_rewards) >= 50 else episode_rewards
                recent_win_rates = episode_win_rates[-50:] if len(episode_win_rates) >= 50 else episode_win_rates
                recent_returns = episode_returns[-50:] if len(episode_returns) >= 50 else episode_returns
                
                # ì‹¤ì œ ë¦¬ì›Œë“œ í‰ê·  (ì¤‘ë³µ ë³´ìƒ ì œê±°ë¨)
                avg_reward = np.mean(recent_rewards)
                avg_win_rate = np.mean(recent_win_rates)
                avg_return = np.mean(recent_returns)
                
                # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ë„ í•¨ê»˜ í‘œì‹œ
                test_info = ""
                if test_return_rates:
                    recent_test_return_rate = np.mean(test_return_rates[-5:]) if len(test_return_rates) >= 5 else test_return_rates[-1]
                    test_info = f" | í…ŒìŠ¤íŠ¸: {recent_test_return_rate:.3f}"
                
                # ìˆ˜ìµë¥ ê³¼ ë¦¬ì›Œë“œ ì¼ì¹˜ì„± í™•ì¸ (ìƒˆë¡œìš´ ë¦¬ì›Œë“œ ë²”ìœ„ì— ë§ì¶¤)
                reward_return_ratio = avg_reward / (avg_return * 100) if avg_return != 0 else 0
                
                print(f"Episode {episode:4d} | "
                        f"í›ˆë ¨ìŠ¹ë¥ : {avg_win_rate:.3f} | "
                        f"í›ˆë ¨ìˆ˜ìµë¥ : {avg_return:.3f} ({avg_return*100:+.1f}%){test_info} | "
                        f"ë¦¬ì›Œë“œ: {avg_reward:7.1f} | "
                        f"ì”ê³ : ${info['balance']:7.0f} | "
                        f"ê±°ë˜: {info.get('total_trades', 0):3d}ê°œ | "
                        f"Îµ: {agent.epsilon:.3f} | "
                        f"LR: {agent.learning_rate:.2e} | "
                        f"111D")
            
            # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥ (í›ˆë ¨ ë°ì´í„° ê¸°ì¤€ - ìˆ˜ìµë¥  ì¤‘ì‹¬)
            if episode % save_interval == 0 and episode > 0:
                # ìµœê·¼ 100 ì—í”¼ì†Œë“œì˜ í‰ê·  ìˆ˜ìµë¥  ê³„ì‚°
                recent_returns = []
                for i in range(max(0, len(episode_rewards)-100), len(episode_rewards)):
                    if i < len(episode_rewards):
                        # ê°„ë‹¨í•œ ìˆ˜ìµë¥  ì¶”ì • (ë¦¬ì›Œë“œ ê¸°ë°˜)
                        estimated_return = episode_rewards[i] / 1000.0  # ë¦¬ì›Œë“œë¥¼ ìˆ˜ìµë¥ ë¡œ ê·¼ì‚¬
                        recent_returns.append(estimated_return)
                
                current_avg_return = np.mean(recent_returns) if recent_returns else 0.0
                
                if current_avg_return > best_return_rate:  # ìˆ˜ìµë¥  ê¸°ì¤€ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ ì¶”ì 
                    best_return_rate = current_avg_return
                    agent.save_model(f'best_train_model_ep{episode}_return{current_avg_return:.3f}.pth')
                    print(f"ğŸ¯ ìƒˆë¡œìš´ í›ˆë ¨ ë°ì´í„°ì…‹ ìµœê³  ìˆ˜ìµë¥ ! ìˆ˜ìµë¥ : {current_avg_return:.3f} ({current_avg_return*100:.1f}%)")
            
            # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ (ê³¼ì í•© ë°©ì§€ ê°•í™”)
            if episode > 500 and test_return_rates:
                # ìµœê·¼ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë“¤ì˜ ìˆ˜ìµë¥  í™•ì¸
                recent_test_returns = test_return_rates[-5:] if len(test_return_rates) >= 5 else test_return_rates
                
                recent_test_return = np.mean(recent_test_returns) if recent_test_returns else 0.0
                
                # ê³¼ì í•© ê°ì§€ ì‹œ ì¡°ê¸° ì¢…ë£Œ
                if len(episode_returns) >= 20:
                    recent_train_return = np.mean(episode_returns[-20:])
                    overfitting_gap = recent_train_return - recent_test_return
                    
                    if overfitting_gap > 0.15:  # ê³¼ì í•©ì´ ì‹¬í•˜ë©´ ì¡°ê¸° ì¢…ë£Œ
                        print(f"ğŸ›‘ ê³¼ì í•©ìœ¼ë¡œ ì¸í•œ ì¡°ê¸° ì¢…ë£Œ: í›ˆë ¨ ìˆ˜ìµë¥ ({recent_train_return:.3f}) - í…ŒìŠ¤íŠ¸ ìˆ˜ìµë¥ ({recent_test_return:.3f}) = {overfitting_gap:.3f}")
                        agent.save_model('agent/early_stop_model.pth')
                    break
        
                if recent_test_return >= 0.20:  # ìˆ˜ìµë¥  20% ì´ìƒ ë‹¬ì„±
                    print(f"ğŸ† 111ì°¨ì› ëª©í‘œ ë‹¬ì„±! í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìˆ˜ìµë¥  {recent_test_return:.3f} ({recent_test_return*100:.1f}%) ë„ë‹¬")
                    agent.save_model('agent/final_optimized_model_111d.pth')
                    break
        
        
        print(f"\n111ì°¨ì› RL Decision ê¸°ë°˜ í›ˆë ¨ ì™„ë£Œ!")
        print(f"   ì´ ì—í”¼ì†Œë“œ: {episode + 1}")
        print(f"   í›ˆë ¨ ë°ì´í„° ìµœê³  ìˆ˜ìµë¥ : {best_return_rate:.3f}")
        print(f"   í›ˆë ¨ ë°ì´í„° ìµœì¢… ìˆ˜ìµë¥ : {np.mean(episode_returns[-50:]) if episode_returns else 0:.3f}")
        if test_return_rates:
            print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„° ìµœê³  ìˆ˜ìµë¥ : {best_test_return_rate:.3f}")
            print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„° ìµœì¢… ìˆ˜ìµë¥ : {test_return_rates[-1]:.3f}")
        print(f"   ìƒíƒœ ì°¨ì›: 111ì°¨ì› (RL Decision ê¸°ë°˜)")
        print(f"   ì•„í‚¤í…ì²˜: DuelingDQN (Value + Advantage ë¶„ë¦¬)")
        print(f"   ì •ê·œí™” ê¸°ë²•: ì—”íŠ¸ë¡œí”¼ ì •ê·œí™”, Spectral Normalization, ì ì‘ì  ë“œë¡­ì•„ì›ƒ")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if test_return_rates and best_test_return_rate > 0:
            best_test_model_path = f'agent/best_test_performance_model_return{best_test_return_rate:.3f}.pth'
            agent.save_model(best_test_model_path)
            print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {best_test_model_path}")
        
        return agent, episode_rewards, episode_win_rates


def split_signal_data(signal_data: List[Dict], 
                     train_ratio: float = 0.8, test_ratio: float = 0.2) -> Tuple[List[Dict], List[Dict]]:
    """ì‹ í˜¸ ë°ì´í„°ë¥¼ í›ˆë ¨ìš©ê³¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¶„í• """
    total_length = len(signal_data)
    train_size = int(total_length * train_ratio)
    
    # í›ˆë ¨ ë°ì´í„°
    train_signal = signal_data[:train_size]
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_signal = signal_data[train_size:]
    
    print(f"ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
    print(f"  - í›ˆë ¨ ë°ì´í„°: {len(train_signal):,}ê°œ ({train_ratio*100:.1f}%)")
    print(f"  - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_signal):,}ê°œ ({test_ratio*100:.1f}%)")
    
    return train_signal, test_signal

def main():
    """111ì°¨ì› RL Decision ê¸°ë°˜ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("111ì°¨ì› RL Decision ê¸°ë°˜ ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œ")
    print("=" * 80)
    
    try:
        # 1. ë°ì´í„° ë¡œë”© (OHLC í¬í•¨ëœ ì‹ í˜¸ ë°ì´í„°ë§Œ ì‚¬ìš©)
        print("\n1ï¸âƒ£ 111ì°¨ì›ìš© ë°ì´í„° ë¡œë”© (OHLC í¬í•¨)...")
        signal_data = DataLoader.load_signal_data()  # í…ŒìŠ¤íŠ¸ìš© 50,000ê°œ ì œí•œ
        if signal_data is None:
            print("ì‹ í˜¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
            return
        
        print(f"ì‹ í˜¸ ë°ì´í„° ë¡œë“œ: {len(signal_data):,}ê°œ (OHLC í¬í•¨)")
        
        # 2. ë°ì´í„° ë¶„í•  (í›ˆë ¨ 80%, í…ŒìŠ¤íŠ¸ 20%)
        print("\n2ï¸âƒ£ ë°ì´í„° ë¶„í• ...")
        train_signal, test_signal = split_signal_data(signal_data, 0.8, 0.2)
        
        # 3. í™˜ê²½ ë° ì—ì´ì „íŠ¸ ìƒì„±
        print("\n3ï¸âƒ£ 111ì°¨ì› í™˜ê²½ ë° ì—ì´ì „íŠ¸ ìƒì„±...")
        train_env = TradingEnvironment(train_signal)
        test_env = TradingEnvironment(test_signal)
        agent = RLAgent(train_env.observation_space.shape[0])  # 111ì°¨ì›
        
        # í™˜ê²½ ì„¤ì • ë¹„êµ ë””ë²„ê¹…
        print(f"\nğŸ” í™˜ê²½ ì„¤ì • ë¹„êµ:")
        print(f"   í›ˆë ¨ í™˜ê²½:")
        print(f"     - ë°ì´í„° í¬ê¸°: {len(train_signal):,}ê°œ")
        print(f"     - ê±°ë˜ ê°„ê²©: {train_env.min_trade_interval}")
        print(f"     - ê±°ë˜ ìˆ˜ìˆ˜ë£Œ: {train_env.trading_cost:.4f}")
        print(f"     - ì´ˆê¸° ì”ê³ : ${train_env.initial_balance:,.0f}")
        print(f"   í…ŒìŠ¤íŠ¸ í™˜ê²½:")
        print(f"     - ë°ì´í„° í¬ê¸°: {len(test_signal):,}ê°œ")
        print(f"     - ê±°ë˜ ê°„ê²©: {test_env.min_trade_interval}")
        print(f"     - ê±°ë˜ ìˆ˜ìˆ˜ë£Œ: {test_env.trading_cost:.4f}")
        print(f"     - ì´ˆê¸° ì”ê³ : ${test_env.initial_balance:,.0f}")
        
        # í™˜ê²½ ì„¤ì • ì¼ì¹˜ í™•ì¸
        env_mismatch = []
        if train_env.min_trade_interval != test_env.min_trade_interval:
            env_mismatch.append(f"ê±°ë˜ ê°„ê²©: {train_env.min_trade_interval} vs {test_env.min_trade_interval}")
        if train_env.trading_cost != test_env.trading_cost:
            env_mismatch.append(f"ê±°ë˜ ìˆ˜ìˆ˜ë£Œ: {train_env.trading_cost} vs {test_env.trading_cost}")
        if train_env.initial_balance != test_env.initial_balance:
            env_mismatch.append(f"ì´ˆê¸° ì”ê³ : {train_env.initial_balance} vs {test_env.initial_balance}")
        
        if env_mismatch:
            print(f"   âš ï¸ í™˜ê²½ ì„¤ì • ë¶ˆì¼ì¹˜:")
            for mismatch in env_mismatch:
                print(f"     - {mismatch}")
        else:
            print(f"   âœ… í™˜ê²½ ì„¤ì • ì¼ì¹˜")
        
        print(f"ìƒíƒœ ê³µê°„: {train_env.observation_space.shape[0]}ì°¨ì›")
        print("Signalì˜ ëª¨ë“  indicatorì™€ raw score í™œìš©")
        
        model_loaded = False
        
        # 1. í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ëª¨ë¸ ìš°ì„  ë¡œë“œ
        import glob
        test_model_files = glob.glob('agent/best_test_performance_model_return*.pth')
        if test_model_files:
            # ê°€ì¥ ë†’ì€ ìŠ¹ë¥ ì˜ í…ŒìŠ¤íŠ¸ ëª¨ë¸ ì„ íƒ
            best_test_model = max(test_model_files, key=lambda x: float(x.split('return')[1].split('.pth')[0]))
            if agent.load_model(best_test_model):
                model_loaded = True
                print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ: {best_test_model}")
        
        # 2. í˜¸í™˜ì„± ëª¨ë“œë¡œ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹œë„
        if not model_loaded:
            for model_file in ['agent/final_optimized_model_111d.pth', 'agent/best_model_111d.pth']:
                if os.path.exists(model_file):
                    print(f"ğŸ”„ í˜¸í™˜ì„± ëª¨ë“œë¡œ {model_file} ë¡œë“œ ì‹œë„...")
                    
                    # ëª¨ë¸ í˜¸í™˜ì„± ì§„ë‹¨
                    diagnosis = agent.diagnose_model_compatibility(model_file)
                    if 'error' not in diagnosis:
                        print(f"   ğŸ“Š í˜¸í™˜ì„± ì§„ë‹¨: {diagnosis['compatibility_rate']:.1%} ({diagnosis['compatible_layers']}/{diagnosis['total_new_layers']} ë ˆì´ì–´)")
                    
                    if agent.load_model_with_compatibility(model_file):
                        model_loaded = True
                        print(f"âœ… í˜¸í™˜ì„± ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_file}")
                        break
        
        # 3. ëª¨ë¸ ë³€í™˜ ì‹œë„ (ê¸°ì¡´ ëª¨ë¸ì„ AdvancedProfitDQN ì•„í‚¤í…ì²˜ë¡œ)
        if not model_loaded:
            for model_file in ['agent/final_optimized_model_111d.pth', 'agent/best_model_111d.pth']:
                if os.path.exists(model_file):
                    print(f"ğŸ”„ ëª¨ë¸ ë³€í™˜ ì‹œë„: {model_file}")
                    if agent.create_compatible_model(model_file):
                        model_loaded = True
                        print(f"âœ… ëª¨ë¸ ë³€í™˜ ì„±ê³µ: {model_file}")
                        break
        
        if not model_loaded:
            print("ìƒˆë¡œìš´ 111ì°¨ì› ëª¨ë¸ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        # 4. í›ˆë ¨ ì „ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€ (ë² ì´ìŠ¤ë¼ì¸)
        print("\n4ï¸âƒ£ í›ˆë ¨ ì „ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€...")
        baseline_results, baseline_stats = PerformanceAnalyzer.evaluate_agent(agent, test_env, num_episodes=5)
        print("=== í›ˆë ¨ ì „ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ ===")
        PerformanceAnalyzer.print_performance_report(baseline_results, baseline_stats)
        
        # 5. í›ˆë ¨ ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨
        print(f"\n5ï¸âƒ£ í›ˆë ¨ ë°ì´í„°ì…‹ìœ¼ë¡œ 111ì°¨ì› RL Decision ê¸°ë°˜ í›ˆë ¨ ì‹œì‘...")
        print(f"   í›ˆë ¨ ë°ì´í„°: {len(train_signal):,}ê°œ")
        print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_signal):,}ê°œ")
        print(f"   ëª©í‘œ ìˆ˜ìµë¥ : 5%+ (ìˆ˜ìµë¥  ì¤‘ì‹¬)")
        print(f"   Signal íŠ¹ì„± í™œìš©: ìˆ˜ìµë¥  ìµœì í™”")
        
        # í›ˆë ¨ ì‹¤í–‰ (ê³¼ì í•© ë°©ì§€ ê°•í™”)
        trained_agent, rewards, win_rates = TrainingManager.train_agent(agent, train_env, episodes=10000, test_env=test_env)
        
        # 6. í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€
        print("\n6ï¸âƒ£ í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€...")
        final_results, final_stats = PerformanceAnalyzer.evaluate_agent(trained_agent, test_env, num_episodes=10)
        print("=== í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ ===")
        PerformanceAnalyzer.print_performance_report(final_results, final_stats)
        
        # 7. ì„±ëŠ¥ ê°œì„ ë„ ë¶„ì„
        improvement = final_stats['avg_return'] - baseline_stats['avg_return']
        print(f"\nğŸš€ 111ì°¨ì› RL Decision ê¸°ë°˜ ì„±ëŠ¥ ê°œì„ ë„ (í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê¸°ì¤€):")
        print(f"   ìˆ˜ìµë¥ : {baseline_stats['avg_return']:.3f} â†’ {final_stats['avg_return']:.3f} ({improvement:+.3f})")
        print(f"   ìŠ¹ë¥ : {baseline_stats['overall_win_rate']:.3f} â†’ {final_stats['overall_win_rate']:.3f}")
        print(f"   Signal í™œìš©ë„: ìµœëŒ€í™”ë¨")
        
        # 8. ìµœì¢… ëª¨ë¸ ì €ì¥
        trained_agent.save_model('agent/final_optimized_model_111d.pth')
        print(f"\nâœ… ìµœì¢… ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: agent/final_optimized_model_111d.pth")
        
        # 9. ì¶”ê°€ í›ˆë ¨ ì—¬ë¶€ í™•ì¸ (ìˆ˜ìµë¥  ê¸°ì¤€)
        if final_stats['avg_return'] < 0.30:  # ìˆ˜ìµë¥  30% ë¯¸ë§Œ
            user_input = input("\nìˆ˜ìµë¥ ì´ ëª©í‘œ(30%)ì— ë¯¸ë‹¬í•©ë‹ˆë‹¤. ì¶”ê°€ í›ˆë ¨ì„ ì›í•˜ì‹œë‚˜ìš”? (y/n): ")
            if user_input.lower() == 'y':
                print("111ì°¨ì› ìˆ˜ìµë¥  ì¤‘ì‹¬ ì¶”ê°€ í›ˆë ¨ ì‹œì‘...")
                TrainingManager.train_agent(trained_agent, train_env, episodes=5000, test_env=test_env)
                
                # ì¶”ê°€ í›ˆë ¨ í›„ ì¬í‰ê°€
                print("\nì¶”ê°€ í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€...")
                additional_results, additional_stats = PerformanceAnalyzer.evaluate_agent(trained_agent, test_env, num_episodes=10)
                print("=== ì¶”ê°€ í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ ===")
                PerformanceAnalyzer.print_performance_report(additional_results, additional_stats)
        else:
            print(f"\nğŸ‰ ëª©í‘œ ìˆ˜ìµë¥  ë‹¬ì„±! (í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìˆ˜ìµë¥ : {final_stats['avg_return']:.3f} ({final_stats['avg_return']*100:.1f}%))")
    
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()