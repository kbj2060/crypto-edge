"""
61ì°¨ì› RL Decision ê¸°ë°˜ ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© AI í›ˆë ¨ ì‹œìŠ¤í…œ - Part 1
- ìƒˆë¡œìš´ RL Decision ìŠ¤í‚¤ë§ˆ í™œìš© (action_value, confidence_value ë“±)
- Conflict ì •ë³´ ë° ì‹œë„ˆì§€ ë©”íƒ€ë°ì´í„° í™œìš©
- ì¤‘ë³µ ê³„ì‚° ì œê±° ë° ì •ë³´ í™œìš© ê·¹ëŒ€í™”
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
import gymnasium as gym
import os

from collections import deque, namedtuple
from gymnasium import spaces
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path

# PyTorch í˜¸í™˜ì„± ì„¤ì •
def setup_pytorch_compatibility():
    """PyTorch ë²„ì „ í˜¸í™˜ì„± ì„¤ì •"""
    try:
        # NumPy 2.0 í˜¸í™˜ì„±ì„ ìœ„í•œ ì•ˆì „í•œ ê¸€ë¡œë²Œ ì„¤ì •
        safe_globals = [
            np.ndarray,
            np.dtype,
            np.float32,
            np.float64,
            np.int32,
            np.int64,
        ]
        
        # numpy.core ëŒ€ì‹  numpy._core ì‚¬ìš© (NumPy 2.0 í˜¸í™˜)
        try:
            import numpy._core.multiarray
            safe_globals.append(numpy._core.multiarray.scalar)
        except ImportError:
            # ì´ì „ ë²„ì „ í˜¸í™˜ì„±
            try:
                import numpy.core.multiarray
                safe_globals.append(numpy.core.multiarray.scalar)
            except ImportError:
                pass
        
        torch.serialization.add_safe_globals(safe_globals)
        print("PyTorch í˜¸í™˜ ì„¤ì • ì™„ë£Œ (NumPy 2.0 í˜¸í™˜)")
    except AttributeError:
        print("PyTorch ì´ì „ ë²„ì „ ê°ì§€ë¨")

setup_pytorch_compatibility()

Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])

class RewardCalculator:
    """ìŠ¹ë¥ ê³¼ ìˆ˜ìµì„±ì„ ìµœì í™”í•˜ëŠ” ë³´ìƒ ê³„ì‚°ê¸° (Signal ê¸°ë°˜)"""
    
    def __init__(self, max_trades_memory: int = 50):
        self.recent_trades = deque(maxlen=max_trades_memory)
        self.baseline_return = 0.0
        
    def calculate_reward(self, current_price: float, entry_price: float, position: float, 
                    action: str, holding_time: int, signal_data: Dict = None,
                    trade_pnl: Optional[float] = None) -> float:
        """ìˆ˜ìµë¥  ì¤‘ì‹¬ ë‹¨ìˆœí™”ëœ ë³´ìƒ ê³„ì‚°"""
        reward = 0.0
        
        # 1. ê±°ë˜ ì™„ë£Œ ì‹œ ìˆ˜ìµë¥  ê¸°ë°˜ ë³´ìƒ (ê°€ì¥ ì¤‘ìš”)
        if trade_pnl is not None:
            self.recent_trades.append(trade_pnl)
            
            # ìˆ˜ìµë¥ ì— ì§ì ‘ ë¹„ë¡€í•˜ëŠ” ë³´ìƒ (ë‹¨ìˆœí™”)
            if trade_pnl > 0:
                # ìˆ˜ìµë¥  1%ë‹¹ 1000ì  ë³´ìƒ
                reward += trade_pnl * 1000
            else:
                # ì†ì‹¤ 1%ë‹¹ 500ì  íŒ¨ë„í‹° (ë³´ìƒë³´ë‹¤ ì‘ê²Œ)
                reward += trade_pnl * 500
        
        # 2. ë¯¸ì‹¤í˜„ ì†ìµ ë³´ìƒ (í¬ì§€ì…˜ ìœ ì§€ ì¤‘)
        elif abs(position) > 0.01:
            unrealized_pnl = self._calculate_unrealized_pnl(current_price, entry_price, position)
            
            # ë¯¸ì‹¤í˜„ ì†ìµì— ë¹„ë¡€í•˜ëŠ” ë³´ìƒ (ê±°ë˜ ì™„ë£Œë³´ë‹¤ ì‘ê²Œ)
            if unrealized_pnl > 0:
                reward += unrealized_pnl * 200  # ìˆ˜ìµë¥  1%ë‹¹ 200ì 
            else:
                reward += unrealized_pnl * 100  # ì†ì‹¤ 1%ë‹¹ 100ì  íŒ¨ë„í‹°
        
        # 3. Signal ì¼ì¹˜ë„ ë³´ìƒ (ë³´ì¡°ì , ê°€ì¤‘ì¹˜ ê°ì†Œ)
        if signal_data and abs(position) > 0.01:
            signal_reward = self._calculate_signal_reward(signal_data, position)
            reward += signal_reward * 0.1  # ê°€ì¤‘ì¹˜ ëŒ€í­ ê°ì†Œ
        
        return reward
    
    def _calculate_unrealized_pnl(self, current_price: float, entry_price: float, position: float) -> float:
        """ë¯¸ì‹¤í˜„ ì†ìµ ê³„ì‚°"""
        if entry_price <= 0:
            return 0.0
        
        price_change = (current_price - entry_price) / entry_price
        return position * price_change
    
    def _calculate_signal_reward(self, signal_data: Dict, position: float) -> float:
        """Signal ë°ì´í„° ê¸°ë°˜ ì¶”ê°€ ë³´ìƒ - ìƒˆë¡œìš´ RL ìŠ¤í‚¤ë§ˆ ê¸°ë°˜"""
        signal_reward = 0.0
        
        # ê° ì‹œê°„ëŒ€ë³„ ì‹ í˜¸ì™€ í¬ì§€ì…˜ ì¼ì¹˜ë„
        for timeframe in ['short_term', 'medium_term', 'long_term']:
            action_value = float(signal_data.get(f'{timeframe}_action_value', 0.0))
            net_score = float(signal_data.get(f'{timeframe}_net_score', 0.0))
            confidence_value = float(signal_data.get(f'{timeframe}_confidence_value', 0.0))
            
            # Action valueì™€ í¬ì§€ì…˜ ì¼ì¹˜ë„ (action_value: -1~1, position: -1~1)
            action_match = 1.0 - abs(action_value - position) / 2.0  # 0~1 ë²”ìœ„
            signal_reward += action_match * abs(net_score) * confidence_value * 0.3
        
        # Conflict ì •ë³´ í™œìš©
        conflict_penalty = float(signal_data.get('conflict_conflict_penalty', 0.0))
        conflict_consensus = float(signal_data.get('conflict_directional_consensus', 0.0))
        
        # Conflictê°€ ì ê³  consensusê°€ ë†’ì„ ë•Œ ë³´ìƒ
        if conflict_penalty == 0.0 and conflict_consensus > 0.5:
            signal_reward += 0.2
        
        return signal_reward

class MultiHeadAttentionBlock(nn.Module):
    """ë‹¤ì¤‘ í—¤ë“œ ì–´í…ì…˜ ë¸”ë¡"""
    
    def __init__(self, hidden_size: int, num_heads: int, dropout: float = 0.1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        
        assert hidden_size % num_heads == 0, "hidden_sizeëŠ” num_headsë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤"
        
        # ì–´í…ì…˜ ë ˆì´ì–´ë“¤
        self.query = nn.Linear(hidden_size, hidden_size)
        self.key = nn.Linear(hidden_size, hidden_size)
        self.value = nn.Linear(hidden_size, hidden_size)
        
        # ì¶œë ¥ í”„ë¡œì ì…˜
        self.output_projection = nn.Linear(hidden_size, hidden_size)
        
        # ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ
        self.layer_norm = nn.LayerNorm(hidden_size)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        batch_size, seq_len, hidden_size = x.shape
        
        # ì–´í…ì…˜ ê³„ì‚°
        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜
        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attention_weights = torch.softmax(attention_scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # ì–´í…ì…˜ ì ìš©
        attention_output = torch.matmul(attention_weights, V)
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, hidden_size
        )
        
        # ì¶œë ¥ í”„ë¡œì ì…˜ ë° ì”ì°¨ ì—°ê²°
        output = self.output_projection(attention_output)
        output = self.layer_norm(x + output)
        
        return output

class AdvancedProfitDQN(nn.Module):
    """ìˆ˜ìµë¥  ìµœì í™”ë¥¼ ìœ„í•œ ê³ ê¸‰ DQN (ì–´í…ì…˜, ì”ì°¨ ì—°ê²°, ë°°ì¹˜ ì •ê·œí™”)"""
    
    def __init__(self, state_size: int, action_size: int = 3, hidden_size: int = 256, 
                 num_heads: int = 8, num_layers: int = 3):
        super().__init__()
        
        self.state_size = state_size
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.num_layers = num_layers
        
        # ì…ë ¥ ì„ë² ë”© ë° ì •ê·œí™” (LayerNorm ì‚¬ìš©ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ë¬¸ì œ í•´ê²°)
        self.input_embedding = nn.Sequential(
            nn.Linear(state_size, hidden_size),
            nn.LayerNorm(hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # ë‹¤ì¤‘ í—¤ë“œ ì–´í…ì…˜ ë¸”ë¡ë“¤
        self.attention_blocks = nn.ModuleList([
            MultiHeadAttentionBlock(hidden_size, num_heads, dropout=0.1)
            for _ in range(num_layers)
        ])
        
        # ì”ì°¨ ì—°ê²°ì„ ìœ„í•œ í”„ë¡œì ì…˜ ë ˆì´ì–´
        self.residual_projections = nn.ModuleList([
            nn.Linear(hidden_size, hidden_size) if i > 0 else nn.Identity()
            for i in range(num_layers)
        ])
        
        # íŠ¹í™”ëœ íŠ¹ì„± ì¶”ì¶œê¸°ë“¤
        self.position_extractor = self._build_specialized_extractor(hidden_size, "position")
        self.leverage_extractor = self._build_specialized_extractor(hidden_size, "leverage")
        self.holding_extractor = self._build_specialized_extractor(hidden_size, "holding")
        self.profit_extractor = self._build_specialized_extractor(hidden_size, "profit")
        
        # ì•¡ì…˜ í—¤ë“œë“¤ (ê°œì„ ëœ êµ¬ì¡°, LayerNorm ì‚¬ìš©)
        self.position_head = nn.Sequential(
            nn.Linear(hidden_size // 2, hidden_size // 4),
            nn.ReLU(),
            nn.LayerNorm(hidden_size // 4),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 4, 101)  # -10.0~10.0 (í¬ì§€ì…˜ í¬ê¸° ëŒ€í­ í™•ëŒ€)
        )
        
        self.leverage_head = nn.Sequential(
            nn.Linear(hidden_size // 2, hidden_size // 4),
            nn.ReLU(),
            nn.LayerNorm(hidden_size // 4),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 4, 20)  # 1~20
        )
        
        self.holding_head = nn.Sequential(
            nn.Linear(hidden_size // 2, hidden_size // 4),
            nn.ReLU(),
            nn.LayerNorm(hidden_size // 4),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 4, 48)  # 30~1440ë¶„
        )
        
        # ìˆ˜ìµë¥  ì˜ˆì¸¡ (ê°œì„ ëœ êµ¬ì¡°, LayerNorm ì‚¬ìš©)
        self.profit_predictor = nn.Sequential(
            nn.Linear(hidden_size // 2, hidden_size // 4),
            nn.ReLU(),
            nn.LayerNorm(hidden_size // 4),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 4, 1)
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.apply(self._init_weights)
    
    def _build_specialized_extractor(self, hidden_size: int, extractor_type: str):
        """íŠ¹í™”ëœ íŠ¹ì„± ì¶”ì¶œê¸° ë¹Œë“œ (LayerNorm ì‚¬ìš©)"""
        if extractor_type == "position":
            # í¬ì§€ì…˜ ê²°ì •ì„ ìœ„í•œ íŠ¹ì„± ì¶”ì¶œ (ê°€ê²©, ëª¨ë©˜í…€ ì¤‘ì‹¬)
            return nn.Sequential(
                nn.Linear(hidden_size, hidden_size // 2),
                nn.ReLU(),
                nn.LayerNorm(hidden_size // 2),
                nn.Dropout(0.1),
                nn.Linear(hidden_size // 2, hidden_size // 2),
                nn.ReLU()
            )
        elif extractor_type == "leverage":
            # ë ˆë²„ë¦¬ì§€ ê²°ì •ì„ ìœ„í•œ íŠ¹ì„± ì¶”ì¶œ (ë³€ë™ì„±, ë¦¬ìŠ¤í¬ ì¤‘ì‹¬)
            return nn.Sequential(
                nn.Linear(hidden_size, hidden_size // 2),
                nn.ReLU(),
                nn.LayerNorm(hidden_size // 2),
                nn.Dropout(0.1),
                nn.Linear(hidden_size // 2, hidden_size // 2),
                nn.ReLU()
            )
        elif extractor_type == "holding":
            # ë³´ìœ  ì‹œê°„ ê²°ì •ì„ ìœ„í•œ íŠ¹ì„± ì¶”ì¶œ (íŠ¸ë Œë“œ, ì§€ì†ì„± ì¤‘ì‹¬)
            return nn.Sequential(
                nn.Linear(hidden_size, hidden_size // 2),
                nn.ReLU(),
                nn.LayerNorm(hidden_size // 2),
                nn.Dropout(0.1),
                nn.Linear(hidden_size // 2, hidden_size // 2),
                nn.ReLU()
            )
        else:  # profit
            # ìˆ˜ìµë¥  ì˜ˆì¸¡ì„ ìœ„í•œ íŠ¹ì„± ì¶”ì¶œ (ì¢…í•©ì  ë¶„ì„)
            return nn.Sequential(
                nn.Linear(hidden_size, hidden_size // 2),
                nn.ReLU(),
                nn.LayerNorm(hidden_size // 2),
                nn.Dropout(0.1),
                nn.Linear(hidden_size // 2, hidden_size // 2),
                nn.ReLU()
            )
    
    def _init_weights(self, module):
        """Xavier ì´ˆê¸°í™”"""
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        # ë°°ì¹˜ ì°¨ì› í™•ì¸
        if x.dim() == 1:
            x = x.unsqueeze(0)
            single_sample = True
        else:
            single_sample = False
        
        # ì…ë ¥ ì„ë² ë”© (LayerNorm ì‚¬ìš©ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ë¬¸ì œ í•´ê²°)
        x = self.input_embedding(x)
        
        # ì‹œí€€ìŠ¤ ì°¨ì› ì¶”ê°€ (ì–´í…ì…˜ì„ ìœ„í•´)
        if x.dim() == 2:
            x = x.unsqueeze(1)  # [batch_size, 1, hidden_size]
        
        # ì–´í…ì…˜ ë¸”ë¡ë“¤ì„ í†µí•œ íŠ¹ì„± ì¶”ì¶œ (ì”ì°¨ ì—°ê²° í¬í•¨)
        for i, attention_block in enumerate(self.attention_blocks):
            residual = x
            x = attention_block(x)
            
            # ì”ì°¨ ì—°ê²°
            if i > 0:
                x = x + self.residual_projections[i](residual)
        
        # ì‹œí€€ìŠ¤ ì°¨ì› ì œê±°
        x = x.squeeze(1)  # [batch_size, hidden_size]
        
        # íŠ¹í™”ëœ íŠ¹ì„± ì¶”ì¶œ
        position_features = self.position_extractor(x)
        leverage_features = self.leverage_extractor(x)
        holding_features = self.holding_extractor(x)
        profit_features = self.profit_extractor(x)
        
        # ê° ì•¡ì…˜ ì°¨ì›ë³„ Qê°’
        position_q = self.position_head(position_features)
        leverage_q = self.leverage_head(leverage_features)
        holding_q = self.holding_head(holding_features)
        profit_pred = self.profit_predictor(profit_features)
        
        # ë‹¨ì¼ ìƒ˜í”Œì´ë©´ ë°°ì¹˜ ì°¨ì› ì œê±°
        if single_sample:
            position_q = position_q.squeeze(0)
            leverage_q = leverage_q.squeeze(0)
            holding_q = holding_q.squeeze(0)
            profit_pred = profit_pred.squeeze(0)
        
        return position_q, leverage_q, holding_q, profit_pred


def analyze_advanced_model(state_size: int = 61, hidden_size: int = 256):
    """AdvancedProfitDQN ëª¨ë¸ ë¶„ì„ í•¨ìˆ˜"""
    print("ğŸš€ AdvancedProfitDQN ëª¨ë¸ ë¶„ì„")
    print("=" * 50)
    
    # ê³ ê¸‰ ëª¨ë¸
    advanced_model = AdvancedProfitDQN(state_size, 3, hidden_size)
    advanced_params = sum(p.numel() for p in advanced_model.parameters())
    
    print(f"ğŸš€ AdvancedProfitDQN:")
    print(f"   - íŒŒë¼ë¯¸í„° ìˆ˜: {advanced_params:,}")
    print(f"   - íŠ¹ì§•: ì–´í…ì…˜, ì”ì°¨ ì—°ê²°, ë°°ì¹˜ ì •ê·œí™”, íŠ¹í™”ëœ ì¶”ì¶œê¸°")
    print(f"   - ì¥ì : ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ, íŠ¹í™”ëœ íŠ¹ì„± ì¶”ì¶œ")
    print(f"   - ë‹¨ì : ë” ë§ì€ íŒŒë¼ë¯¸í„°, í•™ìŠµ ì‹œê°„ ì¦ê°€")
    print("=" * 50)
    
    return advanced_model

def test_model_forward_pass(model, input_size: int = 61, batch_size: int = 32):
    """ëª¨ë¸ì˜ forward pass í…ŒìŠ¤íŠ¸"""
    import time
    print(f"ğŸ§ª {model.__class__.__name__} Forward Pass í…ŒìŠ¤íŠ¸")
    
    # í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±
    test_input = torch.randn(batch_size, input_size)
    
    try:
        with torch.no_grad():
            start_time = time.time()
            position_q, leverage_q, holding_q, profit_pred = model(test_input)
            end_time = time.time()
            
        print(f"   âœ… Forward pass ì„±ê³µ")
        print(f"   - ì‹¤í–‰ ì‹œê°„: {(end_time - start_time)*1000:.2f}ms")
        print(f"   - Position Q shape: {position_q.shape}")
        print(f"   - Leverage Q shape: {leverage_q.shape}")
        print(f"   - Holding Q shape: {holding_q.shape}")
        print(f"   - Profit pred shape: {profit_pred.shape}")
        
        # ì¶œë ¥ ë²”ìœ„ í™•ì¸
        print(f"   - Position Q ë²”ìœ„: [{position_q.min():.3f}, {position_q.max():.3f}]")
        print(f"   - Leverage Q ë²”ìœ„: [{leverage_q.min():.3f}, {leverage_q.max():.3f}]")
        print(f"   - Holding Q ë²”ìœ„: [{holding_q.min():.3f}, {holding_q.max():.3f}]")
        print(f"   - Profit pred ë²”ìœ„: [{profit_pred.min():.3f}, {profit_pred.max():.3f}]")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Forward pass ì‹¤íŒ¨: {e}")
        return False

def benchmark_advanced_model(state_size: int = 61, hidden_size: int = 256, num_tests: int = 100):
    """AdvancedProfitDQN ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    import time
    print("âš¡ AdvancedProfitDQN ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    print("=" * 50)
    
    # ëª¨ë¸ ìƒì„±
    advanced_model = AdvancedProfitDQN(state_size, 3, hidden_size)
    
    # í…ŒìŠ¤íŠ¸ ì…ë ¥
    test_input = torch.randn(32, state_size)
    
    # Advanced ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
    print("ğŸš€ AdvancedProfitDQN ë²¤ì¹˜ë§ˆí¬:")
    advanced_times = []
    for _ in range(num_tests):
        start_time = time.time()
        with torch.no_grad():
            _ = advanced_model(test_input)
        advanced_times.append(time.time() - start_time)
    
    advanced_avg_time = np.mean(advanced_times) * 1000
    advanced_std_time = np.std(advanced_times) * 1000
    
    print(f"   - í‰ê·  ì‹¤í–‰ ì‹œê°„: {advanced_avg_time:.2f}ms Â± {advanced_std_time:.2f}ms")
    print(f"   - íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in advanced_model.parameters()):,}")
    
    return {
        'advanced_avg_time': advanced_avg_time,
        'advanced_std_time': advanced_std_time,
        'parameter_count': sum(p.numel() for p in advanced_model.parameters())
    }

class TradingEnvironment(gym.Env):
    """61ì°¨ì› RL Decision ê¸°ë°˜ ì•”í˜¸í™”í ê±°ë˜ ê°•í™”í•™ìŠµ í™˜ê²½ (Gymnasium í˜¸í™˜)"""
    
    def __init__(self, price_data: pd.DataFrame, signal_data: List[Dict], 
                 initial_balance: float = 10000.0, max_position: float = 1.0):
        super().__init__()
        
        self.price_data = price_data
        self.signal_data = signal_data
        self.initial_balance = initial_balance
        self.max_position = max_position
        
        self.reward_calculator = RewardCalculator()
        
        # ì•¡ì…˜/ìƒíƒœ ìŠ¤í˜ì´ìŠ¤ ì •ì˜ (ë‹¨íƒ€ ìµœì í™”)
        self.action_space = spaces.Box(
            low=np.array([-10.0, 1.0, 0.0]),  # í¬ì§€ì…˜ í¬ê¸° ëŒ€í­ í™•ëŒ€ (-5.0 â†’ -10.0)
            high=np.array([10.0, 10.0, 60.0]),  # í¬ì§€ì…˜ í¬ê¸° ëŒ€í­ í™•ëŒ€ (5.0 â†’ 10.0)
            dtype=np.float32
        )
        
        # ê±°ë˜ ì œí•œ ì„¤ì • (ë‹¨íƒ€ ìµœì í™”)
        self.min_trade_interval = 5  # ìµœì†Œ 5ìŠ¤í… ê°„ê²© (ê³¼ë„í•œ ê±°ë˜ ë°©ì§€)
        self.last_trade_step = -self.min_trade_interval  # ì´ˆê¸°ê°’
        self.trading_cost = 0.0001  # 0.01% ê±°ë˜ ë¹„ìš© (ìˆ˜ìµì„± ëŒ€í­ ê°œì„ )
        
        # 61ì°¨ì› ìƒíƒœ ê³µê°„
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, 
            shape=(61,),  # 20(ê°€ê²©) + 6(ê¸°ìˆ ì ìˆ˜) + 26(ê²°ì •) + 9(í¬íŠ¸í´ë¦¬ì˜¤) = 61ì°¨ì›
            dtype=np.float32
        )
        
        self.reset()
    
    def reset(self, seed=None, options=None):
        """í™˜ê²½ ì´ˆê¸°í™” (Gymnasium í˜¸í™˜)"""
        if seed is not None:
            np.random.seed(seed)
        
        self.current_step = 20
        self.balance = self.initial_balance
        self.current_position = 0.0
        self.current_leverage = 1.0
        self.entry_price = 0.0
        self.unrealized_pnl = 0.0
        self.total_trades = 0
        self.winning_trades = 0
        self.max_drawdown = 0.0
        self.peak_balance = self.initial_balance
        self.consecutive_losses = 0
        self.holding_time = 0
        self.in_position = False
        self.last_trade_pnl = None
        self.last_trade_step = -self.min_trade_interval  # ê±°ë˜ ê°„ê²© ì´ˆê¸°í™”
        
        observation = self._get_observation()
        info = self._create_info_dict()
        
        return observation, info
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """í™˜ê²½ ìŠ¤í… ì‹¤í–‰ (Gymnasium í˜¸í™˜)"""
        if self.current_step >= min(len(self.price_data), len(self.signal_data)) - 1:
            return self._get_observation(), 0.0, True, False, {}
        
        position_change = np.clip(action[0], -2.0, 2.0)
        leverage = np.clip(action[1], 1.0, 10.0)  # ë ˆë²„ë¦¬ì§€ ìµœëŒ€ 10ìœ¼ë¡œ ì œí•œ
        target_holding_minutes = np.clip(action[2], 1.0, 60.0)  # ë‹¨íƒ€ ìµœëŒ€ 1ì‹œê°„
        
        # ê±°ë˜ ê°„ê²© ì œí•œ (ë‹¨íƒ€ í—ˆìš©)
        steps_since_last_trade = self.current_step - self.last_trade_step
        if steps_since_last_trade < self.min_trade_interval and abs(position_change) > 0.05:
            position_change = 0.0  # ìµœì†Œ ê°„ê²©ë§Œ ìœ ì§€
        
        # ë‹¨íƒ€ë¥¼ ìœ„í•œ ì—°ì† ê±°ë˜ í—ˆìš© (ì¡°ê±´ë¶€)
        # if abs(position_change) > 0.05 and self.in_position:
        #     position_change = 0.0  # í¬ì§€ì…˜ì´ ìˆì„ ë•ŒëŠ” ê±°ë˜ ì°¨ë‹¨
        
        current_price = self.price_data.iloc[self.current_step]['close']
        next_price = self.price_data.iloc[self.current_step + 1]['close']
        
        # í¬ì§€ì…˜ ë° ê±°ë˜ ì²˜ë¦¬
        trade_completed, old_position = self._process_position_change(
            position_change, leverage, current_price, target_holding_minutes
        )
        
        # ê±°ë˜ ì™„ë£Œ ì‹œ ê±°ë˜ ìŠ¤í… ì—…ë°ì´íŠ¸
        if trade_completed:
            self.last_trade_step = self.current_step
        
        # Signal ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        current_signal = self.signal_data[self.current_step] if self.current_step < len(self.signal_data) else {}
        
        # ë³´ìƒ ê³„ì‚° (Signal ì •ë³´ í™œìš©)
        reward = self.reward_calculator.calculate_reward(
            current_price=next_price,
            entry_price=self.entry_price,
            position=self.current_position,
            action='TRADE' if abs(position_change) > 0.1 else 'HOLD',
            holding_time=self.holding_time,
            signal_data=current_signal,
            trade_pnl=self.last_trade_pnl if trade_completed else None
        )
        
        # ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ ì´ë™
        self.current_step += 1
        self.holding_time += 3
        
        # í™€ë”© ì‹œê°„ ì´ˆê³¼ì‹œ ê°•ì œ ì²­ì‚°
        if self.in_position and self.holding_time >= target_holding_minutes:
            self._close_position(next_price)
        
        done = (self.current_step >= min(len(self.price_data), len(self.signal_data)) - 1 or 
                self.balance <= self.initial_balance * 0.1)
        
        truncated = False  # Gymnasium í˜¸í™˜ì„ ìœ„í•œ truncated í”Œë˜ê·¸
        info = self._create_info_dict()
        
        return self._get_observation(), reward, done, truncated, info
    
    def _get_observation(self) -> np.ndarray:
        """61ì°¨ì› ìƒíƒœ ê´€ì°°ê°’ ë°˜í™˜"""
        if self.current_step >= min(len(self.price_data), len(self.signal_data)):
            return np.zeros(61, dtype=np.float32)
        
        # Signalê³¼ í˜„ì¬ ìº”ë“¤ ë°ì´í„°
        current_signal = self.signal_data[self.current_step]
        current_candle = {
            'open': self.price_data.iloc[self.current_step]['open'],
            'high': self.price_data.iloc[self.current_step]['high'],
            'low': self.price_data.iloc[self.current_step]['low'],
            'close': self.price_data.iloc[self.current_step]['close'],
            'quote_volume': self.price_data.iloc[self.current_step]['quote_volume'],
        }
        
        # 1. Price Indicators (20ì°¨ì›)
        price_features = self._extract_price_indicators(current_signal, current_candle)
        # 2. Technical Scores (6ì°¨ì›)  
        technical_features = self._extract_technical_scores(current_signal)
        # 3. Decision Features (26ì°¨ì›)
        decision_features = self._extract_decision_features(current_signal)
        # 4. Portfolio Features (9ì°¨ì›)
        portfolio_features = self._get_portfolio_state()
        return np.concatenate([price_features, technical_features, decision_features, portfolio_features]).astype(np.float32)
    
    def _extract_price_indicators(self, signal_data: Dict, current_candle: Dict) -> np.ndarray:
        """Signalì˜ indicatorë“¤ì„ price featureë¡œ í™œìš© (20ì°¨ì›)"""
        current_price = current_candle['close']
        
        # 1. ê°€ê²© ëŒ€ë¹„ ì§€í‘œ ìœ„ì¹˜
        vwap = signal_data.get('indicator_vwap', current_price)
        poc = signal_data.get('indicator_poc', current_price)  
        hvn = signal_data.get('indicator_hvn', current_price)
        lvn = signal_data.get('indicator_lvn', current_price)
        
        # 2. ë³€ë™ì„± ì§€í‘œë“¤
        atr = signal_data.get('indicator_atr', 0.0)
        vwap_std = signal_data.get('indicator_vwap_std', 0.0)
        
        # 3. ì¼ë³„ ê¸°ì¤€ì ë“¤
        prev_high = signal_data.get('indicator_prev_day_high', current_price)
        prev_low = signal_data.get('indicator_prev_day_low', current_price)
        or_high = signal_data.get('indicator_opening_range_high', current_price)
        or_low = signal_data.get('indicator_opening_range_low', current_price)
        
        prev_range = prev_high - prev_low
        prev_day_position = (current_price - prev_low) / prev_range if prev_range > 0 else 0.5
            
        or_range = or_high - or_low  
        or_position = (current_price - or_low) / or_range if or_range > 0 else 0.5
        
        # 4. í˜„ì¬ ìº”ë“¤ ì •ë³´
        high, low, close, open_price = current_candle['high'], current_candle['low'], current_candle['close'], current_candle['open']
        quote_volume = current_candle.get('quote_volume', 0)
        
        return np.array([
            # ê°€ê²© ëŒ€ë¹„ ì§€í‘œ ìœ„ì¹˜ (4ê°œ)
            (current_price - vwap) / current_price if current_price > 0 else 0.0,
            (current_price - poc) / current_price if current_price > 0 else 0.0,   
            (current_price - hvn) / current_price if current_price > 0 else 0.0,   
            (current_price - lvn) / current_price if current_price > 0 else 0.0,
            
            # ë³€ë™ì„± ì§€í‘œë“¤ (2ê°œ)
            atr / current_price if current_price > 0 else 0.0,
            vwap_std / current_price if current_price > 0 else 0.0,
            
            # ì¼ë³„ ê¸°ì¤€ì ë“¤ (4ê°œ)
            prev_day_position,
            or_position,
            (current_price - prev_high) / current_price if current_price > 0 else 0.0,
            (prev_low - current_price) / current_price if current_price > 0 else 0.0,
            
            # í˜„ì¬ ìº”ë“¤ ì •ë³´ (8ê°œ)
            (close - open_price) / open_price if open_price > 0 else 0.0,
            (high - low) / close if close > 0 else 0.0,
            (high - close) / (high - low) if high > low else 0.5,
            (close - low) / (high - low) if high > low else 0.5,
            (close - open_price) / (high - low) if high > low else 0.0,
            min(quote_volume / 1000000, 2.0) if quote_volume > 0 else 0.0,
            1.0 if close > open_price else 0.0,
            (high - max(open_price, close)) / (high - low) if high > low else 0.0,
            
            # ì¶”ê°€ ìº”ë“¤ ì •ë³´ (2ê°œ)
            (low - min(open_price, close)) / (high - low) if high > low else 0.0,
            abs(close - open_price) / (high - low) if high > low else 0.0
        ], dtype=np.float32)
    
    def _extract_technical_scores(self, signals: Dict) -> np.ndarray:
        """ê° ì „ëµì˜ raw scoreë“¤ (25ì°¨ì›) - ìƒˆë¡œìš´ RL ìŠ¤í‚¤ë§ˆ ê¸°ë°˜"""
        # ìƒˆë¡œìš´ RL ìŠ¤í‚¤ë§ˆì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì ìˆ˜ë“¤ ìˆ˜ì§‘
        score_fields = []
        
        # ê° ì‹œê°„ëŒ€ë³„ ì ìˆ˜ë“¤
        # for timeframe in ['short_term', 'medium_term', 'long_term']:
        #     score_fields.extend([
        #         f'{timeframe}_net_score',
        #         f'{timeframe}_buy_score', 
        #         f'{timeframe}_sell_score',
        #         f'{timeframe}_confidence',
        #         f'{timeframe}_market_context'
        #     ])
        
        # Conflict ê´€ë ¨ ì ìˆ˜ë“¤ (ì¤‘ë³µ ì œê±° - Decision Featuresì—ì„œ ì²˜ë¦¬)
        # score_fields.extend([
        #     'conflict_conflict_severity',
        #     'conflict_directional_consensus',
        #     'conflict_conflict_penalty',
        #     'conflict_consensus_bonus',
        #     'conflict_diversity_bonus'
        # ])
        
        # Indicator ê´€ë ¨ ì ìˆ˜ë“¤
        indicator_fields = [
            'indicator_vwap', 'indicator_atr', 'indicator_poc', 
            'indicator_hvn', 'indicator_lvn', 'indicator_vwap_std'
        ]
        
        # ìˆ˜ì§‘ëœ ì ìˆ˜ë“¤ ì •ê·œí™”
        all_scores = []
        for field in score_fields + indicator_fields:
            value = signals.get(field)
            try:
                score = float(value)
                # ì •ê·œí™” (ëŒ€ë¶€ë¶„ 0~1 ë²”ìœ„ë¡œ ê°€ì •)
                if 'indicator_' in field:
                    # IndicatorëŠ” ê°€ê²© ëŒ€ë¹„ ë¹„ìœ¨ë¡œ ì •ê·œí™”
                    score = min(abs(score) / 1000.0, 1.0)  # ê°€ê²© ëŒ€ë¹„ 0.1% ë‹¨ìœ„
                all_scores.append(score)
            except:
                all_scores.append(0.0)
        
        # 6ì°¨ì›ìœ¼ë¡œ ë§ì¶”ê¸° (Indicatorë§Œ ì‚¬ìš©)
        if len(all_scores) >= 6:
            return np.array(all_scores[:6], dtype=np.float32)
        else:
            return np.array(all_scores + [0.0] * (6 - len(all_scores)), dtype=np.float32)
    
    def _get_portfolio_state(self) -> np.ndarray:
        """í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ì •ë³´ (9ì°¨ì›)"""
        return np.array([
            self.current_position,
            self.current_leverage / 20.0,
            (self.balance - self.initial_balance) / self.initial_balance,
            self.unrealized_pnl / self.initial_balance if self.initial_balance > 0 else 0.0,
            min(self.total_trades / 100.0, 1.0),
            self.winning_trades / max(self.total_trades, 1),
            self.max_drawdown,
            min(self.consecutive_losses / 10.0, 1.0),
            min(self.holding_time / 1440.0, 1.0)
        ], dtype=np.float32)
    
    def _extract_decision_features(self, signals: Dict) -> np.ndarray:
        """Decision íŠ¹ì„±ë“¤ (26ì°¨ì›) - ìƒˆë¡œìš´ RL ìŠ¤í‚¤ë§ˆ ê¸°ë°˜"""
        # ê° ì‹œê°„ëŒ€ë³„ íŠ¹ì„± (3 Ã— 6 = 18ê°œ)
        timeframe_features = []
        for timeframe in ['short_term', 'medium_term', 'long_term']:
            # ìƒˆë¡œìš´ RL ìŠ¤í‚¤ë§ˆ í•„ë“œë“¤ ì‚¬ìš©
            action_value = float(signals.get(f'{timeframe}_action', 0.0))
            net_score = float(signals.get(f'{timeframe}_net_score', 0.0))
            buy_score = float(signals.get(f'{timeframe}_buy_score', 0.0))
            sell_score = float(signals.get(f'{timeframe}_sell_score', 0.0))
            confidence_value = float(signals.get(f'{timeframe}_confidence', 0.0))
            market_context_value = float(signals.get(f'{timeframe}_market_context', 0.0))
            
            timeframe_features.extend([action_value, net_score, buy_score, sell_score, confidence_value, market_context_value])
        
        # ì¶”ê°€ ë©”íƒ€ ì •ë³´ (3ê°œ)
        signals_used = []
        for timeframe in ['short_term', 'medium_term', 'long_term']:
            used = signals.get(f'{timeframe}_signals_used', 0)
            
            signals_used.append(min(float(used) / 10.0, 1.0))
        
        # Conflict ì •ë³´ (3ê°œ)
        conflict_severity = float(signals.get('conflict_conflict_severity', 0.0))
        conflict_consensus = float(signals.get('conflict_directional_consensus', 0.0))
        conflict_penalty = float(signals.get('conflict_conflict_penalty', 0.0))
        
        # Long term íŠ¹í™” ì •ë³´ (2ê°œ)
        institutional_bias = float(signals.get('long_term_institutional_bias', 0.0))
        macro_trend_strength = float(signals.get('long_term_macro_trend_strength', 0.0))
        
        return np.array(
            timeframe_features + 
            signals_used + 
            [conflict_severity, conflict_consensus, conflict_penalty, institutional_bias, macro_trend_strength],
            dtype=np.float32
        )
    
    
    def _process_position_change(self, position_change: float, leverage: float, 
                               current_price: float, target_holding_minutes: float) -> Tuple[bool, float]:
        """í¬ì§€ì…˜ ë³€ê²½ ì²˜ë¦¬"""
        old_position = self.current_position
        new_position = np.clip(self.current_position + position_change, -1.0, 1.0)
        trade_completed = False
        
        if abs(new_position - self.current_position) > 0.01:
            # ê¸°ì¡´ í¬ì§€ì…˜ ì²­ì‚°
            if abs(self.current_position) > 0.01:
                trade_completed = True
                self.last_trade_pnl = self._calculate_trade_pnl(current_price, self.entry_price, old_position)
                self._close_position(current_price)
            
            # ìƒˆ í¬ì§€ì…˜ ì§„ì…
            if abs(new_position) > 0.01:
                self.current_position = new_position
                self.current_leverage = leverage
                self.entry_price = current_price
                self.holding_time = 0
                self.in_position = True
        
        return trade_completed, old_position
    
    def _calculate_trade_pnl(self, exit_price: float, entry_price: float, position: float) -> float:
        """ê±°ë˜ ì†ìµ ê³„ì‚°"""
        if entry_price <= 0:
            return 0.0
        
        price_change = (exit_price - entry_price) / entry_price
        return position * price_change
    
    def _close_position(self, exit_price: float):
        """í¬ì§€ì…˜ ì²­ì‚°"""
        if abs(self.current_position) < 0.01:
            return
        
        pnl = self._calculate_trade_pnl(exit_price, self.entry_price, self.current_position)
        pnl_usd = pnl * self.current_leverage * self.balance
        
        # ê±°ë˜ ìˆ˜ìˆ˜ë£Œ ì°¨ê° (ê°œì„ ëœ ë¹„ìš© êµ¬ì¡°)
        trade_volume = abs(self.current_position) * self.current_leverage * self.balance
        fee = trade_volume * self.trading_cost  # 0.1% ê±°ë˜ ë¹„ìš©
        pnl_usd -= fee
        
        # ì”ê³  ë° í†µê³„ ì—…ë°ì´íŠ¸
        self.balance += pnl_usd
        self._update_trading_stats(pnl_usd)
        
        # í¬ì§€ì…˜ ì´ˆê¸°í™”
        self.current_position = 0.0
        self.unrealized_pnl = 0.0
        self.in_position = False
        self.holding_time = 0
        self.last_trade_pnl = pnl
    
    def _update_trading_stats(self, pnl_usd: float):
        """ê±°ë˜ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.total_trades += 1
        
        if pnl_usd > 0:
            self.winning_trades += 1
            self.consecutive_losses = 0
        else:
            self.consecutive_losses += 1
        
        # ìµœëŒ€ ë‚™í­ ì—…ë°ì´íŠ¸
        if self.balance > self.peak_balance:
            self.peak_balance = self.balance
        else:
            drawdown = (self.peak_balance - self.balance) / self.peak_balance
            self.max_drawdown = max(self.max_drawdown, drawdown)
    
    def _create_info_dict(self) -> Dict:
        """ì •ë³´ ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        current_price = self.price_data.iloc[min(self.current_step, len(self.price_data)-1)]['close']
        
        return {
            'balance': self.balance,
            'position': self.current_position,
            'unrealized_pnl': self.unrealized_pnl,
            'total_trades': self.total_trades,
            'win_rate': self.winning_trades / max(self.total_trades, 1),
            'current_price': current_price,
            'entry_price': self.entry_price,
            'holding_time': self.holding_time,
            'max_drawdown': self.max_drawdown,
            'trade_completed': hasattr(self, 'last_trade_pnl') and self.last_trade_pnl is not None,
            'trade_pnl': self.last_trade_pnl if hasattr(self, 'last_trade_pnl') else None
        }

"""
61ì°¨ì› RL Decision ê¸°ë°˜ ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© AI í›ˆë ¨ ì‹œìŠ¤í…œ - Part 2
- RLAgent í´ë˜ìŠ¤ ë° í›ˆë ¨/í‰ê°€ ì‹œìŠ¤í…œ
- ìƒˆë¡œìš´ Decision ìŠ¤í‚¤ë§ˆ ë°ì´í„° ë¡œë” ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
"""

class RLAgent:
    """61ì°¨ì› RL Decision ê¸°ë°˜ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸"""
    
    def __init__(self, state_size: int = 61, learning_rate: float = 5e-4, 
                    gamma: float = 0.99, epsilon: float = 0.2, epsilon_decay: float = 0.995,
                    hidden_size: int = 256):
        
        self.state_size = state_size
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.hidden_size = hidden_size
        self.epsilon_min = 0.05  # 5%ë¡œ ê°ì†Œ (ì ì ˆí•œ íƒí—˜)
        
        # Îµ ê°’ì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì´ˆê¸°í™”
        if self.epsilon < self.epsilon_min:
            self.epsilon = 0.15  # 15%ë¡œ ì´ˆê¸°í™” (ì ì ˆí•œ íƒí—˜)
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° ë””ë°”ì´ìŠ¤ ì„¤ì •
        if torch.cuda.is_available():
            self.device = torch.device("cuda")
            print(f"ğŸš€ GPU ì‚¬ìš©: {torch.cuda.get_device_name(0)}")
            print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        else:
            self.device = torch.device("cpu")
            print("âš ï¸  GPU ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
            print("   GPU ì‚¬ìš©ì„ ì›í•˜ë©´ PyTorch CUDA ë²„ì „ì„ ì„¤ì¹˜í•˜ì„¸ìš”:")
            print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
        print(f"Using device: {self.device} for {state_size}ì°¨ì› ëª¨ë¸")
        
        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” (AdvancedProfitDQNë§Œ ì‚¬ìš©)
        print("ğŸš€ AdvancedProfitDQN ì•„í‚¤í…ì²˜ ì‚¬ìš© (ì–´í…ì…˜, ì”ì°¨ ì—°ê²°, ë°°ì¹˜ ì •ê·œí™”)")
        self.q_network = AdvancedProfitDQN(state_size, 3, hidden_size).to(self.device)
        self.target_network = AdvancedProfitDQN(state_size, 3, hidden_size).to(self.device)
        
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # ê²½í—˜ ë¦¬í”Œë ˆì´
        self.memory = deque(maxlen=200000)  # ë©”ëª¨ë¦¬ í¬ê¸° ì¦ê°€
        self.batch_size = 512  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
        
        # í•™ìŠµ ì¶”ì 
        self.training_rewards = []
        self.losses = []
        self.win_rates = []
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
        self.target_update_freq = 1000
        self.update_count = 0
    
    def remember(self, state, action, reward, next_state, done):
        """ê²½í—˜ ì €ì¥"""
        self.memory.append(Experience(state, action, reward, next_state, done))
    
    def act(self, state: np.ndarray) -> np.ndarray:
        """ì•¡ì…˜ ì„ íƒ"""
        if np.random.random() <= self.epsilon:
            return self._get_random_action()
        
        return self._get_greedy_action(state)
    
    def _get_random_action(self) -> np.ndarray:
        """ìŠ¤ë§ˆíŠ¸í•œ ëœë¤ ì•¡ì…˜"""
        return np.array([
            np.random.uniform(-1.0, 1.0),
            np.random.uniform(1.0, 5.0),
            np.random.uniform(30.0, 180.0)
        ])
    
    def _get_greedy_action(self, state: np.ndarray) -> np.ndarray:
        """Qê°’ ê¸°ë°˜ íƒìš•ì  ì•¡ì…˜ ì„ íƒ (ìˆ˜ìµë¥  ìµœì í™”)"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            position_q, leverage_q, holding_q, profit_pred = self.q_network(state_tensor)
            
            # ìˆ˜ìµë¥  ì˜ˆì¸¡ì„ ê³ ë ¤í•œ ì•¡ì…˜ ì„ íƒ
            position_idx = torch.argmax(position_q).item()
            leverage_idx = torch.argmax(leverage_q).item()
            holding_idx = torch.argmax(holding_q).item()
            
            # ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€í™˜ (í¬ì§€ì…˜ í¬ê¸° ëŒ€í­ í™•ëŒ€)
            position = -10.0 + (position_idx * 0.2)  # -10.0~10.0 (101ê°œ êµ¬ê°„)
            leverage = 1.0 + leverage_idx
            holding = 30.0 + (holding_idx * 30.0)
            
            return np.array([position, leverage, holding])
    
    def replay(self):
        """ê²½í—˜ ë¦¬í”Œë ˆì´ í•™ìŠµ"""
        if len(self.memory) < self.batch_size * 2:
            return
        
        batch = random.sample(self.memory, self.batch_size)
        loss = self._compute_loss(batch)
        
        # ì—­ì „íŒŒ
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()
        
        self.losses.append(loss.item())
        
        # ì ì‘ì  ì—¡ì‹¤ë¡  ê°ì†Œ (ì„±ëŠ¥ì— ë”°ë¼ ì¡°ì •)
        if self.epsilon > self.epsilon_min:
            # ìµœê·¼ ì„±ëŠ¥ ê¸°ë°˜ ì ì‘ì  ê°ì†Œ
            if len(self.training_rewards) > 50:
                recent_rewards = self.training_rewards[-50:]
                avg_recent_reward = np.mean(recent_rewards)
                
                # ì„±ëŠ¥ì´ ì¢‹ìœ¼ë©´ ë” ë¹ ë¥´ê²Œ ê°ì†Œ
                if avg_recent_reward > 100:  # ë¦¬ì›Œë“œê°€ 100 ì´ìƒì´ë©´
                    self.epsilon *= 0.95  # ë” ë¹ ë¥¸ ê°ì†Œ
                elif avg_recent_reward > 0:
                    self.epsilon *= 0.98  # ì¤‘ê°„ ê°ì†Œ
                else:
                    self.epsilon *= 0.99  # ëŠë¦° ê°ì†Œ
            else:
                self.epsilon *= 0.99  # ì´ˆê¸°ì—ëŠ” ëŠë¦° ê°ì†Œ
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
        self.update_count += 1
        if self.update_count % self.target_update_freq == 0:
            self.update_target_network()
    
    def _compute_loss(self, batch) -> torch.Tensor:
        """ìˆ˜ìµë¥  ìµœì í™” ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°"""
        # íš¨ìœ¨ì ì¸ í…ì„œ ë³€í™˜ (numpy ë°°ì—´ì„ ë¨¼ì € ê²°í•©)
        states = torch.FloatTensor(np.array([e.state for e in batch])).to(self.device)
        actions = [e.action for e in batch]
        rewards = torch.FloatTensor(np.array([e.reward for e in batch])).to(self.device)
        next_states = torch.FloatTensor(np.array([e.next_state for e in batch])).to(self.device)
        dones = [bool(e.done) for e in batch]
        
        # í˜„ì¬ Qê°’ë“¤ê³¼ ìˆ˜ìµë¥  ì˜ˆì¸¡
        current_position_q, current_leverage_q, current_holding_q, current_profit_pred = self.q_network(states)
        
        # íƒ€ê²Ÿ Qê°’ë“¤
        with torch.no_grad():
            next_position_q, next_leverage_q, next_holding_q, next_profit_pred = self.target_network(next_states)
            
            target_position_q = current_position_q.clone()
            target_leverage_q = current_leverage_q.clone()
            target_holding_q = current_holding_q.clone()
            
            for i, (action, reward, done) in enumerate(zip(actions, rewards, dones)):
                pos_idx = int(np.clip((action[0] + 2.0) / 0.2, 0, 20))
                lev_idx = int(np.clip(action[1] - 1, 0, 19))
                hold_idx = int(np.clip((action[2] - 30.0) / 30.0, 0, 47))
                
                if not done:
                    # ìˆ˜ìµë¥  ê¸°ë°˜ íƒ€ê²Ÿ (ë” ê°•í•œ ë³´ìƒ ê°€ì¤‘ì¹˜)
                    target_q = reward + self.gamma * torch.max(next_position_q[i])
                    target_position_q[i, pos_idx] = target_q
                    target_leverage_q[i, lev_idx] = target_q
                    target_holding_q[i, hold_idx] = target_q
                else:
                    # ìµœì¢… ë³´ìƒ (ìˆ˜ìµë¥  ì¤‘ì‹¬)
                    target_position_q[i, pos_idx] = reward
                    target_leverage_q[i, lev_idx] = reward
                    target_holding_q[i, hold_idx] = reward
        
        # Q-learning ì†ì‹¤ (ìˆ˜ìµë¥  ê°€ì¤‘ì¹˜ ì ìš©)
        pos_loss = F.smooth_l1_loss(current_position_q, target_position_q)
        lev_loss = F.smooth_l1_loss(current_leverage_q, target_leverage_q)
        hold_loss = F.smooth_l1_loss(current_holding_q, target_holding_q)
        
        # ìˆ˜ìµë¥  ì˜ˆì¸¡ ì†ì‹¤ (ë³´ì¡° í•™ìŠµ)
        profit_targets = rewards.unsqueeze(1)  # ì‹¤ì œ ìˆ˜ìµë¥ ì„ íƒ€ê²Ÿìœ¼ë¡œ
        profit_loss = F.mse_loss(current_profit_pred, profit_targets)
        
        # ìˆ˜ìµë¥  ì¤‘ì‹¬ ê°€ì¤‘ì¹˜ (ìˆ˜ìµë¥  ì˜ˆì¸¡ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        total_loss = (pos_loss + lev_loss + hold_loss) + 0.5 * profit_loss
        
        return total_loss
    
    def update_target_network(self):
        """íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def save_model(self, filepath: str) -> bool:
        """ëª¨ë¸ ì €ì¥"""
        try:
            os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)
            
            save_dict = {
                'q_network': self.q_network.state_dict(),
                'target_network': self.target_network.state_dict(),
                'optimizer': self.optimizer.state_dict(),
                'epsilon': float(self.epsilon),
                'training_rewards': [float(r) for r in self.training_rewards],
                'losses': [float(l) for l in self.losses],
                'win_rates': [float(w) for w in self.win_rates],
                'update_count': int(self.update_count),
                'state_size': int(self.state_size)
            }
            
            torch.save(save_dict, filepath)
            print(f"61ì°¨ì› ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")
            return True
            
        except Exception as e:
            print(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def load_model(self, filepath: str) -> bool:
        """ëª¨ë¸ ë¡œë“œ"""
        if not os.path.exists(filepath):
            print(f"ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {filepath}")
            return False
        
        try:
            try:
                checkpoint = torch.load(filepath, map_location=self.device, weights_only=True)
            except:
                checkpoint = torch.load(filepath, map_location=self.device, weights_only=False)
            
            # ìƒíƒœ í¬ê¸° í™•ì¸
            model_state_size = checkpoint.get('state_size', 60)
            if model_state_size != self.state_size:
                print(f"âŒ ëª¨ë¸ ì°¨ì› ë¶ˆì¼ì¹˜: ê¸°ëŒ€ {self.state_size}, ì‹¤ì œ {model_state_size}")
                return False
            
            self.q_network.load_state_dict(checkpoint['q_network'])
            self.target_network.load_state_dict(checkpoint['target_network'])
            self.optimizer.load_state_dict(checkpoint['optimizer'])
            self.epsilon = checkpoint.get('epsilon', self.epsilon)
            self.training_rewards = checkpoint.get('training_rewards', [])
            self.losses = checkpoint.get('losses', [])
            self.win_rates = checkpoint.get('win_rates', [])
            self.update_count = checkpoint.get('update_count', 0)
            
            print(f"âœ… 61ì°¨ì› ëª¨ë¸ ë¡œë“œ ì„±ê³µ! ì—¡ì‹¤ë¡ : {self.epsilon:.3f}")
            return True
            
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def load_model_with_compatibility(self, filepath: str) -> bool:
        """í˜¸í™˜ì„±ì„ ê³ ë ¤í•œ ëª¨ë¸ ë¡œë“œ (êµ¬ì¡° ì°¨ì´ ë¬´ì‹œ)"""
        if not os.path.exists(filepath):
            print(f"ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {filepath}")
            return False
        
        try:
            checkpoint = torch.load(filepath, map_location=self.device, weights_only=False)
            
            # ìƒíƒœ í¬ê¸° í™•ì¸
            model_state_size = checkpoint.get('state_size', 60)
            print(f"ê¸°ì¡´ ëª¨ë¸ ì°¨ì›: {model_state_size}ì°¨ì›")
            
            # ê¸°ì¡´ ëª¨ë¸ê³¼ í˜„ì¬ ëª¨ë¸ì˜ êµ¬ì¡°ê°€ ì™„ì „íˆ ë‹¤ë¥´ë¯€ë¡œ
            # í˜¸í™˜ ê°€ëŠ¥í•œ ë¶€ë¶„ë§Œ ë¡œë“œí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ìƒˆë¡œ ì´ˆê¸°í™”
            print("âš ï¸ ëª¨ë¸ êµ¬ì¡°ê°€ ë‹¤ë¦…ë‹ˆë‹¤. í˜¸í™˜ ê°€ëŠ¥í•œ ë¶€ë¶„ë§Œ ë¡œë“œí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ìƒˆë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            
            # í˜„ì¬ ëª¨ë¸ì˜ state_dict
            current_state_dict = self.q_network.state_dict()
            loaded_state_dict = checkpoint['q_network']
            
            # í˜¸í™˜ ê°€ëŠ¥í•œ ë ˆì´ì–´ë§Œ ë¡œë“œ
            compatible_state_dict = {}
            loaded_count = 0
            initialized_count = 0
            
            for key in current_state_dict.keys():
                if key in loaded_state_dict:
                    # í¬ê¸°ê°€ ê°™ì€ ê²½ìš°ë§Œ ë¡œë“œ
                    if current_state_dict[key].shape == loaded_state_dict[key].shape:
                        compatible_state_dict[key] = loaded_state_dict[key]
                        print(f"   âœ… {key}: ë¡œë“œë¨")
                        loaded_count += 1
                    else:
                        compatible_state_dict[key] = current_state_dict[key]
                        print(f"   âš ï¸ {key}: í¬ê¸° ë¶ˆì¼ì¹˜ ({loaded_state_dict[key].shape} â†’ {current_state_dict[key].shape}), ìƒˆë¡œ ì´ˆê¸°í™”")
                        initialized_count += 1
                else:
                    compatible_state_dict[key] = current_state_dict[key]
                    print(f"   âŒ {key}: ëˆ„ë½, ìƒˆë¡œ ì´ˆê¸°í™”")
                    initialized_count += 1
            
            # ëˆ„ë½ëœ ë ˆì´ì–´ë“¤ í™•ì¸
            missing_in_current = set(loaded_state_dict.keys()) - set(current_state_dict.keys())
            if missing_in_current:
                print(f"   ğŸ“ í˜„ì¬ ëª¨ë¸ì— ì—†ëŠ” ë ˆì´ì–´ë“¤: {len(missing_in_current)}ê°œ")
                for key in sorted(missing_in_current):
                    print(f"      - {key}: {loaded_state_dict[key].shape}")
            
            # í˜¸í™˜ ê°€ëŠ¥í•œ state_dictë¡œ ë¡œë“œ
            try:
                self.q_network.load_state_dict(compatible_state_dict)
                self.target_network.load_state_dict(compatible_state_dict)
                print("   âœ… ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"   âŒ ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
                # ë¶€ë¶„ì  ë¡œë“œë¥¼ ì‹œë„
                try:
                    self.q_network.load_state_dict(compatible_state_dict, strict=False)
                    self.target_network.load_state_dict(compatible_state_dict, strict=False)
                    print("   âš ï¸ ë¶€ë¶„ì  ë¡œë“œë¡œ ë³µêµ¬ë¨")
                except Exception as e2:
                    print(f"   âŒ ë¶€ë¶„ì  ë¡œë“œë„ ì‹¤íŒ¨: {e2}")
                    return False
            
            # ì˜µí‹°ë§ˆì´ì € ë¡œë“œ (í˜¸í™˜ì„± í™•ì¸)
            try:
                if 'optimizer' in checkpoint:
                    self.optimizer.load_state_dict(checkpoint['optimizer'])
                    print("   âœ… ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¡œë“œë¨")
                else:
                    print("   âš ï¸ ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
            except Exception as e:
                print(f"   âš ï¸ ì˜µí‹°ë§ˆì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
            
            # ê¸°íƒ€ íŒŒë¼ë¯¸í„°ë“¤
            self.epsilon = checkpoint.get('epsilon', self.epsilon)
            self.training_rewards = checkpoint.get('training_rewards', [])
            self.losses = checkpoint.get('losses', [])
            self.win_rates = checkpoint.get('win_rates', [])
            self.update_count = checkpoint.get('update_count', 0)
            
            print(f"âœ… {model_state_size}ì°¨ì› â†’ 61ì°¨ì› í˜¸í™˜ì„± ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
            print(f"   - ë¡œë“œëœ ë ˆì´ì–´: {loaded_count}ê°œ")
            print(f"   - ìƒˆë¡œ ì´ˆê¸°í™”ëœ ë ˆì´ì–´: {initialized_count}ê°œ")
            
            # ëª¨ë¸ ê²€ì¦
            try:
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¡œ ëª¨ë¸ì´ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
                test_input = torch.randn(1, self.state_size).to(self.device)
                with torch.no_grad():
                    _ = self.q_network(test_input)
                print("   âœ… ëª¨ë¸ ê²€ì¦ ì™„ë£Œ - ì •ìƒ ì‘ë™")
            except Exception as e:
                print(f"   âš ï¸ ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
                print("   ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆì§€ë§Œ ì˜ˆìƒê³¼ ë‹¤ë¥´ê²Œ ì‘ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            return True
            
        except Exception as e:
            print(f"âŒ í˜¸í™˜ì„± ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"   ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            import traceback
            print(f"   ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False

    def create_compatible_model(self, old_model_path: str) -> bool:
        """ê¸°ì¡´ ëª¨ë¸ì„ AdvancedProfitDQN ì•„í‚¤í…ì²˜ë¡œ ë³€í™˜"""
        try:
            print(f"ğŸ”„ ê¸°ì¡´ ëª¨ë¸ì„ AdvancedProfitDQN ì•„í‚¤í…ì²˜ë¡œ ë³€í™˜ ì¤‘...")
            
            # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ
            checkpoint = torch.load(old_model_path, map_location=self.device, weights_only=False)
            old_state_dict = checkpoint['q_network']
            
            # ìƒˆë¡œìš´ AdvancedProfitDQN ëª¨ë¸ ìƒì„±
            new_model = AdvancedProfitDQN(self.state_size, 3, self.hidden_size).to(self.device)
            new_state_dict = new_model.state_dict()
            
            # í˜¸í™˜ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë§Œ ë³µì‚¬
            compatible_weights = {}
            for key in new_state_dict.keys():
                if key in old_state_dict and new_state_dict[key].shape == old_state_dict[key].shape:
                    compatible_weights[key] = old_state_dict[key]
                    print(f"   âœ… {key}: ë³€í™˜ë¨")
                else:
                    compatible_weights[key] = new_state_dict[key]
                    print(f"   âŒ {key}: ìƒˆë¡œ ì´ˆê¸°í™”")
            
            # ìƒˆë¡œìš´ ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ë¡œë“œ
            new_model.load_state_dict(compatible_weights)
            
            # í˜„ì¬ ì—ì´ì „íŠ¸ì˜ ë„¤íŠ¸ì›Œí¬ êµì²´
            self.q_network = new_model
            self.target_network = AdvancedProfitDQN(self.state_size, 3, self.hidden_size).to(self.device)
            self.target_network.load_state_dict(compatible_weights)
            
            print(f"âœ… AdvancedProfitDQN ì•„í‚¤í…ì²˜ë¡œ ë³€í™˜ ì™„ë£Œ!")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return False

    def diagnose_model_compatibility(self, model_path: str) -> Dict:
        """ëª¨ë¸ í˜¸í™˜ì„± ì§„ë‹¨"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
            old_state_dict = checkpoint['q_network']
            current_state_dict = self.q_network.state_dict()
            
            diagnosis = {
                'total_old_layers': len(old_state_dict),
                'total_new_layers': len(current_state_dict),
                'compatible_layers': 0,
                'incompatible_layers': 0,
                'missing_layers': 0,
                'compatibility_rate': 0.0
            }
            
            for key in current_state_dict.keys():
                if key in old_state_dict:
                    if current_state_dict[key].shape == old_state_dict[key].shape:
                        diagnosis['compatible_layers'] += 1
                    else:
                        diagnosis['incompatible_layers'] += 1
                else:
                    diagnosis['missing_layers'] += 1
            
            diagnosis['compatibility_rate'] = diagnosis['compatible_layers'] / len(current_state_dict)
            
            return diagnosis
            
        except Exception as e:
            return {'error': str(e)}

class DataLoader:
    """61ì°¨ì› RL Decision ê¸°ë°˜ ë°ì´í„° ë¡œë”© í´ë˜ìŠ¤"""
    
    @staticmethod
    def load_price_data(file_path: str = 'data/ETHUSDC_3m_historical_data.csv') -> Optional[pd.DataFrame]:
        """ê°€ê²© ë°ì´í„° ë¡œë“œ"""
        try:
            required_columns = ['open', 'high', 'low', 'close', 'volume', 'quote_volume']
            
            df = pd.read_csv(file_path)
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.set_index('timestamp')
            df = df[required_columns]
            
            price_data = df.reset_index()
            print(f"ê°€ê²© ë°ì´í„° ë¡œë“œ: {len(price_data):,}ê°œ ìº”ë“¤")
            return price_data
            
        except Exception as e:
            print(f"ê°€ê²© ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    @staticmethod
    def load_signal_data(agent_folder: str = "agent") -> Optional[List[Dict]]:
        """61ì°¨ì›ìš© RL Decision ë°ì´í„° ë¡œë“œ"""
        parquet_files = []
        
        if Path(agent_folder).exists():
            parquet_files = list(Path(agent_folder).glob("*.parquet"))
        
        if parquet_files:
            try:
                print(f"Signal ë°ì´í„° ë¡œë“œ ì¤‘: {parquet_files[0].name}")
                signal_df = pd.read_parquet(parquet_files[0])
                print(f"Signal ë°ì´í„° ë¡œë“œ: {len(signal_df):,}ê°œ ë ˆì½”ë“œ")
                
                return DataLoader._convert_parquet_to_signal_dicts(signal_df)
                
            except Exception as e:
                print(f"Parquet ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print("Parquet íŒŒì¼ì´ ì—†ì–´ ê¸°ë³¸ Signalì„ ìƒì„±í•©ë‹ˆë‹¤.")
        return None
    
    @staticmethod
    def _convert_parquet_to_signal_dicts(signal_df: pd.DataFrame) -> List[Dict]:
        """Parquetì„ Signal Dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (61ì°¨ì›ìš©) - ìƒˆë¡œìš´ RL ìŠ¤í‚¤ë§ˆ"""
        signal_data = []
        
        print("61ì°¨ì›ìš© RL ìŠ¤í‚¤ë§ˆ Signal ë°ì´í„° ë³€í™˜ ì¤‘...")
        
        for idx, row in signal_df.iterrows():
            # ê° í–‰ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ìƒˆë¡œìš´ RL ìŠ¤í‚¤ë§ˆ í˜•íƒœ ìœ ì§€)
            signal_dict = {}
            
            for col, value in row.items():
                if pd.notna(value):
                    # ìˆ˜ì¹˜ ë°ì´í„°ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                    signal_dict[col] = value
                else:
                    # ê¸°ë³¸ê°’ ì„¤ì • (ìƒˆë¡œìš´ RL ìŠ¤í‚¤ë§ˆì— ë§ê²Œ)
                    if 'action_value' in col or 'net_score' in col or 'buy_score' in col or 'sell_score' in col:
                        signal_dict[col] = 0.0
                    elif 'confidence_value' in col or 'market_context_value' in col:
                        signal_dict[col] = 0.0
                    elif 'conflict_' in col:
                        signal_dict[col] = 0.0
                    elif 'leverage' in col or 'signals_used' in col or 'strategies_count' in col:
                        signal_dict[col] = 0
                    elif 'max_holding_minutes' in col:
                        signal_dict[col] = 0
                    else:
                        signal_dict[col] = 0.0
            
            signal_data.append(signal_dict)
            
            if (idx + 1) % 5000 == 0:
                print(f"   ë³€í™˜ ì§„í–‰: {idx + 1:,}/{len(signal_df):,}")
        
        print(f"61ì°¨ì›ìš© RL ìŠ¤í‚¤ë§ˆ Signal ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(signal_data):,}ê°œ")
        return signal_data
    

class PerformanceAnalyzer:
    """61ì°¨ì› RL Decision ê¸°ë°˜ ì„±ëŠ¥ ë¶„ì„ í´ë˜ìŠ¤"""
    
    @staticmethod
    def evaluate_agent(agent: RLAgent, env: TradingEnvironment, num_episodes: int = 10) -> Tuple[List[Dict], Dict]:
        """61ì°¨ì› ì—ì´ì „íŠ¸ ì„±ëŠ¥ í‰ê°€"""
        print(f"61ì°¨ì› ì—ì´ì „íŠ¸ ì„±ëŠ¥ í‰ê°€ ì¤‘ ({num_episodes} ì—í”¼ì†Œë“œ)...")
        
        original_epsilon = agent.epsilon
        agent.epsilon = 0.0
        
        results = []
        all_trades = []
        
        for episode in range(num_episodes):
            # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì™„ì „ ì´ˆê¸°í™”
            state, _ = env.reset()
            episode_reward = 0
            episode_trades = []
            episode_balance = env.initial_balance
            
            # í…ŒìŠ¤íŠ¸ í™˜ê²½ ìƒíƒœ í™•ì¸
            print(f"   ì—í”¼ì†Œë“œ {episode+1}: ì´ˆê¸° ì”ê³  ${episode_balance:.0f}")
            
            for step in range(500):
                action = agent.act(state)
                next_state, reward, done, truncated, info = env.step(action)
                
                episode_reward += reward
                episode_balance = info['balance']
                
                if info.get('trade_completed', False):
                    trade_pnl = info.get('trade_pnl', 0.0)
                    episode_trades.append(1 if trade_pnl > 0 else 0)
                
                state = next_state
                if done:
                    break
            
            episode_return = (episode_balance - env.initial_balance) / env.initial_balance
            win_rate = np.mean(episode_trades) if episode_trades else 0.0
            
            # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
            print(f"   ì—í”¼ì†Œë“œ {episode+1}: ê±°ë˜ {len(episode_trades)}ê°œ, ìŠ¹ë¥  {win_rate:.1%}, ìˆ˜ìµë¥  {episode_return:.1%}, ì”ê³  ${episode_balance:.0f}")
            
            results.append({
                'episode': episode,
                'total_reward': episode_reward,
                'final_balance': episode_balance,
                'return': episode_return,
                'win_rate': win_rate,
                'total_trades': len(episode_trades),
                'max_drawdown': info.get('max_drawdown', 0.0)
            })
            
            all_trades.extend(episode_trades)
        
        agent.epsilon = original_epsilon
        
        overall_stats = {
            'avg_return': np.mean([r['return'] for r in results]),
            'avg_reward': np.mean([r['total_reward'] for r in results]),
            'overall_win_rate': np.mean(all_trades) if all_trades else 0.0,
            'avg_trades_per_episode': np.mean([r['total_trades'] for r in results]),
            'avg_max_drawdown': np.mean([r['max_drawdown'] for r in results]),
            'consistency': 1.0 - np.std([r['return'] for r in results]) if len(results) > 1 else 1.0,
            'total_trades': len(all_trades),
            'model_dimension': agent.state_size
        }
        
        return results, overall_stats
    
    @staticmethod
    def print_performance_report(results: List[Dict], stats: Dict):
        """61ì°¨ì› ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        print("\n" + "="*60)
        print(f"61ì°¨ì› RL Decision ê¸°ë°˜ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼")
        print("="*60)
        print(f"ëª¨ë¸ ì°¨ì›: {stats['model_dimension']}ì°¨ì›")
        print(f"ì „ì²´ ìŠ¹ë¥ : {stats['overall_win_rate']:.3f}")
        print(f"í‰ê·  ìˆ˜ìµë¥ : {stats['avg_return']:.3f} ({stats['avg_return']*100:.1f}%)")
        print(f"í‰ê·  ë¦¬ì›Œë“œ: {stats['avg_reward']:.1f}")
        print(f"ì—í”¼ì†Œë“œë‹¹ í‰ê·  ê±°ë˜ ìˆ˜: {stats['avg_trades_per_episode']:.1f}")
        print(f"í‰ê·  ìµœëŒ€ ë‚™í­: {stats['avg_max_drawdown']:.3f}")
        print(f"ì„±ê³¼ ì¼ê´€ì„±: {stats['consistency']:.3f}")
        print(f"ì´ ê±°ë˜ ìˆ˜: {stats['total_trades']}")
        
        grade = PerformanceAnalyzer._get_performance_grade(stats)
        print(f"\nì„±ëŠ¥ ë“±ê¸‰: {grade}")
        
        recommendations = PerformanceAnalyzer._get_recommendations(stats)
        print("\nê°œì„  ì œì•ˆ:")
        for rec in recommendations:
            print(f"  â€¢ {rec}")
    
    @staticmethod
    def _get_performance_grade(stats: Dict) -> str:
        """ì„±ëŠ¥ ë“±ê¸‰ ì‚°ì¶œ (ìˆ˜ìµë¥  ì¤‘ì‹¬)"""
        avg_return = stats['avg_return']
        consistency = stats['consistency']
        win_rate = stats['overall_win_rate']  # ë³´ì¡° ì§€í‘œ
        
        score = 0
        
        # ìˆ˜ìµë¥ ì´ ì£¼ìš” í‰ê°€ ê¸°ì¤€ (ê°€ì¤‘ì¹˜ ë†’ìŒ)
        if avg_return >= 0.20: score += 5  # 20% ì´ìƒ
        elif avg_return >= 0.15: score += 4  # 15% ì´ìƒ
        elif avg_return >= 0.10: score += 3  # 10% ì´ìƒ
        elif avg_return >= 0.05: score += 2  # 5% ì´ìƒ
        elif avg_return >= 0.0: score += 1   # 0% ì´ìƒ
        
        # ì¼ê´€ì„± (ì¤‘ìš”í•˜ì§€ë§Œ ìˆ˜ìµë¥ ë³´ë‹¤ ë‚®ì€ ê°€ì¤‘ì¹˜)
        if consistency >= 0.8: score += 2
        elif consistency >= 0.6: score += 1
        
        # ìŠ¹ë¥  (ë³´ì¡° ì§€í‘œ)
        if win_rate >= 0.6: score += 1
        
        grades = {8: "A+ (ìš°ìˆ˜)", 7: "A (ì¢‹ìŒ)", 6: "B+ (ì–‘í˜¸)", 5: "B (ë³´í†µ)", 
                 4: "C+ (ë¯¸í¡)", 3: "C (ê°œì„ í•„ìš”)", 2: "D (ë‚˜ì¨)", 1: "F (ë§¤ìš°ë‚˜ì¨)", 0: "F (ì‹¤íŒ¨)"}
        
        return grades.get(score, "F (ì‹¤íŒ¨)")
    
    @staticmethod
    def _get_recommendations(stats: Dict) -> List[str]:
        """ì„±ëŠ¥ ê¸°ë°˜ ê°œì„  ì œì•ˆ (ìˆ˜ìµë¥  ì¤‘ì‹¬)"""
        recommendations = []
        
        # ìˆ˜ìµë¥ ì´ ì£¼ìš” ê¸°ì¤€
        if stats['avg_return'] < 0.05:
            recommendations.append("ìˆ˜ìµë¥ ì´ 5% ë¯¸ë§Œì…ë‹ˆë‹¤. ìˆ˜ìµë¥  ì¤‘ì‹¬ ë³´ìƒ í•¨ìˆ˜ë¥¼ ë” ê°•í™”í•˜ì„¸ìš”.")
        
        if stats['avg_return'] < 0.10:
            recommendations.append("ìˆ˜ìµë¥ ì´ 10% ë¯¸ë§Œì…ë‹ˆë‹¤. 61ì°¨ì› ìƒíƒœ ê³µê°„ì˜ ìˆ˜ìµë¥  ìµœì í™”ë¥¼ ë” í™œìš©í•˜ì„¸ìš”.")
        
        if stats['avg_return'] < 0.15:
            recommendations.append("ìˆ˜ìµë¥ ì´ 15% ë¯¸ë§Œì…ë‹ˆë‹¤. Signal ê¸°ë°˜ ìˆ˜ìµë¥  ì˜ˆì¸¡ì„ ê°œì„ í•˜ì„¸ìš”.")
        
        # ë¦¬ìŠ¤í¬ ê´€ë¦¬
        if stats['avg_max_drawdown'] > 0.2:
            recommendations.append("ìµœëŒ€ ë‚™í­ì´ í½ë‹ˆë‹¤. ìˆ˜ìµë¥ ê³¼ ë¦¬ìŠ¤í¬ì˜ ê· í˜•ì„ ë§ì¶”ì„¸ìš”.")
        
        # ì¼ê´€ì„±
        if stats['consistency'] < 0.5:
            recommendations.append("ì„±ê³¼ ì¼ê´€ì„±ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤. ìˆ˜ìµë¥  ì•ˆì •ì„±ì„ ìœ„í•œ ë” ë§ì€ í›ˆë ¨ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ê±°ë˜ ë¹ˆë„
        if stats['avg_trades_per_episode'] < 3:
            recommendations.append("ê±°ë˜ ë¹ˆë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ìˆ˜ìµë¥  ê¸°íšŒë¥¼ ë†“ì¹˜ì§€ ì•Šë„ë¡ Signal ê°ë„ë¥¼ ì¡°ì •í•˜ì„¸ìš”.")
        
        # ìŠ¹ë¥ ì€ ë³´ì¡° ì§€í‘œ
        if stats['overall_win_rate'] < 0.4:
            recommendations.append("ìŠ¹ë¥ ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤. ìˆ˜ìµë¥ ê³¼ ìŠ¹ë¥ ì˜ ê· í˜•ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        if not recommendations:
            recommendations.append("61ì°¨ì› RL Decision ê¸°ë°˜ ìˆ˜ìµë¥  ì¤‘ì‹¬ ì‹œìŠ¤í…œì´ ì˜ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤!")
        
        return recommendations

class TrainingManager:
    """61ì°¨ì› RL Decision ê¸°ë°˜ í›ˆë ¨ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def train_agent(agent: RLAgent, train_env: TradingEnvironment, 
                   episodes: int = 1000, save_interval: int = 100, 
                   test_env: TradingEnvironment = None) -> Tuple[RLAgent, List[float], List[float]]:
        """61ì°¨ì› RL Decision ê¸°ë°˜ ì—ì´ì „íŠ¸ í›ˆë ¨ (í…ŒìŠ¤íŠ¸ í™˜ê²½ ëª¨ë‹ˆí„°ë§ í¬í•¨)"""
        print(f"61ì°¨ì› RL Decision ê¸°ë°˜ ê°•í™”í•™ìŠµ í›ˆë ¨ ì‹œì‘ ({episodes} ì—í”¼ì†Œë“œ)")
        print(f"ìƒíƒœ ê³µê°„: {train_env.observation_space.shape[0]}ì°¨ì›")
        if test_env:
            print(f"í…ŒìŠ¤íŠ¸ í™˜ê²½ ëª¨ë‹ˆí„°ë§: í™œì„±í™”")
        
        episode_rewards = []
        episode_win_rates = []
        episode_returns = []  # í›ˆë ¨ ë°ì´í„° ìˆ˜ìµë¥  ì¶”ì 
        test_win_rates = []  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìŠ¹ë¥  ì¶”ì 
        best_win_rate = 0.0
        best_test_win_rate = 0.0
        
        for episode in range(episodes):
            state, _ = train_env.reset()
            total_reward = 0
            episode_trades = []
            steps = 0
            
            while steps < 500:
                action = agent.act(state)
                next_state, reward, done, truncated, info = train_env.step(action)
                
                agent.remember(state, action, reward, next_state, done)
                
                if info.get('trade_completed', False):
                    trade_pnl = info.get('trade_pnl', 0.0)
                    episode_trades.append(1 if trade_pnl > 0 else 0)
                
                state = next_state
                total_reward += reward
                steps += 1
                
                if len(agent.memory) > agent.batch_size:
                    agent.replay()
                
                if done:
                    break
            
            episode_rewards.append(total_reward)
            episode_win_rate = np.mean(episode_trades) if episode_trades else 0.0
            episode_win_rates.append(episode_win_rate)
            
            # í›ˆë ¨ ë°ì´í„° ìˆ˜ìµë¥  ê³„ì‚° (ì”ê³  ë³€í™” ê¸°ë°˜)
            initial_balance = train_env.initial_balance  # í™˜ê²½ì˜ ì‹¤ì œ ì´ˆê¸° ì”ê³  ì‚¬ìš©
            final_balance = info.get('balance', initial_balance)
            episode_return = (final_balance - initial_balance) / initial_balance
            episode_returns.append(episode_return)
            
            agent.training_rewards.append(total_reward)
            agent.win_rates.append(episode_win_rate)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€ (20 ì—í”¼ì†Œë“œë§ˆë‹¤)
            if test_env and episode % 10 == 0 and episode > 0:
                print(f"\nğŸ“Š Episode {episode}: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
                test_results, test_stats = PerformanceAnalyzer.evaluate_agent(agent, test_env, num_episodes=5)
                test_return = test_stats['avg_return']
                test_win_rates.append(test_stats['overall_win_rate'])  # ìŠ¹ë¥ ë„ ì¶”ì í•˜ì§€ë§Œ ì €ì¥ ê¸°ì¤€ì€ ìˆ˜ìµë¥ 
                
                print(f"   í…ŒìŠ¤íŠ¸ ìˆ˜ìµë¥ : {test_return:.3f} ({test_return*100:.1f}%) (ì´ì „ ìµœê³ : {best_test_win_rate:.3f})")
                
                if test_return > best_test_win_rate:  # ë³€ìˆ˜ëª…ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì§€ë§Œ ìˆ˜ìµë¥ ë¡œ ì‚¬ìš©
                    best_test_win_rate = test_return
                    # ì—í”¼ì†Œë“œë³„ ëª¨ë¸ ì €ì¥ (ìˆ˜ìµë¥  ê¸°ì¤€)
                    agent.save_model(f'best_test_model_ep{episode}_return{test_return:.3f}.pth')
                    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì—…ë°ì´íŠ¸ (ìˆ˜ìµë¥  ê¸°ì¤€)
                    agent.save_model('agent/best_test_performance_model_return{:.3f}.pth'.format(test_return))
                    print(f"ğŸ¯ ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìµœê³  ìˆ˜ìµë¥ ! ìˆ˜ìµë¥ : {test_return:.3f} ({test_return*100:.1f}%)")
                    print(f"   ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì—…ë°ì´íŠ¸: best_test_performance_model_return{test_return:.3f}.pth")
                print()  # ë¹ˆ ì¤„ ì¶”ê°€
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if episode % 10 == 0 or episode < 10:
                recent_rewards = episode_rewards[-50:] if len(episode_rewards) >= 50 else episode_rewards
                recent_win_rates = episode_win_rates[-50:] if len(episode_win_rates) >= 50 else episode_win_rates
                recent_returns = episode_returns[-50:] if len(episode_returns) >= 50 else episode_returns
                
                avg_reward = np.mean(recent_rewards)
                avg_win_rate = np.mean(recent_win_rates)
                avg_return = np.mean(recent_returns)
                
                # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ë„ í•¨ê»˜ í‘œì‹œ
                test_info = ""
                if test_win_rates:
                    recent_test_win_rate = np.mean(test_win_rates[-5:]) if len(test_win_rates) >= 5 else test_win_rates[-1]
                    test_info = f" | í…ŒìŠ¤íŠ¸: {recent_test_win_rate:.3f}"
                
                print(f"Episode {episode:4d} | "
                        f"í›ˆë ¨ìŠ¹ë¥ : {avg_win_rate:.3f} | "
                        f"í›ˆë ¨ìˆ˜ìµë¥ : {avg_return:.3f}{test_info} | "
                        f"ë¦¬ì›Œë“œ: {avg_reward:7.1f} | "
                        f"ì”ê³ : ${info['balance']:7.0f} | "
                        f"Îµ: {agent.epsilon:.3f} | "
                        f"61D")
            
            # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥ (í›ˆë ¨ ë°ì´í„° ê¸°ì¤€ - ìˆ˜ìµë¥  ì¤‘ì‹¬)
            if episode % save_interval == 0 and episode > 0:
                # ìµœê·¼ 100 ì—í”¼ì†Œë“œì˜ í‰ê·  ìˆ˜ìµë¥  ê³„ì‚°
                recent_returns = []
                for i in range(max(0, len(episode_rewards)-100), len(episode_rewards)):
                    if i < len(episode_rewards):
                        # ê°„ë‹¨í•œ ìˆ˜ìµë¥  ì¶”ì • (ë¦¬ì›Œë“œ ê¸°ë°˜)
                        estimated_return = episode_rewards[i] / 1000.0  # ë¦¬ì›Œë“œë¥¼ ìˆ˜ìµë¥ ë¡œ ê·¼ì‚¬
                        recent_returns.append(estimated_return)
                
                current_avg_return = np.mean(recent_returns) if recent_returns else 0.0
                
                if current_avg_return > best_win_rate:  # ë³€ìˆ˜ëª…ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì§€ë§Œ ìˆ˜ìµë¥ ë¡œ ì‚¬ìš©
                    best_win_rate = current_avg_return
                    agent.save_model(f'best_train_model_ep{episode}_return{current_avg_return:.3f}.pth')
                    print(f"ğŸ¯ ìƒˆë¡œìš´ í›ˆë ¨ ë°ì´í„°ì…‹ ìµœê³  ìˆ˜ìµë¥ ! ìˆ˜ìµë¥ : {current_avg_return:.3f} ({current_avg_return*100:.1f}%)")
            
            # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ (í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê¸°ì¤€ - ìˆ˜ìµë¥  ì¤‘ì‹¬)
            if episode > 1000 and test_win_rates:
                # ìµœê·¼ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë“¤ì˜ ìˆ˜ìµë¥  í™•ì¸
                recent_test_returns = []
                for i in range(max(0, len(test_win_rates)-5), len(test_win_rates)):
                    if i < len(test_win_rates):
                        # í…ŒìŠ¤íŠ¸ ìˆ˜ìµë¥  ì¶”ì • (ìŠ¹ë¥ ì„ ìˆ˜ìµë¥ ë¡œ ê·¼ì‚¬)
                        estimated_return = test_win_rates[i] * 0.1  # ìŠ¹ë¥  65% = ìˆ˜ìµë¥  6.5%ë¡œ ê·¼ì‚¬
                        recent_test_returns.append(estimated_return)
                
                recent_test_return = np.mean(recent_test_returns) if recent_test_returns else 0.0
                
                if recent_test_return >= 0.30:  # ìˆ˜ìµë¥  5% ì´ìƒ ë‹¬ì„±
                    print(f"ğŸ† 61ì°¨ì› ëª©í‘œ ë‹¬ì„±! í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìˆ˜ìµë¥  {recent_test_return:.3f} ({recent_test_return*100:.1f}%) ë„ë‹¬")
                    agent.save_model('agent/final_optimized_model_61d.pth')
                    break
        
        print(f"\n61ì°¨ì› RL Decision ê¸°ë°˜ í›ˆë ¨ ì™„ë£Œ!")
        print(f"   ì´ ì—í”¼ì†Œë“œ: {episode + 1}")
        print(f"   í›ˆë ¨ ë°ì´í„° ìµœê³  ìŠ¹ë¥ : {best_win_rate:.3f}")
        print(f"   í›ˆë ¨ ë°ì´í„° ìµœì¢… ìŠ¹ë¥ : {np.mean(episode_win_rates[-50:]) if episode_win_rates else 0:.3f}")
        if test_win_rates:
            print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„° ìµœê³  ìŠ¹ë¥ : {best_test_win_rate:.3f}")
            print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„° ìµœì¢… ìŠ¹ë¥ : {test_win_rates[-1]:.3f}")
        print(f"   ìƒíƒœ ì°¨ì›: 61ì°¨ì› (RL Decision ê¸°ë°˜)")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if test_win_rates and best_test_win_rate > 0:
            best_test_model_path = f'agent/best_test_performance_model_wr{best_test_win_rate:.3f}.pth'
            agent.save_model(best_test_model_path)
            print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {best_test_model_path}")
        
        return agent, episode_rewards, episode_win_rates

def split_data(price_data: pd.DataFrame, signal_data: List[Dict], 
               train_ratio: float = 0.8, test_ratio: float = 0.2) -> Tuple[pd.DataFrame, List[Dict], pd.DataFrame, List[Dict]]:
    """ë°ì´í„°ë¥¼ í›ˆë ¨ìš©ê³¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¶„í• """
    total_length = min(len(price_data), len(signal_data))
    train_size = int(total_length * train_ratio)
    
    # í›ˆë ¨ ë°ì´í„°
    train_price = price_data.iloc[:train_size].reset_index(drop=True)
    train_signal = signal_data[:train_size]
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_price = price_data.iloc[train_size:].reset_index(drop=True)
    test_signal = signal_data[train_size:]
    
    print(f"ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
    print(f"  - í›ˆë ¨ ë°ì´í„°: {len(train_price):,}ê°œ ({train_ratio*100:.1f}%)")
    print(f"  - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_price):,}ê°œ ({test_ratio*100:.1f}%)")
    
    return train_price, train_signal, test_price, test_signal

def main():
    """61ì°¨ì› RL Decision ê¸°ë°˜ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("61ì°¨ì› RL Decision ê¸°ë°˜ ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œ")
    print("=" * 80)
    
    try:
        # 1. ë°ì´í„° ë¡œë”©
        print("\n1ï¸âƒ£ 61ì°¨ì›ìš© ë°ì´í„° ë¡œë”©...")
        price_data = DataLoader.load_price_data()
        if price_data is None:
            print("ê°€ê²© ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
            return
        
        signal_data = DataLoader.load_signal_data()
        
        # ë°ì´í„° ê¸¸ì´ ë§ì¶”ê¸°
        min_length = min(len(price_data), len(signal_data))
        price_data = price_data.iloc[:min_length].reset_index(drop=True)
        signal_data = signal_data[:min_length]
        
        print(f"ìµœì¢… 61ì°¨ì›ìš© ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {min_length:,}ê°œ")
        
        # 2. ë°ì´í„° ë¶„í•  (í›ˆë ¨ 80%, í…ŒìŠ¤íŠ¸ 20%)
        print("\n2ï¸âƒ£ ë°ì´í„° ë¶„í• ...")
        train_price, train_signal, test_price, test_signal = split_data(price_data, signal_data, 0.8, 0.2)
        
        # 3. í™˜ê²½ ë° ì—ì´ì „íŠ¸ ìƒì„±
        print("\n3ï¸âƒ£ 61ì°¨ì› í™˜ê²½ ë° ì—ì´ì „íŠ¸ ìƒì„±...")
        train_env = TradingEnvironment(train_price, train_signal)
        test_env = TradingEnvironment(test_price, test_signal)
        agent = RLAgent(train_env.observation_space.shape[0])  # 61ì°¨ì›
        
        print(f"ìƒíƒœ ê³µê°„: {train_env.observation_space.shape[0]}ì°¨ì›")
        print("Signalì˜ ëª¨ë“  indicatorì™€ raw score í™œìš©")
        
        model_loaded = False
        
        # 1. í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ëª¨ë¸ ìš°ì„  ë¡œë“œ
        import glob
        test_model_files = glob.glob('agent/best_test_performance_model_return*.pth')
        if test_model_files:
            # ê°€ì¥ ë†’ì€ ìŠ¹ë¥ ì˜ í…ŒìŠ¤íŠ¸ ëª¨ë¸ ì„ íƒ
            best_test_model = max(test_model_files, key=lambda x: float(x.split('return')[1].split('.pth')[0]))
            if agent.load_model(best_test_model):
                model_loaded = True
                print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ: {best_test_model}")
        
        # 2. í˜¸í™˜ì„± ëª¨ë“œë¡œ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹œë„
        if not model_loaded:
            for model_file in ['agent/final_optimized_model_61d.pth', 'agent/best_model_61d.pth']:
                if os.path.exists(model_file):
                    print(f"ğŸ”„ í˜¸í™˜ì„± ëª¨ë“œë¡œ {model_file} ë¡œë“œ ì‹œë„...")
                    
                    # ëª¨ë¸ í˜¸í™˜ì„± ì§„ë‹¨
                    diagnosis = agent.diagnose_model_compatibility(model_file)
                    if 'error' not in diagnosis:
                        print(f"   ğŸ“Š í˜¸í™˜ì„± ì§„ë‹¨: {diagnosis['compatibility_rate']:.1%} ({diagnosis['compatible_layers']}/{diagnosis['total_new_layers']} ë ˆì´ì–´)")
                    
                    if agent.load_model_with_compatibility(model_file):
                        model_loaded = True
                        print(f"âœ… í˜¸í™˜ì„± ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_file}")
                        break
        
        # 3. ëª¨ë¸ ë³€í™˜ ì‹œë„ (ê¸°ì¡´ ëª¨ë¸ì„ AdvancedProfitDQN ì•„í‚¤í…ì²˜ë¡œ)
        if not model_loaded:
            for model_file in ['agent/final_optimized_model_61d.pth', 'agent/best_model_61d.pth']:
                if os.path.exists(model_file):
                    print(f"ğŸ”„ ëª¨ë¸ ë³€í™˜ ì‹œë„: {model_file}")
                    if agent.create_compatible_model(model_file):
                        model_loaded = True
                        print(f"âœ… ëª¨ë¸ ë³€í™˜ ì„±ê³µ: {model_file}")
                        break
        
        if not model_loaded:
            print("ìƒˆë¡œìš´ 61ì°¨ì› ëª¨ë¸ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        # 4. í›ˆë ¨ ì „ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€ (ë² ì´ìŠ¤ë¼ì¸)
        print("\n4ï¸âƒ£ í›ˆë ¨ ì „ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€...")
        baseline_results, baseline_stats = PerformanceAnalyzer.evaluate_agent(agent, test_env, num_episodes=5)
        print("=== í›ˆë ¨ ì „ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ ===")
        PerformanceAnalyzer.print_performance_report(baseline_results, baseline_stats)
        
        # 5. í›ˆë ¨ ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨
        print(f"\n5ï¸âƒ£ í›ˆë ¨ ë°ì´í„°ì…‹ìœ¼ë¡œ 61ì°¨ì› RL Decision ê¸°ë°˜ í›ˆë ¨ ì‹œì‘...")
        print(f"   í›ˆë ¨ ë°ì´í„°: {len(train_price):,}ê°œ")
        print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_price):,}ê°œ")
        print(f"   ëª©í‘œ ìˆ˜ìµë¥ : 5%+ (ìˆ˜ìµë¥  ì¤‘ì‹¬)")
        print(f"   Signal íŠ¹ì„± í™œìš©: ìˆ˜ìµë¥  ìµœì í™”")
        
        # í›ˆë ¨ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ í™˜ê²½ ëª¨ë‹ˆí„°ë§ í¬í•¨)
        trained_agent, rewards, win_rates = TrainingManager.train_agent(agent, train_env, episodes=500, test_env=test_env)
        
        # 6. í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€
        print("\n6ï¸âƒ£ í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€...")
        final_results, final_stats = PerformanceAnalyzer.evaluate_agent(trained_agent, test_env, num_episodes=10)
        print("=== í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ ===")
        PerformanceAnalyzer.print_performance_report(final_results, final_stats)
        
        # 7. ì„±ëŠ¥ ê°œì„ ë„ ë¶„ì„
        improvement = final_stats['overall_win_rate'] - baseline_stats['overall_win_rate']
        print(f"\nğŸš€ 61ì°¨ì› RL Decision ê¸°ë°˜ ì„±ëŠ¥ ê°œì„ ë„ (í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê¸°ì¤€):")
        print(f"   ìŠ¹ë¥ : {baseline_stats['overall_win_rate']:.3f} â†’ {final_stats['overall_win_rate']:.3f} ({improvement:+.3f})")
        print(f"   í‰ê·  ìˆ˜ìµë¥ : {baseline_stats['avg_return']:.3f} â†’ {final_stats['avg_return']:.3f}")
        print(f"   Signal í™œìš©ë„: ìµœëŒ€í™”ë¨")
        
        # 8. ìµœì¢… ëª¨ë¸ ì €ì¥
        trained_agent.save_model('agent/final_optimized_model_61d.pth')
        print(f"\nâœ… ìµœì¢… ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: agent/final_optimized_model_61d.pth")
        
        # 9. ì¶”ê°€ í›ˆë ¨ ì—¬ë¶€ í™•ì¸ (ìˆ˜ìµë¥  ê¸°ì¤€)
        if final_stats['avg_return'] < 0.05:  # ìˆ˜ìµë¥  5% ë¯¸ë§Œ
            user_input = input("\nìˆ˜ìµë¥ ì´ ëª©í‘œ(5%)ì— ë¯¸ë‹¬í•©ë‹ˆë‹¤. ì¶”ê°€ í›ˆë ¨ì„ ì›í•˜ì‹œë‚˜ìš”? (y/n): ")
            if user_input.lower() == 'y':
                print("61ì°¨ì› ìˆ˜ìµë¥  ì¤‘ì‹¬ ì¶”ê°€ í›ˆë ¨ ì‹œì‘...")
                TrainingManager.train_agent(trained_agent, train_env, episodes=1000, test_env=test_env)
                
                # ì¶”ê°€ í›ˆë ¨ í›„ ì¬í‰ê°€
                print("\nì¶”ê°€ í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€...")
                additional_results, additional_stats = PerformanceAnalyzer.evaluate_agent(trained_agent, test_env, num_episodes=10)
                print("=== ì¶”ê°€ í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ ===")
                PerformanceAnalyzer.print_performance_report(additional_results, additional_stats)
        else:
            print(f"\nğŸ‰ ëª©í‘œ ìˆ˜ìµë¥  ë‹¬ì„±! (í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìˆ˜ìµë¥ : {final_stats['avg_return']:.3f} ({final_stats['avg_return']*100:.1f}%))")
    
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()