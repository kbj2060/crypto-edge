"""
Multi-Timeframe Transformer ë”¥ëŸ¬ë‹ ëª¨ë¸
- Decision ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ì„œ ì˜ì‚¬ê²°ì •ì„ ìˆ˜í–‰
- ë‹¤ì¤‘ ì‹œê°„í”„ë ˆì„ ë¶„ì„ (3m, 15m, 1h)
- Transformer ê¸°ë°˜ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
- ìˆ˜ìµë¥  ìµœì í™” ì¤‘ì‹¬ ì„¤ê³„
"""

# =============================================================================
# í•˜ì´í¼íŒŒë¼ë¯¸í„° í™˜ê²½ë³€ìˆ˜ ì„¤ì •
# =============================================================================

# ëª¨ë¸ ì•„í‚¤í…ì²˜ íŒŒë¼ë¯¸í„°
MODEL_INPUT_SIZE = 58
MODEL_D_MODEL = 256
MODEL_NHEAD = 8
MODEL_NUM_LAYERS = 6
MODEL_DROPOUT = 0.1
MODEL_MAX_SEQ_LEN = 100

# í›ˆë ¨ íŒŒë¼ë¯¸í„°
TRAINING_BATCH_SIZE = 64
TRAINING_EPOCHS = 100
SEQUENCE_LENGTH = 30
TRAINING_VALIDATION_SPLIT = 0.2
TRAINING_LEARNING_RATE = 5e-3
TRAINING_WEIGHT_DECAY = 1e-5
TRAINING_PATIENCE = 20
LABEL_PROFIT_THRESHOLD = 0.01

# ë°ì´í„° ì²˜ë¦¬ íŒŒë¼ë¯¸í„°
DATA_TEST_LIMIT = 10000
DATA_NORMALIZATION_ENABLED = True

# ì†ì‹¤ í•¨ìˆ˜ ê°€ì¤‘ì¹˜
LOSS_PROFIT_WEIGHT = 3.0
LOSS_ACTION_WEIGHT = 2.0
LOSS_OTHER_WEIGHT = 1.0

# ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
GRADIENT_CLIP_NORM = 1.0

# ëª¨ë¸ ì €ì¥ ê²½ë¡œ
MODEL_SAVE_PATH = 'agent/best_multitimeframe_model.pth'
MODEL_SEQUENCE_SAVE_PATH = 'agent/best_multitimeframe_sequence_model.pth'
MODEL_FINAL_SAVE_PATH = 'agent/multitimeframe_transformer_trained.pth'

# ë°ì´í„° íŒŒì¼ ê²½ë¡œ
DATA_FILE_PATH = 'agent/decisions_data.parquet'

# í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°
TEST_SAMPLES_COUNT = 10

# ê³ ê¸‰ ë¼ë²¨ë§ íŒŒë¼ë¯¸í„°
LABEL_PROFIT_THRESHOLD = 0.02  # 2% ê¸°ë³¸ ì„ê³„ê°’
LABEL_VOLATILITY_FACTOR = 1.5  # ë³€ë™ì„± ì¡°ì • ê³„ìˆ˜
LABEL_TREND_FACTOR = 1.2       # íŠ¸ë Œë“œ ê°•ë„ ê³„ìˆ˜
LABEL_VOLUME_FACTOR = 1.1      # ê±°ë˜ëŸ‰ ê³„ìˆ˜
LABEL_MIN_CONFIDENCE = 0.3     # ìµœì†Œ ì‹ ë¢°ë„
LABEL_MAX_CONFIDENCE = 0.95    # ìµœëŒ€ ì‹ ë¢°ë„
LABEL_LOOKAHEAD_STEPS = 5      # ë¯¸ë˜ ë°ì´í„° ì°¸ì¡° ìŠ¤í…

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
import os
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path
import json
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_absolute_error, accuracy_score

# PyTorch í˜¸í™˜ì„± ì„¤ì •
def setup_pytorch_compatibility():
    """PyTorch ë²„ì „ í˜¸í™˜ì„± ì„¤ì •"""
    try:
        safe_globals = [
            np.ndarray, np.dtype, np.float32, np.float64, np.int32, np.int64,
        ]
        
        try:
            import numpy._core.multiarray
            safe_globals.append(numpy._core.multiarray.scalar)
        except ImportError:
            try:
                import numpy.core.multiarray
                safe_globals.append(numpy.core.multiarray.scalar)
            except ImportError:
                pass
        
        torch.serialization.add_safe_globals(safe_globals)
        print("PyTorch í˜¸í™˜ ì„¤ì • ì™„ë£Œ (NumPy 2.0 í˜¸í™˜)")
    except AttributeError:
        print("PyTorch ì´ì „ ë²„ì „ ê°ì§€ë¨")

setup_pytorch_compatibility()

class DataNormalizer:
    """ë°ì´í„° ì •ê·œí™” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.scalers = {}
        self.is_fitted = False
    
    def fit_transform(self, data_list: List[Dict]) -> List[Dict]:
        """ë°ì´í„° ì •ê·œí™” ì ìš©"""
        if not data_list:
            return data_list
        
        # íŠ¹ì„±ë³„ë¡œ ë¶„ë¥˜
        price_features = ['open', 'high', 'low', 'close', 'volume', 'quote_volume']
        indicator_features = [f'indicator_{col}' for col in [
            'vwap', 'atr', 'poc', 'hvn', 'lvn', 'vwap_std',
            'prev_day_high', 'prev_day_low', 'opening_range_high', 'opening_range_low'
        ]]
        score_features = [col for col in data_list[0].keys() if col.endswith('_score')]
        entry_stop_features = [col for col in data_list[0].keys() if col.endswith('_entry') or col.endswith('_stop')]
        
        # ë°ì´í„°ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        price_data = np.array([[float(data.get(f, 0.0)) for f in price_features] for data in data_list])
        indicator_data = np.array([[float(data.get(f, 0.0)) for f in indicator_features] for data in data_list])
        score_data = np.array([[float(data.get(f, 0.0)) for f in score_features] for data in data_list])
        entry_stop_data = np.array([[float(data.get(f, 0.0)) for f in entry_stop_features] for data in data_list])
        
        # ê°€ê²© ë°ì´í„°: Min-Max ì •ê·œí™” (0-1 ë²”ìœ„)
        self.scalers['price'] = MinMaxScaler()
        price_normalized = self.scalers['price'].fit_transform(price_data)
        
        # ì§€í‘œ ë°ì´í„°: Standard ì •ê·œí™”
        self.scalers['indicator'] = StandardScaler()
        indicator_normalized = self.scalers['indicator'].fit_transform(indicator_data)
        
        # ì ìˆ˜ ë°ì´í„°: Min-Max ì •ê·œí™” (0-1 ë²”ìœ„)
        self.scalers['score'] = MinMaxScaler()
        score_normalized = self.scalers['score'].fit_transform(score_data)
        
        # Entry/Stop ë°ì´í„°: Min-Max ì •ê·œí™” (0-1 ë²”ìœ„)
        self.scalers['entry_stop'] = MinMaxScaler()
        entry_stop_normalized = self.scalers['entry_stop'].fit_transform(entry_stop_data)
        
        # ì •ê·œí™”ëœ ë°ì´í„°ë¥¼ ì›ë³¸ í˜•íƒœë¡œ ë³µì›
        normalized_data = []
        for i, data in enumerate(data_list):
            normalized_item = data.copy()
            
            # ê°€ê²© ë°ì´í„° ì •ê·œí™”
            for j, feature in enumerate(price_features):
                normalized_item[feature] = price_normalized[i][j]
            
            # ì§€í‘œ ë°ì´í„° ì •ê·œí™”
            for j, feature in enumerate(indicator_features):
                normalized_item[feature] = indicator_normalized[i][j]
            
            # ì ìˆ˜ ë°ì´í„° ì •ê·œí™”
            for j, feature in enumerate(score_features):
                normalized_item[feature] = score_normalized[i][j]
            
            # Entry/Stop ë°ì´í„° ì •ê·œí™”
            for j, feature in enumerate(entry_stop_features):
                normalized_item[feature] = entry_stop_normalized[i][j]
            
            normalized_data.append(normalized_item)
        
        self.is_fitted = True
        return normalized_data
    
    def transform(self, data_list: List[Dict]) -> List[Dict]:
        """ì´ë¯¸ í”¼íŒ…ëœ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ì •ê·œí™” ì ìš© (fit ì—†ì´)"""
        if not self.is_fitted:
            raise ValueError("ìŠ¤ì¼€ì¼ëŸ¬ê°€ í”¼íŒ…ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. fit_transformì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        if not data_list:
            return data_list
        
        # íŠ¹ì„±ë³„ë¡œ ë¶„ë¥˜ (fit_transformê³¼ ë™ì¼)
        price_features = ['open', 'high', 'low', 'close', 'volume', 'quote_volume']
        indicator_features = [f'indicator_{col}' for col in [
            'vwap', 'atr', 'poc', 'hvn', 'lvn', 'vwap_std',
            'prev_day_high', 'prev_day_low', 'opening_range_high', 'opening_range_low'
        ]]
        score_features = [col for col in data_list[0].keys() if col.endswith('_score')]
        entry_stop_features = [col for col in data_list[0].keys() if col.endswith('_entry') or col.endswith('_stop')]
        
        # ë°ì´í„°ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        price_data = np.array([[float(data.get(f, 0.0)) for f in price_features] for data in data_list])
        indicator_data = np.array([[float(data.get(f, 0.0)) for f in indicator_features] for data in data_list])
        score_data = np.array([[float(data.get(f, 0.0)) for f in score_features] for data in data_list])
        entry_stop_data = np.array([[float(data.get(f, 0.0)) for f in entry_stop_features] for data in data_list])
        
        # ì´ë¯¸ í”¼íŒ…ëœ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ transformë§Œ ìˆ˜í–‰
        price_normalized = self.scalers['price'].transform(price_data)
        indicator_normalized = self.scalers['indicator'].transform(indicator_data)
        score_normalized = self.scalers['score'].transform(score_data)
        entry_stop_normalized = self.scalers['entry_stop'].transform(entry_stop_data)
        
        # ì •ê·œí™”ëœ ë°ì´í„°ë¥¼ ì›ë³¸ í˜•íƒœë¡œ ë³µì›
        normalized_data = []
        for i, data in enumerate(data_list):
            normalized_item = data.copy()
            
            # ê°€ê²© ë°ì´í„° ì •ê·œí™”
            for j, feature in enumerate(price_features):
                normalized_item[feature] = price_normalized[i][j]
            
            # ì§€í‘œ ë°ì´í„° ì •ê·œí™”
            for j, feature in enumerate(indicator_features):
                normalized_item[feature] = indicator_normalized[i][j]
            
            # ì ìˆ˜ ë°ì´í„° ì •ê·œí™”
            for j, feature in enumerate(score_features):
                normalized_item[feature] = score_normalized[i][j]
            
            # Entry/Stop ë°ì´í„° ì •ê·œí™”
            for j, feature in enumerate(entry_stop_features):
                normalized_item[feature] = entry_stop_normalized[i][j]
            
            normalized_data.append(normalized_item)
        
        return normalized_data

class PositionalEncoding(nn.Module):
    """ìœ„ì¹˜ ì¸ì½”ë”©"""
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=0.1)
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

class MultiTimeframeTransformer(nn.Module):
    """Multi-Timeframe Transformer ëª¨ë¸"""
    
    def __init__(self, 
                 input_size: int = MODEL_INPUT_SIZE,
                 d_model: int = MODEL_D_MODEL,
                 nhead: int = MODEL_NHEAD,
                 num_layers: int = MODEL_NUM_LAYERS,
                 dropout: float = MODEL_DROPOUT,
                 max_seq_len: int = MODEL_MAX_SEQ_LEN):
        super().__init__()
        
        self.input_size = input_size
        self.d_model = d_model
        self.nhead = nhead
        self.num_layers = num_layers
        self.max_seq_len = max_seq_len
        
        # ì…ë ¥ ì„ë² ë”©
        self.input_embedding = nn.Sequential(
            nn.Linear(input_size, d_model),
            nn.LayerNorm(d_model),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # ìœ„ì¹˜ ì¸ì½”ë”©
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)
        
        # Transformer ì¸ì½”ë”
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            activation='relu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # ì‹œê°„í”„ë ˆì„ë³„ íŠ¹ì„± ì¶”ì¶œê¸°
        self.timeframe_extractors = nn.ModuleDict({
            'short_term': self._build_timeframe_extractor(d_model, 'short'),
            'medium_term': self._build_timeframe_extractor(d_model, 'medium'),
            'long_term': self._build_timeframe_extractor(d_model, 'long')
        })
        
        # ì˜ì‚¬ê²°ì • í—¤ë“œë“¤ (ë‹¨ìˆœí™”: action, confidence, profitë§Œ)
        self.decision_heads = nn.ModuleDict({
            'action': nn.Sequential(
                nn.Linear(d_model, d_model // 2),
                nn.ReLU(),
                nn.LayerNorm(d_model // 2),
                nn.Dropout(dropout),
                nn.Linear(d_model // 2, 3)  # BUY, SELL, HOLD
            ),
            'confidence': nn.Sequential(
                nn.Linear(d_model, d_model // 2),
                nn.ReLU(),
                nn.LayerNorm(d_model // 2),
                nn.Dropout(dropout),
                nn.Linear(d_model // 2, 1),
                nn.Sigmoid()
            )
        })
        
        # ìˆ˜ìµë¥  ì˜ˆì¸¡ê¸°
        self.profit_predictor = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.LayerNorm(d_model // 2),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, 1)
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.apply(self._init_weights)
    
    def _build_timeframe_extractor(self, d_model: int, timeframe: str):
        """ì‹œê°„í”„ë ˆì„ë³„ íŠ¹ì„± ì¶”ì¶œê¸°"""
        if timeframe == 'short':
            # ë‹¨ê¸°: ë¹ ë¥¸ ë°˜ì‘, ë†’ì€ ë¯¼ê°ë„
            return nn.Sequential(
                nn.Linear(d_model, d_model // 2),
                nn.ReLU(),
                nn.LayerNorm(d_model // 2),
                nn.Dropout(0.05),  # ë‚®ì€ ë“œë¡­ì•„ì›ƒ
                nn.Linear(d_model // 2, d_model // 2)
            )
        elif timeframe == 'medium':
            # ì¤‘ê¸°: ê· í˜•ì¡íŒ ë¶„ì„
            return nn.Sequential(
                nn.Linear(d_model, d_model // 2),
                nn.ReLU(),
                nn.LayerNorm(d_model // 2),
                nn.Dropout(0.1),
                nn.Linear(d_model // 2, d_model // 2)
            )
        else:  # long
            # ì¥ê¸°: ì•ˆì •ì , ë‚®ì€ ë¯¼ê°ë„
            return nn.Sequential(
                nn.Linear(d_model, d_model // 2),
                nn.ReLU(),
                nn.LayerNorm(d_model // 2),
                nn.Dropout(0.15),  # ë†’ì€ ë“œë¡­ì•„ì›ƒ
                nn.Linear(d_model // 2, d_model // 2)
            )
    
    def _init_weights(self, module):
        """Xavier ì´ˆê¸°í™”"""
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            nn.init.constant_(module.bias, 0)
    
    def forward(self, x, mask=None):
        """
        Args:
            x: [batch_size, seq_len, input_size] ë˜ëŠ” [batch_size, input_size]
            mask: [batch_size, seq_len] (ì„ íƒì )
        """
        # ë°°ì¹˜ ì°¨ì› í™•ì¸
        if x.dim() == 2:
            x = x.unsqueeze(1)  # [batch_size, 1, input_size]
            single_sample = True
        else:
            single_sample = False
        
        batch_size, seq_len, _ = x.shape
        
        # ì…ë ¥ ì„ë² ë”©
        x = self.input_embedding(x)  # [batch_size, seq_len, d_model]
        
        # ìœ„ì¹˜ ì¸ì½”ë”©
        x = x.transpose(0, 1)  # [seq_len, batch_size, d_model]
        x = self.pos_encoding(x)
        x = x.transpose(0, 1)  # [batch_size, seq_len, d_model]
        
        # Transformer ì¸ì½”ë”
        if mask is not None:
            # ë§ˆìŠ¤í¬ ì ìš© (íŒ¨ë”© í† í° ë“±)
            x = self.transformer(x, src_key_padding_mask=mask)
        else:
            x = self.transformer(x)
        
        # ìµœì¢… íŠ¹ì„± (ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ìš”ì†Œ ì‚¬ìš©)
        final_features = x[:, -1, :]  # [batch_size, d_model]
        
        # ì‹œê°„í”„ë ˆì„ë³„ íŠ¹ì„± ì¶”ì¶œ
        timeframe_features = {}
        for timeframe, extractor in self.timeframe_extractors.items():
            timeframe_features[timeframe] = extractor(final_features)
        
        # ì˜ì‚¬ê²°ì • ì¶œë ¥
        decisions = {}
        for head_name, head in self.decision_heads.items():
            decisions[head_name] = head(final_features)
        
        # ìˆ˜ìµë¥  ì˜ˆì¸¡
        profit_pred = self.profit_predictor(final_features)
        
        # ë‹¨ì¼ ìƒ˜í”Œì´ë©´ ë°°ì¹˜ ì°¨ì› ì œê±°
        if single_sample:
            for key in decisions:
                decisions[key] = decisions[key].squeeze(0)
            profit_pred = profit_pred.squeeze(0)
            for key in timeframe_features:
                timeframe_features[key] = timeframe_features[key].squeeze(0)
        
        return decisions, profit_pred, timeframe_features

class MultiTimeframeDecisionEngine:
    """Multi-Timeframe ì˜ì‚¬ê²°ì • ì—”ì§„"""
    
    def __init__(self, 
                 model_path: str = MODEL_SAVE_PATH,
                 input_size: int = MODEL_INPUT_SIZE,
                 d_model: int = MODEL_D_MODEL,
                 nhead: int = MODEL_NHEAD,
                 num_layers: int = MODEL_NUM_LAYERS,
                 device: str = 'auto'):
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        print(f"Multi-Timeframe Transformer ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.model = MultiTimeframeTransformer(
            input_size=input_size,
            d_model=d_model,
            nhead=nhead,
            num_layers=num_layers
        ).to(self.device)
        
        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=TRAINING_LEARNING_RATE,
            weight_decay=TRAINING_WEIGHT_DECAY
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='max',
            factor=0.5,
            patience=10,
        )
        
        # ëª¨ë¸ ë¡œë“œ
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
        
        # ì˜ì‚¬ê²°ì • íˆìŠ¤í† ë¦¬
        self.decision_history = []
        self.performance_history = []
        
        # í†µê³„
        self.total_decisions = 0
        self.correct_decisions = 0
        self.total_profit = 0.0
    
    def _extract_features(self, decision_data: Dict) -> List[float]:
        """ë‹¨ì¼ Decision ë°ì´í„°ì—ì„œ 58ì°¨ì› íŠ¹ì„± ì¶”ì¶œ"""
        features = []
        
        # 1. ì „ëµ Action (16ê°œ)
        strategy_actions = [
            'bollinger_squeeze_action', 'zscore_mean_reversion_action', 'vpvr_action', 
            'ema_confluence_action', 'ichimoku_action', 'htf_trend_action', 
            'funding_rate_action', 'orderflow_cvd_action', 'support_resistance_action', 
            'vwap_pinball_action', 'multi_timeframe_action', 'session_action', 
            'vol_spike_action', 'vpvr_micro_action', 'oi_delta_action', 'liquidity_grab_action'
        ]
        
        for action_col in strategy_actions:
            action = decision_data.get(action_col, 'HOLD')
            if isinstance(action, str):
                if action.upper() == 'BUY':
                    action_value = 1.0
                elif action.upper() == 'SELL':
                    action_value = -1.0
                else:
                    action_value = 0.0
            else:
                action_value = float(action) if action else 0.0
            features.append(action_value)
        
        # 2. ì „ëµ Score (16ê°œ)
        strategy_scores = [
            'bollinger_squeeze_score', 'session_score', 'vpvr_score', 'oi_delta_score',
            'ema_confluence_score', 'multi_timeframe_score', 'vol_spike_score', 
            'vpvr_micro_score', 'htf_trend_score', 'liquidity_grab_score', 
            'ichimoku_score', 'funding_rate_score', 'support_resistance_score', 
            'zscore_mean_reversion_score', 'vwap_pinball_score', 'orderflow_cvd_score'
        ]
        
        for score_col in strategy_scores:
            features.append(float(decision_data.get(score_col, 0.0)))
        
        # 3. ì „ëµ Confidence (1ê°œ - vpvr_confidenceë§Œ ìˆìŒ)
        confidence = decision_data.get('vpvr_confidence', 0.0)
        if isinstance(confidence, str):
            confidence_value = 0.0
        else:
            confidence_value = float(confidence) if confidence else 0.0
        features.append(confidence_value)
        
        # 4. ì „ëµ Entry (4ê°œ)
        strategy_entries = ['vpvr_micro_entry', 'vwap_pinball_entry', 'session_entry', 'vpvr_entry']
        for entry_col in strategy_entries:
            features.append(float(decision_data.get(entry_col, 0.0)))
        
        # 5. ì „ëµ Stop (4ê°œ)
        strategy_stops = ['vpvr_stop', 'session_stop', 'vpvr_micro_stop', 'vwap_pinball_stop']
        for stop_col in strategy_stops:
            features.append(float(decision_data.get(stop_col, 0.0)))
        
        # 6. Indicator ì •ë³´ (10ê°œ)
        indicator_fields = [
            'indicator_prev_day_high', 'indicator_poc', 'indicator_hvn', 'indicator_vwap_std',
            'indicator_opening_range_high', 'indicator_lvn', 'indicator_opening_range_low',
            'indicator_vwap', 'indicator_prev_day_low', 'indicator_atr'
        ]
        
        for field in indicator_fields:
            features.append(float(decision_data.get(field, 0.0)))
        
        # 7. OHLC ë°ì´í„° (6ê°œ)
        features.extend([
            float(decision_data.get('open')),
            float(decision_data.get('high')),
            float(decision_data.get('low')),
            float(decision_data.get('close')),
            float(decision_data.get('volume')),
            float(decision_data.get('quote_volume'))
        ])
        
        # 8. Timestamp (1ê°œ)
        timestamp = decision_data.get('timestamp')
        if isinstance(timestamp, str):
            try:
                timestamp = pd.to_datetime(timestamp).timestamp()
            except:
                timestamp = 0.0
        else:
            timestamp = float(timestamp) if timestamp else 0.0
        
        features.append(timestamp)
        
        # ì°¨ì› ê²€ì¦
        assert len(features) == 58, f"Expected 58 features, got {len(features)}"
        
        return features
    
    def preprocess_decision_data(self, decision_data: Dict) -> torch.Tensor:
        """ë‹¨ì¼ Decision ë°ì´í„° ì „ì²˜ë¦¬ (í˜¸í™˜ì„± ìœ ì§€)"""
        features = self._extract_features(decision_data)
        return torch.FloatTensor(features).unsqueeze(0).to(self.device)
    
    def preprocess_sequence_data(self, decision_data: List[Dict], seq_len: int = SEQUENCE_LENGTH) -> torch.Tensor:
        """ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ë°ì´í„° ì „ì²˜ë¦¬"""
        sequences = []
        
        for i in range(len(decision_data) - seq_len + 1):
            sequence = []
            for j in range(seq_len):
                features = self._extract_features(decision_data[i + j])
                sequence.append(features)
            sequences.append(sequence)
        
        return torch.FloatTensor(sequences).to(self.device)
    
    def make_decision(self, decision_data: Dict) -> Dict:
        """ë‹¨ì¼ ë°ì´í„° ì˜ì‚¬ê²°ì • ìˆ˜í–‰ (í˜¸í™˜ì„± ìœ ì§€)"""
        self.model.eval()
        
        with torch.no_grad():
            # ë°ì´í„° ì „ì²˜ë¦¬
            input_tensor = self.preprocess_decision_data(decision_data)
            
            # ëª¨ë¸ ì¶”ë¡ 
            decisions, profit_pred, timeframe_features = self.model(input_tensor)
            
            # ì˜ì‚¬ê²°ì • ê²°ê³¼ ìƒì„± (ë‹¨ìˆœí™”)
            result = {
                'action': self._interpret_action(decisions['action']),
                'confidence': float(decisions['confidence'].item()),
                'profit': float(profit_pred.item()),
                'timestamp': datetime.now().isoformat(),
                'model_version': 'MultiTimeframeTransformer_v1.0'
            }
            
            # íˆìŠ¤í† ë¦¬ì— ì €ì¥
            self.decision_history.append({
                'input': decision_data,
                'output': result,
                'timestamp': datetime.now()
            })
            
            self.total_decisions += 1
            
            return result
    
    def make_sequence_decision(self, decision_sequence: List[Dict], seq_len: int = SEQUENCE_LENGTH) -> Dict:
        """ì‹œí€€ìŠ¤ ê¸°ë°˜ ì˜ì‚¬ê²°ì • ìˆ˜í–‰ (Transformerì˜ ì§„ì •í•œ ì¥ì  í™œìš©)"""
        self.model.eval()
        
        with torch.no_grad():
            # ì‹œí€€ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬
            if len(decision_sequence) < seq_len:
                # ì‹œí€€ìŠ¤ê°€ ì§§ìœ¼ë©´ íŒ¨ë”©
                padded_sequence = decision_sequence + [decision_sequence[-1]] * (seq_len - len(decision_sequence))
                input_tensor = self.preprocess_sequence_data(padded_sequence, seq_len)
            else:
                # ë§ˆì§€ë§‰ seq_lenê°œë§Œ ì‚¬ìš©
                recent_sequence = decision_sequence[-seq_len:]
                input_tensor = self.preprocess_sequence_data(recent_sequence, seq_len)
            
            # ëª¨ë¸ ì¶”ë¡  (ì‹œí€€ìŠ¤ í˜•íƒœ)
            decisions, profit_pred, timeframe_features = self.model(input_tensor)
            
            # ì˜ì‚¬ê²°ì • ê²°ê³¼ ìƒì„± (ë‹¨ìˆœí™”)
            result = {
                'action': self._interpret_action(decisions['action']),
                'confidence': float(decisions['confidence'].item()),
                'profit': float(profit_pred.item()),
                'sequence_length': seq_len,
                'timestamp': datetime.now().isoformat(),
                'model_version': 'MultiTimeframeTransformer_v2.0_Sequence'
            }
            
            # íˆìŠ¤í† ë¦¬ì— ì €ì¥
            self.decision_history.append({
                'input': decision_sequence[-1] if decision_sequence else {},  # ë§ˆì§€ë§‰ ë°ì´í„°ë§Œ ì €ì¥
                'output': result,
                'timestamp': datetime.now()
            })
            
            self.total_decisions += 1
            
            return result
    
    def _interpret_action(self, action_logits: torch.Tensor) -> str:
        """ì•¡ì…˜ ë¡œì§“ì„ í•´ì„ (í™•ë¥  ë¶„í¬ í™•ì¸ í¬í•¨)"""
        # ì†Œí”„íŠ¸ë§¥ìŠ¤ ì ìš©í•˜ì—¬ í™•ë¥ ë¡œ ë³€í™˜
        probabilities = torch.softmax(action_logits, dim=-1)
        action_idx = torch.argmax(action_logits).item()
        actions = ['HOLD', 'BUY', 'SELL']
        
        # ë””ë²„ê¹…: ì•¡ì…˜ ë¶„í¬ ì¶œë ¥
        probs = probabilities.cpu().numpy()
        print(f"    ì•¡ì…˜ í™•ë¥ : HOLD={probs[0].item():.3f}, BUY={probs[1].item():.3f}, SELL={probs[2].item():.3f}")
        
        return actions[action_idx]
    
    
    def train_on_batch(self, batch_data: List[Dict], batch_labels: List[Dict]) -> float:
        """ë‹¨ì¼ ë°ì´í„° ë°°ì¹˜ í•™ìŠµ (í˜¸í™˜ì„± ìœ ì§€)"""
        self.model.train()
        
        # ë°°ì¹˜ ì „ì²˜ë¦¬
        inputs = []
        targets = []
        
        for data, labels in zip(batch_data, batch_labels):
            input_tensor = self.preprocess_decision_data(data)
            inputs.append(input_tensor)
            
            # íƒ€ê²Ÿ ìƒì„± (ë‹¨ìˆœí™”)
            target = {
                'action': torch.tensor([labels.get('action', 0)], dtype=torch.long).to(self.device),
                'confidence': torch.tensor([labels.get('confidence', 0.5)], dtype=torch.float).to(self.device),
                'profit': torch.tensor([labels.get('profit', 0.0)], dtype=torch.float).to(self.device)
            }
            targets.append(target)
        
        # ë°°ì¹˜ ê²°í•©
        batch_input = torch.cat(inputs, dim=0)
        
        # ìˆœì „íŒŒ
        decisions, profit_pred, _ = self.model(batch_input)
        
        # ì†ì‹¤ ê³„ì‚° (ê°€ì¤‘ì¹˜ ì ìš©)
        total_loss = 0.0
        
        # ê°€ì¤‘ì¹˜ ì„¤ì •
        profit_loss_weight = LOSS_PROFIT_WEIGHT
        action_loss_weight = LOSS_ACTION_WEIGHT
        other_loss_weight = LOSS_OTHER_WEIGHT
        
        # ì•¡ì…˜ ë¶„ë¥˜ ì†ì‹¤
        action_targets = torch.cat([t['action'] for t in targets])
        action_loss = F.cross_entropy(decisions['action'], action_targets)
        total_loss += action_loss * action_loss_weight
        
        # íšŒê·€ ì†ì‹¤ë“¤ (ë‹¨ìˆœí™”)
        for key in ['confidence']:
            pred = decisions[key].squeeze()
            target = torch.cat([t[key] for t in targets])
            loss = F.mse_loss(pred, target)
            total_loss += loss * other_loss_weight
        
        # ìˆ˜ìµë¥  ì˜ˆì¸¡ ì†ì‹¤ (ê°€ì¥ ì¤‘ìš”)
        profit_targets = torch.cat([t['profit'] for t in targets])
        profit_loss = F.mse_loss(profit_pred.squeeze(), profit_targets)
        total_loss += profit_loss * profit_loss_weight
        
        # ì—­ì „íŒŒ
        self.optimizer.zero_grad()
        total_loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), GRADIENT_CLIP_NORM)
        
        self.optimizer.step()
        
        return float(total_loss.item())
    
    def train_on_sequence_batch(self, batch_sequences: List[List[Dict]], batch_labels: List[Dict], seq_len: int = SEQUENCE_LENGTH) -> float:
        """ì‹œí€€ìŠ¤ ë°°ì¹˜ í•™ìŠµ (Transformerì˜ ì§„ì •í•œ ì¥ì  í™œìš©)"""
        self.model.train()
        
        # ë°°ì¹˜ ì „ì²˜ë¦¬
        inputs = []
        targets = []
        
        for sequence, labels in zip(batch_sequences, batch_labels):
            # âœ… ì‹œí€€ìŠ¤ë¥¼ ì§ì ‘ í…ì„œë¡œ ë³€í™˜ (preprocess_sequence_data ì‚¬ìš© ì•ˆ í•¨)
            if len(sequence) < seq_len:
                # íŒ¨ë”©
                padded_sequence = sequence + [sequence[-1]] * (seq_len - len(sequence))
            else:
                # ë§ˆì§€ë§‰ seq_lenê°œë§Œ ì‚¬ìš©
                padded_sequence = sequence[-seq_len:]
            
            # âœ… ì§ì ‘ íŠ¹ì„± ì¶”ì¶œ
            sequence_features = []
            for data_point in padded_sequence:
                features = self._extract_features(data_point)
                sequence_features.append(features)
            
            # [seq_len, feature_dim] â†’ [1, seq_len, feature_dim]
            input_tensor = torch.FloatTensor(sequence_features).unsqueeze(0).to(self.device)
            inputs.append(input_tensor)
            
            # íƒ€ê²Ÿ ìƒì„± (ë‹¨ìˆœí™”)
            target = {
                'action': torch.tensor([labels.get('action', 0)], dtype=torch.long).to(self.device),
                'confidence': torch.tensor([labels.get('confidence', 0.5)], dtype=torch.float).to(self.device),
                'profit': torch.tensor([labels.get('profit', 0.0)], dtype=torch.float).to(self.device)
            }
            targets.append(target)
        
        # ë°°ì¹˜ ê²°í•©
        batch_input = torch.cat(inputs, dim=0)
        
        # ìˆœì „íŒŒ
        decisions, profit_pred, _ = self.model(batch_input)
        
        # ì†ì‹¤ ê³„ì‚° (ê°€ì¤‘ì¹˜ ì ìš©)
        total_loss = 0.0
        
        # ê°€ì¤‘ì¹˜ ì„¤ì •
        profit_loss_weight = LOSS_PROFIT_WEIGHT
        action_loss_weight = LOSS_ACTION_WEIGHT
        other_loss_weight = LOSS_OTHER_WEIGHT
        
        # ì•¡ì…˜ ë¶„ë¥˜ ì†ì‹¤
        action_targets = torch.cat([t['action'] for t in targets])
        action_loss = F.cross_entropy(decisions['action'], action_targets)
        total_loss += action_loss * action_loss_weight
        
        # íšŒê·€ ì†ì‹¤ë“¤ (ë‹¨ìˆœí™”)
        for key in ['confidence']:
            pred = decisions[key].squeeze()
            target = torch.cat([t[key] for t in targets])
            loss = F.mse_loss(pred, target)
            total_loss += loss * other_loss_weight
        
        # ìˆ˜ìµë¥  ì˜ˆì¸¡ ì†ì‹¤ (ê°€ì¥ ì¤‘ìš”)
        profit_targets = torch.cat([t['profit'] for t in targets])
        profit_loss = F.mse_loss(profit_pred.squeeze(), profit_targets)
        total_loss += profit_loss * profit_loss_weight
        
        # ì—­ì „íŒŒ
        self.optimizer.zero_grad()
        total_loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), GRADIENT_CLIP_NORM)
        
        self.optimizer.step()
        
        return float(total_loss.item())
    
    def save_model(self, filepath: str) -> bool:
        """ëª¨ë¸ ì €ì¥"""
        try:
            os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)
            
            save_dict = {
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scheduler_state_dict': self.scheduler.state_dict(),
                'total_decisions': self.total_decisions,
                'correct_decisions': self.correct_decisions,
                'total_profit': self.total_profit,
                'decision_history': self.decision_history[-1000:],  # ìµœê·¼ 1000ê°œë§Œ ì €ì¥
                'model_config': {
                    'input_size': self.model.input_size,
                    'd_model': self.model.d_model,
                    'nhead': self.model.nhead,
                    'num_layers': self.model.num_layers
                }
            }
            
            torch.save(save_dict, filepath)
            print(f"Multi-Timeframe Transformer ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")
            return True
            
        except Exception as e:
            print(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def load_model(self, filepath: str) -> bool:
        """ëª¨ë¸ ë¡œë“œ"""
        if not os.path.exists(filepath):
            print(f"ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {filepath}")
            return False
        
        try:
            checkpoint = torch.load(filepath, map_location=self.device)
            
            # ëª¨ë¸ ìƒíƒœ ë¡œë“œ
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            
            # í†µê³„ ë¡œë“œ
            self.total_decisions = checkpoint.get('total_decisions', 0)
            self.correct_decisions = checkpoint.get('correct_decisions', 0)
            self.total_profit = checkpoint.get('total_profit', 0.0)
            self.decision_history = checkpoint.get('decision_history', [])
            
            print(f"Multi-Timeframe Transformer ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {filepath}")
            print(f"   ì´ ì˜ì‚¬ê²°ì •: {self.total_decisions}")
            print(f"   ì •í™•ë„: {self.correct_decisions/max(self.total_decisions, 1):.3f}")
            print(f"   ì´ ìˆ˜ìµ: {self.total_profit:.3f}")
            
            return True
            
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def get_performance_stats(self) -> Dict:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        accuracy = self.correct_decisions / max(self.total_decisions, 1)
        avg_profit = self.total_profit / max(self.total_decisions, 1)
        
        return {
            'total_decisions': self.total_decisions,
            'correct_decisions': self.correct_decisions,
            'accuracy': accuracy,
            'total_profit': self.total_profit,
            'avg_profit_per_decision': avg_profit,
            'recent_decisions': len(self.decision_history)
        }

def create_sample_decision_data() -> Dict:
    """ìƒ˜í”Œ Decision ë°ì´í„° ìƒì„± (58ì°¨ì›)"""
    actions = ['HOLD', 'BUY', 'SELL']
    
    data = {}
    
    # 1. ì „ëµ Action (16ê°œ)
    strategy_actions = [
        'bollinger_squeeze_action', 'zscore_mean_reversion_action', 'vpvr_action', 
        'ema_confluence_action', 'ichimoku_action', 'htf_trend_action', 
        'funding_rate_action', 'orderflow_cvd_action', 'support_resistance_action', 
        'vwap_pinball_action', 'multi_timeframe_action', 'session_action', 
        'vol_spike_action', 'vpvr_micro_action', 'oi_delta_action', 'liquidity_grab_action'
    ]
    
    for action_col in strategy_actions:
        data[action_col] = random.choice(actions)
    
    # 2. ì „ëµ Score (16ê°œ)
    strategy_scores = [
        'bollinger_squeeze_score', 'session_score', 'vpvr_score', 'oi_delta_score',
        'ema_confluence_score', 'multi_timeframe_score', 'vol_spike_score', 
        'vpvr_micro_score', 'htf_trend_score', 'liquidity_grab_score', 
        'ichimoku_score', 'funding_rate_score', 'support_resistance_score', 
        'zscore_mean_reversion_score', 'vwap_pinball_score', 'orderflow_cvd_score'
    ]
    
    for score_col in strategy_scores:
        data[score_col] = random.uniform(0.0, 1.0)
    
    # 3. ì „ëµ Confidence (1ê°œ)
    data['vpvr_confidence'] = random.uniform(0.0, 1.0)
    
    # 4. ì „ëµ Entry (4ê°œ)
    strategy_entries = ['vpvr_micro_entry', 'vwap_pinball_entry', 'session_entry', 'vpvr_entry']
    for entry_col in strategy_entries:
        data[entry_col] = random.uniform(2000, 3000)
    
    # 5. ì „ëµ Stop (4ê°œ)
    strategy_stops = ['vpvr_stop', 'session_stop', 'vpvr_micro_stop', 'vwap_pinball_stop']
    for stop_col in strategy_stops:
        data[stop_col] = random.uniform(2000, 3000)
    
    # 6. Indicator ì •ë³´ (10ê°œ)
    indicator_fields = [
        'indicator_prev_day_high', 'indicator_poc', 'indicator_hvn', 'indicator_vwap_std',
        'indicator_opening_range_high', 'indicator_lvn', 'indicator_opening_range_low',
        'indicator_vwap', 'indicator_prev_day_low', 'indicator_atr'
    ]
    
    for field in indicator_fields:
        if 'atr' in field or 'std' in field:
            data[field] = random.uniform(1, 50)
        else:
            data[field] = random.uniform(2000, 3000)
    
    # 7. OHLC ë°ì´í„° (6ê°œ)
    base_price = random.uniform(2000, 3000)
    data['open'] = base_price
    data['high'] = base_price + random.uniform(0, 50)
    data['low'] = base_price - random.uniform(0, 50)
    data['close'] = base_price + random.uniform(-20, 20)
    data['volume'] = random.uniform(1000, 10000)
    data['quote_volume'] = random.uniform(1000000, 10000000)
    
    # 8. Timestamp (1ê°œ)
    data['timestamp'] = int(datetime.now().timestamp() * 1000)
    
    return data

class DecisionDataLoader:
    """Decision ë°ì´í„° ë¡œë”"""
    
    @staticmethod
    def load_decision_data(file_path: str = 'agent/decisions_data.parquet') -> Optional[List[Dict]]:
        """Decision ë°ì´í„° ë¡œë“œ"""
        if not os.path.exists(file_path):
            print(f"Decision ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return None
        
        try:
            print(f"Decision ë°ì´í„° ë¡œë“œ ì¤‘: {file_path}")
            df = pd.read_parquet(file_path)
            print(f"Decision ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df):,}ê°œ ë ˆì½”ë“œ")
            
            # DataFrameì„ Dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë©”ëª¨ë¦¬ ìµœì í™”)
            decision_data = []
            for idx, row in df.iterrows():
                decision_dict = {}
                for col, value in row.items():
                    if pd.notna(value):
                        # ë°ì´í„° íƒ€ì… ìµœì í™”
                        if 'action' in col:
                            # ì•¡ì…˜ì€ ë¬¸ìì—´ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€
                            decision_dict[col] = str(value) if value else 'HOLD'
                        elif 'score' in col or 'value' in col:
                            decision_dict[col] = float(value) if value else 0.0
                        elif 'confidence' in col:
                            # confidenceëŠ” ë¬¸ìì—´ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€
                            decision_dict[col] = str(value) if value else 'LOW'
                        elif 'count' in col or 'used' in col:
                            decision_dict[col] = int(value) if value else 0
                        elif 'timestamp' in col:
                            # timestampëŠ” ë¬¸ìì—´ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€
                            decision_dict[col] = str(value) if value else ''
                        else:
                            decision_dict[col] = float(value) if value else 0.0
                    else:
                        # ê¸°ë³¸ê°’ ì„¤ì • (ìµœì í™”ëœ íƒ€ì…)
                        if 'action' in col:
                            decision_dict[col] = 'HOLD'
                        elif 'score' in col or 'value' in col:
                            decision_dict[col] = 0.0
                        elif 'confidence' in col:
                            decision_dict[col] = 'LOW'
                        elif 'count' in col or 'used' in col:
                            decision_dict[col] = 0
                        elif 'timestamp' in col:
                            decision_dict[col] = ''
                        else:
                            decision_dict[col] = 0.0
                
                decision_data.append(decision_dict)
                
                if (idx + 1) % 10000 == 0:
                    print(f"  ë³€í™˜ ì§„í–‰: {idx + 1:,}/{len(df):,}")
            
            print(f"Decision ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(decision_data):,}ê°œ")
            return decision_data
            
        except Exception as e:
            print(f"Decision ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    @staticmethod
    def create_realistic_training_labels(decision_data: List[Dict], lookback_steps: int = SEQUENCE_LENGTH):
        """ê³ ê¸‰ ë¼ë²¨ë§ ë¡œì§ - ì‹œì¥ ìƒí™©ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•œ ë¼ë²¨ ìƒì„±"""
        labels = []
        
        for i in range(lookback_steps, len(decision_data) - LABEL_LOOKAHEAD_STEPS):
            label = {
                'action': 0,        # int (0, 1, 2)
                'confidence': 0.5,  # float (0.0-1.0)
                'profit': 0.0       # float (ìˆ˜ìµë¥ )
            }
            
            # í˜„ì¬ ë°ì´í„° ì¶”ì¶œ
            current_data = decision_data[i]
            current_price = current_data.get('close', 0)
            
            # ë¯¸ë˜ ë°ì´í„° ì¶”ì¶œ
            future_prices = []
            future_volumes = []
            for j in range(1, LABEL_LOOKAHEAD_STEPS + 1):
                future_data = decision_data[i + j]
                future_prices.append(future_data.get('close', 0))
                future_volumes.append(future_data.get('volume', 0))
            
            if current_price > 0 and all(p > 0 for p in future_prices):
                # 1. ê¸°ë³¸ ìˆ˜ìµë¥  ê³„ì‚°
                future_returns = [(p - current_price) / current_price for p in future_prices]
                max_return = max(future_returns)
                min_return = min(future_returns)
                avg_return = np.mean(future_returns)
                
                # 2. ì‹œì¥ ìƒí™© ë¶„ì„
                market_context = DecisionDataLoader._analyze_market_context(
                    decision_data, i, lookback_steps
                )
                
                # 3. ë™ì  ì„ê³„ê°’ ê³„ì‚°
                dynamic_threshold = DecisionDataLoader._calculate_dynamic_threshold(
                    market_context, current_data
                )
                
                # 4. ê³ ê¸‰ ë¼ë²¨ ìƒì„±
                label = DecisionDataLoader._generate_advanced_label(
                    max_return, min_return, avg_return, 
                    market_context, dynamic_threshold, future_volumes
                )
            
            labels.append(label)
        
        # ì•¡ì…˜ ë¶„í¬ í†µê³„ ì¶œë ¥
        action_counts = {'HOLD': 0, 'BUY': 0, 'SELL': 0}
        for label in labels:
            action_counts[['HOLD', 'BUY', 'SELL'][label['action']]] += 1
        
        total_labels = len(labels)
        print(f"  ğŸ“Š ë¼ë²¨ ì•¡ì…˜ ë¶„í¬:")
        print(f"    HOLD: {action_counts['HOLD']:,}ê°œ ({action_counts['HOLD']/total_labels*100:.1f}%)")
        print(f"    BUY:  {action_counts['BUY']:,}ê°œ ({action_counts['BUY']/total_labels*100:.1f}%)")
        print(f"    SELL: {action_counts['SELL']:,}ê°œ ({action_counts['SELL']/total_labels*100:.1f}%)")
        
        return labels
    
    @staticmethod
    def _analyze_market_context(decision_data: List[Dict], current_idx: int, lookback_steps: int) -> Dict:
        """ì‹œì¥ ìƒí™© ì¢…í•© ë¶„ì„"""
        context = {
            'volatility': 0.0,
            'trend_strength': 0.0,
            'volume_trend': 0.0,
            'price_momentum': 0.0,
            'market_regime': 'normal'  # normal, trending, volatile, consolidation
        }
        
        # ê³¼ê±° ë°ì´í„° ì¶”ì¶œ
        start_idx = max(0, current_idx - lookback_steps)
        historical_data = decision_data[start_idx:current_idx]
        
        if len(historical_data) < 5:
            return context
        
        # ê°€ê²© ë°ì´í„° ì¶”ì¶œ
        prices = [d.get('close', 0) for d in historical_data if d.get('close', 0) > 0]
        volumes = [d.get('volume', 0) for d in historical_data if d.get('volume', 0) > 0]
        
        if len(prices) < 5:
            return context
        
        # 1. ë³€ë™ì„± ê³„ì‚° (ATR ê¸°ë°˜)
        price_changes = [abs(prices[i] - prices[i-1]) / prices[i-1] for i in range(1, len(prices))]
        context['volatility'] = np.mean(price_changes) if price_changes else 0.0
        
        # 2. íŠ¸ë Œë“œ ê°•ë„ ê³„ì‚° (ì„ í˜• íšŒê·€ ê¸°ìš¸ê¸°)
        x = np.arange(len(prices))
        y = np.array(prices)
        if len(x) > 1:
            slope = np.polyfit(x, y, 1)[0]
            context['trend_strength'] = abs(slope) / prices[-1] if prices[-1] > 0 else 0.0
        
        # 3. ê±°ë˜ëŸ‰ íŠ¸ë Œë“œ
        if len(volumes) > 1:
            volume_changes = [volumes[i] - volumes[i-1] for i in range(1, len(volumes))]
            context['volume_trend'] = np.mean(volume_changes) / volumes[-1] if volumes[-1] > 0 else 0.0
        
        # 4. ê°€ê²© ëª¨ë©˜í…€ (ìµœê·¼ vs ê³¼ê±°)
        if len(prices) >= 10:
            recent_avg = np.mean(prices[-5:])
            past_avg = np.mean(prices[:5])
            context['price_momentum'] = (recent_avg - past_avg) / past_avg if past_avg > 0 else 0.0
        
        # 5. ì‹œì¥ ë ˆì§ ë¶„ë¥˜
        if context['volatility'] > 0.02:  # 2% ì´ìƒ ë³€ë™ì„±
            context['market_regime'] = 'volatile'
        elif abs(context['trend_strength']) > 0.001:  # ê°•í•œ íŠ¸ë Œë“œ
            context['market_regime'] = 'trending'
        elif context['volatility'] < 0.005:  # ë‚®ì€ ë³€ë™ì„±
            context['market_regime'] = 'consolidation'
        
        return context
    
    @staticmethod
    def _calculate_dynamic_threshold(market_context: Dict, current_data: Dict) -> float:
        """ì‹œì¥ ìƒí™©ì— ë”°ë¥¸ ë™ì  ì„ê³„ê°’ ê³„ì‚°"""
        base_threshold = LABEL_PROFIT_THRESHOLD
        
        # ë³€ë™ì„± ì¡°ì •
        volatility_factor = 1.0 + (market_context['volatility'] * LABEL_VOLATILITY_FACTOR)
        
        # íŠ¸ë Œë“œ ì¡°ì •
        trend_factor = 1.0 + (abs(market_context['trend_strength']) * LABEL_TREND_FACTOR)
        
        # ê±°ë˜ëŸ‰ ì¡°ì •
        volume_factor = 1.0 + (abs(market_context['volume_trend']) * LABEL_VOLUME_FACTOR)
        
        # ì‹œì¥ ë ˆì§ë³„ ì¡°ì •
        regime_multiplier = {
            'volatile': 1.5,      # ë³€ë™ì„± ì‹œì¥: ì„ê³„ê°’ ë†’ì„
            'trending': 0.8,      # íŠ¸ë Œë“œ ì‹œì¥: ì„ê³„ê°’ ë‚®ì¶¤
            'consolidation': 1.2, # íš¡ë³´ ì‹œì¥: ì„ê³„ê°’ ì•½ê°„ ë†’ì„
            'normal': 1.0         # ì •ìƒ ì‹œì¥: ê¸°ë³¸ê°’
        }
        
        regime_factor = regime_multiplier.get(market_context['market_regime'], 1.0)
        
        # ìµœì¢… ë™ì  ì„ê³„ê°’
        dynamic_threshold = base_threshold * volatility_factor * trend_factor * volume_factor * regime_factor
        
        # ìµœì†Œ/ìµœëŒ€ ì œí•œ
        return max(LABEL_PROFIT_THRESHOLD, min(0.05, dynamic_threshold))  # 0.5% ~ 5% ë²”ìœ„
    
    @staticmethod
    def _generate_advanced_label(max_return: float, min_return: float, avg_return: float,
                                market_context: Dict, dynamic_threshold: float, 
                                future_volumes: List[float]) -> Dict:
        """ê³ ê¸‰ ë¼ë²¨ ìƒì„± ë¡œì§"""
        label = {
            'action': 0,                        # int (0, 1, 2)
            'confidence': LABEL_MIN_CONFIDENCE,  # float (0.0-1.0)
            'profit': 0.0                       # float (ìˆ˜ìµë¥ )
        }
        
        # ê±°ë˜ëŸ‰ ê°€ì¤‘ì¹˜ ê³„ì‚°
        volume_weight = 1.0
        if future_volumes:
            avg_volume = np.mean(future_volumes)
            if avg_volume > 0:
                # ê±°ë˜ëŸ‰ì´ í‰ê· ë³´ë‹¤ ë†’ìœ¼ë©´ ì‹ ë¢°ë„ ì¦ê°€
                volume_weight = min(1.5, avg_volume / (avg_volume * 0.8))
        
        # 1. BUY ì¡°ê±´ (ìƒìŠ¹ ê°€ëŠ¥ì„±)
        if max_return > dynamic_threshold:
            label['action'] = 1  # BUY
            
            # ìˆ˜ìµë¥  ê¸°ë°˜ ì‹ ë¢°ë„
            return_confidence = min(0.8, 0.4 + max_return * 15)
            
            # ì‹œì¥ ìƒí™© ê¸°ë°˜ ì‹ ë¢°ë„ ì¡°ì •
            market_confidence = 1.0
            if market_context['trend_strength'] > 0:  # ìƒìŠ¹ íŠ¸ë Œë“œ
                market_confidence += 0.1
            if market_context['price_momentum'] > 0:  # ìƒìŠ¹ ëª¨ë©˜í…€
                market_confidence += 0.1
            if market_context['market_regime'] == 'trending':  # íŠ¸ë Œë“œ ì‹œì¥
                market_confidence += 0.1
            
            # ìµœì¢… ì‹ ë¢°ë„
            label['confidence'] = min(LABEL_MAX_CONFIDENCE, 
                                    return_confidence * market_confidence * volume_weight)
            label['profit'] = max_return
            
        # 2. SELL ì¡°ê±´ (í•˜ë½ ê°€ëŠ¥ì„±)
        elif min_return < -dynamic_threshold:
            label['action'] = 2  # SELL
            
            # ìˆ˜ìµë¥  ê¸°ë°˜ ì‹ ë¢°ë„
            return_confidence = min(0.8, 0.4 + abs(min_return) * 15)
            
            # ì‹œì¥ ìƒí™© ê¸°ë°˜ ì‹ ë¢°ë„ ì¡°ì •
            market_confidence = 1.0
            if market_context['trend_strength'] < 0:  # í•˜ë½ íŠ¸ë Œë“œ
                market_confidence += 0.1
            if market_context['price_momentum'] < 0:  # í•˜ë½ ëª¨ë©˜í…€
                market_confidence += 0.1
            if market_context['market_regime'] == 'trending':  # íŠ¸ë Œë“œ ì‹œì¥
                market_confidence += 0.1
            
            # ìµœì¢… ì‹ ë¢°ë„
            label['confidence'] = min(LABEL_MAX_CONFIDENCE, 
                                    return_confidence * market_confidence * volume_weight)
            label['profit'] = abs(min_return)
            
        # 3. HOLD ì¡°ê±´ (í° ì›€ì§ì„ ì—†ìŒ)
        else:
            label['action'] = 0  # HOLD
            label['profit'] = 0.0
            
            # HOLD ì‹ ë¢°ë„ëŠ” ì‹œì¥ ì•ˆì •ì„±ì— ë”°ë¼ ê²°ì •
            stability_score = 1.0 - market_context['volatility'] * 10  # ë³€ë™ì„±ì´ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì‹ ë¢°ë„
            label['confidence'] = max(LABEL_MIN_CONFIDENCE, 
                                    min(0.7, stability_score * volume_weight))
        
        return label

class MultiTimeframeTrainer:
    """Multi-Timeframe Transformer í›ˆë ¨ í´ë˜ìŠ¤"""
    
    def __init__(self, engine: MultiTimeframeDecisionEngine):
        self.engine = engine
        self.training_history = []
    
    def train_on_sequence_data(self, 
                                decision_data: List[Dict], 
                                seq_len: int = SEQUENCE_LENGTH,
                                batch_size: int = TRAINING_BATCH_SIZE,
                                epochs: int = TRAINING_EPOCHS,
                                validation_split: float = TRAINING_VALIDATION_SPLIT) -> Dict:
        """ì‹œí€€ìŠ¤ ë°ì´í„°ë¡œ í›ˆë ¨ (Transformerì˜ ì§„ì •í•œ ì¥ì  í™œìš©)"""
        print(f"Multi-Timeframe Transformer ì‹œí€€ìŠ¤ í›ˆë ¨ ì‹œì‘")
        print(f"  ë°ì´í„° í¬ê¸°: {len(decision_data):,}ê°œ")
        print(f"  ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len}")
        print(f"  ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"  ì—í¬í¬: {epochs}")
        print(f"  ê²€ì¦ ë¹„ìœ¨: {validation_split:.1%}")
        
        # ë°ì´í„° ë¶„í• 
        split_idx = int(len(decision_data) * (1 - validation_split))
        train_data_raw = decision_data[:split_idx]
        val_data_raw = decision_data[split_idx:]
        
        # ì •ê·œí™”
        normalizer = DataNormalizer()
        train_data_raw = normalizer.fit_transform(train_data_raw)
        val_data_raw = normalizer.transform(val_data_raw)
        
        # ë¼ë²¨ ìƒì„±
        train_realistic_labels = DecisionDataLoader.create_realistic_training_labels(
            train_data_raw, lookback_steps=seq_len
        )
        val_realistic_labels = DecisionDataLoader.create_realistic_training_labels(
            val_data_raw, lookback_steps=seq_len
        )
        
        print(f"  í›ˆë ¨ ë¼ë²¨: {len(train_realistic_labels):,}ê°œ")
        print(f"  ê²€ì¦ ë¼ë²¨: {len(val_realistic_labels):,}ê°œ")
        
        # ì‹œí€€ìŠ¤ ìƒì„± (ìˆ˜ì •ë¨)
        train_sequences = []
        train_sequence_labels = []
        
        for i in range(len(train_realistic_labels)):
            sequence = train_data_raw[i:i + seq_len]
            label = train_realistic_labels[i]
            
            train_sequences.append(sequence)
            train_sequence_labels.append(label)
        
        val_sequences = []
        val_sequence_labels = []
        
        for i in range(len(val_realistic_labels)):
            sequence = val_data_raw[i:i + seq_len]
            label = val_realistic_labels[i]
            
            val_sequences.append(sequence)
            val_sequence_labels.append(label)
        
        print(f"  í›ˆë ¨ ì‹œí€€ìŠ¤: {len(train_sequences):,}ê°œ")
        print(f"  ê²€ì¦ ì‹œí€€ìŠ¤: {len(val_sequences):,}ê°œ")
        
        # í›ˆë ¨ ë£¨í”„
        best_val_loss = float('inf')
        patience = TRAINING_PATIENCE
        patience_counter = 0
        
        for epoch in range(epochs):
            # í›ˆë ¨
            train_loss = self._train_sequence_epoch(train_sequences, train_sequence_labels, seq_len, batch_size)
            
            # ê²€ì¦
            val_loss = self._validate_sequence_epoch(val_sequences, val_sequence_labels, seq_len, batch_size)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            self.engine.scheduler.step(val_loss)
            
            # íˆìŠ¤í† ë¦¬ ì €ì¥
            epoch_stats = {
                'epoch': epoch + 1,
                'train_loss': train_loss,
                'val_loss': val_loss,
                'lr': self.engine.optimizer.param_groups[0]['lr']
            }
            self.training_history.append(epoch_stats)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            print(f"Epoch {epoch+1:3d}/{epochs} | "
                  f"Train Loss: {train_loss:.4f} | "
                  f"Val Loss: {val_loss:.4f} | "
                  f"LR: {self.engine.optimizer.param_groups[0]['lr']:.2e}")
            
            # ì¡°ê¸° ì¢…ë£Œ
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # ìµœê³  ëª¨ë¸ ì €ì¥
                self.engine.save_model(MODEL_SEQUENCE_SAVE_PATH)
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"ì¡°ê¸° ì¢…ë£Œ: {patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ")
                    break
        
        print(f"ì‹œí€€ìŠ¤ í›ˆë ¨ ì™„ë£Œ! ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}")
        
        return {
            'best_val_loss': best_val_loss,
            'total_epochs': len(self.training_history),
            'training_history': self.training_history,
            'sequence_length': seq_len
        }
    
    def _train_sequence_epoch(self, sequences: List[List[Dict]], labels: List[Dict], seq_len: int, batch_size: int) -> float:
        """ì‹œí€€ìŠ¤ ì—í¬í¬ í›ˆë ¨"""
        self.engine.model.train()
        
        total_loss = 0.0
        num_batches = 0
        
        # ë°ì´í„° ì…”í”Œ
        indices = list(range(len(sequences)))
        random.shuffle(indices)
        total_batches = (len(sequences) + batch_size - 1) // batch_size
        
        for i in range(0, len(sequences), batch_size):
            batch_indices = indices[i:i + batch_size]
            batch_sequences = [sequences[idx] for idx in batch_indices]
            batch_labels = [labels[idx] for idx in batch_indices]
            
            if i % 100 == 0:
                print(f"    ë°°ì¹˜ {num_batches}/{total_batches} ì²˜ë¦¬ ì¤‘... ({num_batches/total_batches*100:.1f}%)")
            
            loss = self.engine.train_on_sequence_batch(batch_sequences, batch_labels, seq_len)
            total_loss += loss
            num_batches += 1
        
        return total_loss / num_batches if num_batches > 0 else 0.0
    
    def _validate_sequence_epoch(self, sequences: List[List[Dict]], labels: List[Dict], seq_len: int, batch_size: int) -> float:
        """ì‹œí€€ìŠ¤ ì—í¬í¬ ê²€ì¦"""
        self.engine.model.eval()
        
        total_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for i in range(0, len(sequences), batch_size):
                batch_sequences = sequences[i:i + batch_size]
                batch_labels = labels[i:i + batch_size]
                
                # ê²€ì¦ìš© ì†ì‹¤ ê³„ì‚°
                loss = self._compute_sequence_validation_loss(batch_sequences, batch_labels, seq_len)
                total_loss += loss
                num_batches += 1
        
        return total_loss / num_batches if num_batches > 0 else 0.0
    
    def _compute_sequence_validation_loss(self, batch_sequences: List[List[Dict]], batch_labels: List[Dict], seq_len: int) -> float:
        """ì‹œí€€ìŠ¤ ê²€ì¦ ì†ì‹¤ ê³„ì‚°"""
        # ë°°ì¹˜ ì „ì²˜ë¦¬
        inputs = []
        targets = []
        
        for sequence, labels in zip(batch_sequences, batch_labels):
            # ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì •
            if len(sequence) < seq_len:
                padded_sequence = sequence + [sequence[-1]] * (seq_len - len(sequence))
            else:
                padded_sequence = sequence[-seq_len:]
            
            # ì§ì ‘ íŠ¹ì„± ì¶”ì¶œ
            sequence_features = []
            for data_point in padded_sequence:
                features = self.engine._extract_features(data_point)
                sequence_features.append(features)
            
            # [seq_len, feature_dim] â†’ [1, seq_len, feature_dim]
            input_tensor = torch.FloatTensor(sequence_features).unsqueeze(0).to(self.engine.device)
            inputs.append(input_tensor)
            
            # íƒ€ê²Ÿ ìƒì„±
            target = {
                'action': torch.tensor([labels.get('action', 0)], dtype=torch.long).to(self.engine.device),
                'confidence': torch.tensor([labels.get('confidence', 0.5)], dtype=torch.float).to(self.engine.device),
                'profit': torch.tensor([labels.get('profit', 0.0)], dtype=torch.float).to(self.engine.device)
            }
            targets.append(target)
        
        # ë°°ì¹˜ ê²°í•©
        batch_input = torch.cat(inputs, dim=0)
        
        # ìˆœì „íŒŒ
        decisions, profit_pred, _ = self.engine.model(batch_input)
        
        # ì†ì‹¤ ê³„ì‚°
        total_loss = 0.0
        
        # ì•¡ì…˜ ë¶„ë¥˜ ì†ì‹¤
        action_targets = torch.cat([t['action'] for t in targets])
        action_loss = F.cross_entropy(decisions['action'], action_targets)
        total_loss += action_loss
        
        # íšŒê·€ ì†ì‹¤ë“¤ (ë‹¨ìˆœí™”)
        for key in ['confidence']:
            pred = decisions[key].squeeze()
            target = torch.cat([t[key] for t in targets])
            loss = F.mse_loss(pred, target)
            total_loss += loss
        
        # ìˆ˜ìµë¥  ì˜ˆì¸¡ ì†ì‹¤
        profit_targets = torch.cat([t['profit'] for t in targets])
        profit_loss = F.mse_loss(profit_pred.squeeze(), profit_targets)
        total_loss += profit_loss
        
        return float(total_loss.item())
    
    # def train_on_decision_data(self, 
    #                           decision_data: List[Dict], 
    #                           labels: List[Dict],
    #                           batch_size: int = TRAINING_BATCH_SIZE,
    #                           epochs: int = TRAINING_EPOCHS,
    #                           validation_split: float = TRAINING_VALIDATION_SPLIT) -> Dict:
    #     """Decision ë°ì´í„°ë¡œ í›ˆë ¨"""
    #     print(f"Multi-Timeframe Transformer í›ˆë ¨ ì‹œì‘")
    #     print(f"  ë°ì´í„° í¬ê¸°: {len(decision_data):,}ê°œ")
    #     print(f"  ë°°ì¹˜ í¬ê¸°: {batch_size}")
    #     print(f"  ì—í¬í¬: {epochs}")
    #     print(f"  ê²€ì¦ ë¹„ìœ¨: {validation_split:.1%}")
        
    #     # ğŸ”¥ ë°ì´í„° ë¶„í•  ë¨¼ì € (ì •ê·œí™” ì „ì—!)
    #     split_idx = int(len(decision_data) * (1 - validation_split))
    #     train_data = decision_data[:split_idx]
    #     train_labels = labels[:split_idx]
    #     val_data = decision_data[split_idx:]
    #     val_labels = labels[split_idx:]
        
    #     print(f"  í›ˆë ¨ ë°ì´í„°: {len(train_data):,}ê°œ")
    #     print(f"  ê²€ì¦ ë°ì´í„°: {len(val_data):,}ê°œ")
        
    #     # ğŸ”¥ í›ˆë ¨ì…‹ìœ¼ë¡œë§Œ ì •ê·œí™” fit
    #     print("  í›ˆë ¨ì…‹ìœ¼ë¡œ ì •ê·œí™” fit ì¤‘...")
    #     normalizer = DataNormalizer()
    #     train_data = normalizer.fit_transform(train_data)
        
    #     # ğŸ”¥ ê²€ì¦ì…‹ì€ transformë§Œ (fit ì—†ì´!)
    #     print("  ê²€ì¦ì…‹ì— ì •ê·œí™” transform ì ìš© ì¤‘...")
    #     val_data = normalizer.transform(val_data)
        
    #     # í›ˆë ¨ ë£¨í”„
    #     best_val_loss = float('inf')
    #     patience = TRAINING_PATIENCE
    #     patience_counter = 0
        
    #     for epoch in range(epochs):
    #         # í›ˆë ¨
    #         train_loss = self._train_epoch(train_data, train_labels, batch_size)
            
    #         # ê²€ì¦
    #         val_loss = self._validate_epoch(val_data, val_labels, batch_size)
            
    #         # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
    #         self.engine.scheduler.step(val_loss)
            
    #         # íˆìŠ¤í† ë¦¬ ì €ì¥
    #         epoch_stats = {
    #             'epoch': epoch + 1,
    #             'train_loss': train_loss,
    #             'val_loss': val_loss,
    #             'lr': self.engine.optimizer.param_groups[0]['lr']
    #         }
    #         self.training_history.append(epoch_stats)
            
    #         # ì§„í–‰ ìƒí™© ì¶œë ¥
    #         print(f"Epoch {epoch+1:3d}/{epochs} | "
    #               f"Train Loss: {train_loss:.4f} | "
    #               f"Val Loss: {val_loss:.4f} | "
    #               f"LR: {self.engine.optimizer.param_groups[0]['lr']:.2e}")
            
    #         # ì¡°ê¸° ì¢…ë£Œ
    #         if val_loss < best_val_loss:
    #             best_val_loss = val_loss
    #             patience_counter = 0
    #             # ìµœê³  ëª¨ë¸ ì €ì¥
    #             self.engine.save_model(MODEL_SAVE_PATH)
    #         else:
    #             patience_counter += 1
    #             if patience_counter >= patience:
    #                 print(f"ì¡°ê¸° ì¢…ë£Œ: {patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ")
    #                 break
        
    #     print(f"í›ˆë ¨ ì™„ë£Œ! ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}")
        
    #     return {
    #         'best_val_loss': best_val_loss,
    #         'total_epochs': len(self.training_history),
    #         'training_history': self.training_history
    #     }
    
    def _train_epoch(self, data: List[Dict], labels: List[Dict], batch_size: int) -> float:
        """í•œ ì—í¬í¬ í›ˆë ¨"""
        self.engine.model.train()
        
        total_loss = 0.0
        num_batches = 0
        
        # ë°ì´í„° ì…”í”Œ
        indices = list(range(len(data)))
        random.shuffle(indices)
        
        for i in range(0, len(data), batch_size):
            batch_indices = indices[i:i + batch_size]
            batch_data = [data[idx] for idx in batch_indices]
            batch_labels = [labels[idx] for idx in batch_indices]
            
            loss = self.engine.train_on_batch(batch_data, batch_labels)
            total_loss += loss
            num_batches += 1
        
        return total_loss / num_batches if num_batches > 0 else 0.0
    
    def _validate_epoch(self, data: List[Dict], labels: List[Dict], batch_size: int) -> float:
        """í•œ ì—í¬í¬ ê²€ì¦"""
        self.engine.model.eval()
        
        total_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for i in range(0, len(data), batch_size):
                batch_data = data[i:i + batch_size]
                batch_labels = labels[i:i + batch_size]
                
                # ê²€ì¦ìš© ì†ì‹¤ ê³„ì‚° (í›ˆë ¨í•˜ì§€ ì•ŠìŒ)
                loss = self._compute_validation_loss(batch_data, batch_labels)
                total_loss += loss
                num_batches += 1
        
        return total_loss / num_batches if num_batches > 0 else 0.0
    
    def _compute_validation_loss(self, batch_data: List[Dict], batch_labels: List[Dict]) -> float:
        """ê²€ì¦ ì†ì‹¤ ê³„ì‚°"""
        # ë°°ì¹˜ ì „ì²˜ë¦¬
        inputs = []
        targets = []
        
        for data, labels in zip(batch_data, batch_labels):
            input_tensor = self.engine.preprocess_decision_data(data)
            inputs.append(input_tensor)
            
            target = {
                'action': torch.tensor([labels.get('action', 0)], dtype=torch.long).to(self.engine.device),
                'confidence': torch.tensor([labels.get('confidence', 0.5)], dtype=torch.float).to(self.engine.device),
                'profit': torch.tensor([labels.get('profit', 0.0)], dtype=torch.float).to(self.engine.device)
            }
            targets.append(target)
        
        # ë°°ì¹˜ ê²°í•©
        batch_input = torch.cat(inputs, dim=0)
        
        # ìˆœì „íŒŒ
        decisions, profit_pred, _ = self.engine.model(batch_input)
        
        # ì†ì‹¤ ê³„ì‚°
        total_loss = 0.0
        
        # ì•¡ì…˜ ë¶„ë¥˜ ì†ì‹¤
        action_targets = torch.cat([t['action'] for t in targets])
        action_loss = F.cross_entropy(decisions['action'], action_targets)
        total_loss += action_loss
        
        # íšŒê·€ ì†ì‹¤ë“¤ (ë‹¨ìˆœí™”)
        for key in ['confidence']:
            pred = decisions[key].squeeze()
            target = torch.cat([t[key] for t in targets])
            loss = F.mse_loss(pred, target)
            total_loss += loss
        
        # ìˆ˜ìµë¥  ì˜ˆì¸¡ ì†ì‹¤
        profit_targets = torch.cat([t['profit'] for t in targets])
        profit_loss = F.mse_loss(profit_pred.squeeze(), profit_targets)
        total_loss += profit_loss
        
        return float(total_loss.item())

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("Multi-Timeframe Transformer ë”¥ëŸ¬ë‹ ëª¨ë¸")
    print("=" * 60)
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    engine = MultiTimeframeDecisionEngine(
        input_size=MODEL_INPUT_SIZE,
        d_model=MODEL_D_MODEL,
        nhead=MODEL_NHEAD,
        num_layers=MODEL_NUM_LAYERS
    )
    
    # 1. Decision ë°ì´í„° ë¡œë“œ (í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì œí•œ)
    print("\n1ï¸âƒ£ Decision ë°ì´í„° ë¡œë“œ...")
    decision_data = DecisionDataLoader.load_decision_data(DATA_FILE_PATH)
    
    # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë°ì´í„° ì œí•œ
    if len(decision_data) > DATA_TEST_LIMIT:
        decision_data = decision_data[:DATA_TEST_LIMIT]
        print(f"   ğŸ§ª í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë°ì´í„°ë¥¼ {DATA_TEST_LIMIT:,}ê°œë¡œ ì œí•œí–ˆìŠµë‹ˆë‹¤.")
    
    # 2. ë°ì´í„° ì •ê·œí™”ëŠ” í›ˆë ¨ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ìˆ˜í–‰ (Look-ahead Bias ë°©ì§€)
    print("\n2ï¸âƒ£ ë°ì´í„° ì •ê·œí™”ëŠ” í›ˆë ¨ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ìˆ˜í–‰ë©ë‹ˆë‹¤.")
    print("   ğŸ”¥ Look-ahead Bias ë°©ì§€ë¥¼ ìœ„í•´ í›ˆë ¨/ê²€ì¦ ë¶„í•  í›„ ì •ê·œí™” ì ìš©")
    # ì •ê·œí™”ëŠ” train_on_sequence_data í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ìˆ˜í–‰ë¨
    
    # 3. Decision ë°ì´í„°ì—ì„œ ê°€ê²© ì •ë³´ ì¶”ì¶œ
    print("\n3ï¸âƒ£ Decision ë°ì´í„°ì—ì„œ ê°€ê²© ì •ë³´ ì¶”ì¶œ...")
    price_data = []
    for data in decision_data:
        price_row = {
            'open': data.get('open'),
            'high': data.get('high'),
            'low': data.get('low'),
            'close': data.get('close'),
            'volume': data.get('volume'),
            'quote_volume': data.get('quote_volume')
        }
        price_data.append(price_row)
    
    price_df = pd.DataFrame(price_data)
    print(f"ê°€ê²© ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {len(price_df):,}ê°œ")

    # 4. í›ˆë ¨ ì‹¤í–‰ (ì‹œí€€ìŠ¤ ê¸°ë°˜) - ë¼ë²¨ ìƒì„±ì€ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬
    print("\n4ï¸âƒ£ Multi-Timeframe Transformer ì‹œí€€ìŠ¤ í›ˆë ¨ ì‹œì‘...")
    print("   ğŸ”¥ Look-ahead Bias ë°©ì§€ë¥¼ ìœ„í•´ í˜„ì‹¤ì ì¸ ë¼ë²¨ ìƒì„± ë°©ë²• ì‚¬ìš©")
    print("   ğŸ”¥ ë¼ë²¨ ìƒì„±ì€ í›ˆë ¨ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ë°ì´í„° ë¶„í•  í›„ ìˆ˜í–‰")
    
    print(f"ìµœì¢… ë°ì´í„° ê¸¸ì´: {len(decision_data):,}ê°œ")
    
    trainer = MultiTimeframeTrainer(engine)
    training_results = trainer.train_on_sequence_data(
        decision_data=decision_data,
        seq_len=SEQUENCE_LENGTH,
        batch_size=TRAINING_BATCH_SIZE,
        epochs=TRAINING_EPOCHS,
        validation_split=TRAINING_VALIDATION_SPLIT
    )
    
    # 6. í›ˆë ¨ ê²°ê³¼ ì¶œë ¥
    print(f"\n6ï¸âƒ£ í›ˆë ¨ ê²°ê³¼:")
    print(f"  ìµœê³  ê²€ì¦ ì†ì‹¤: {training_results['best_val_loss']:.4f}")
    print(f"  ì´ ì—í¬í¬: {training_results['total_epochs']}")
    
    # 7. í…ŒìŠ¤íŠ¸ (ì‹œí€€ìŠ¤ ê¸°ë°˜)
    print(f"\n7ï¸âƒ£ í›ˆë ¨ëœ ì‹œí€€ìŠ¤ ëª¨ë¸ í…ŒìŠ¤íŠ¸...")
    
    # ë‹¨ì¼ ë°ì´í„° í…ŒìŠ¤íŠ¸ (í˜¸í™˜ì„±)
    test_data = decision_data[0]
    decision = engine.make_decision(test_data)
    print("ë‹¨ì¼ ë°ì´í„° í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"  ì•¡ì…˜: {decision['action']}")
    print(f"  ì‹ ë¢°ë„: {decision['confidence']:.3f}")
    print(f"  ìˆ˜ìµë¥  ì˜ˆì¸¡: {decision['profit']:.3f}")
    
    # ì‹œí€€ìŠ¤ ë°ì´í„° í…ŒìŠ¤íŠ¸ (ìƒˆë¡œìš´ ê¸°ëŠ¥)
    if len(decision_data) >= SEQUENCE_LENGTH:
        print(f"\nì‹œí€€ìŠ¤ ë°ì´í„° í…ŒìŠ¤íŠ¸ ({TEST_SAMPLES_COUNT}ê°œ ìƒ˜í”Œ):")
        
        # ì—¬ëŸ¬ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸í•˜ì—¬ ì•¡ì…˜ ë¶„í¬ í™•ì¸
        test_action_counts = {'HOLD': 0, 'BUY': 0, 'SELL': 0}
        
        for i in range(min(TEST_SAMPLES_COUNT, len(decision_data) - SEQUENCE_LENGTH)):
            test_sequence = decision_data[i:i+SEQUENCE_LENGTH]
            sequence_decision = engine.make_sequence_decision(test_sequence, seq_len=SEQUENCE_LENGTH)
            
            action = sequence_decision['action']
            test_action_counts[action] += 1
            
            if i < 3:  # ì²˜ìŒ 3ê°œë§Œ ìƒì„¸ ì¶œë ¥
                print(f"  ìƒ˜í”Œ {i+1}:")
                print(f"    ì•¡ì…˜: {action}")
                print(f"    ì‹ ë¢°ë„: {sequence_decision['confidence']:.3f}")
                print(f"    ìˆ˜ìµë¥  ì˜ˆì¸¡: {sequence_decision['profit']:.3f}")
        
        # ì•¡ì…˜ ë¶„í¬ í†µê³„
        total_tests = sum(test_action_counts.values())
        print(f"\n  ğŸ“Š í…ŒìŠ¤íŠ¸ ì•¡ì…˜ ë¶„í¬ (ì´ {total_tests}ê°œ ìƒ˜í”Œ):")
        print(f"    HOLD: {test_action_counts['HOLD']}ê°œ ({test_action_counts['HOLD']/total_tests*100:.1f}%)")
        print(f"    BUY:  {test_action_counts['BUY']}ê°œ ({test_action_counts['BUY']/total_tests*100:.1f}%)")
        print(f"    SELL: {test_action_counts['SELL']}ê°œ ({test_action_counts['SELL']/total_tests*100:.1f}%)")
    
    # 8. ìµœì¢… ëª¨ë¸ ì €ì¥
    engine.save_model(MODEL_FINAL_SAVE_PATH)
    print(f"\nâœ… í›ˆë ¨ëœ ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {MODEL_FINAL_SAVE_PATH}")

if __name__ == "__main__":
    main()
