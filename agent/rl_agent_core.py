"""
ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ ì½”ì–´ ëª¨ë“ˆ (80ì°¨ì› Signal ê¸°ë°˜)
- Signalì˜ ëª¨ë“  indicatorì™€ raw score í™œìš©
- ì¤‘ë³µ ê³„ì‚° ì œê±° ë° ì •ë³´ í™œìš© ê·¹ëŒ€í™”
"""

from pathlib import Path
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque, namedtuple
import random
import gym
from gym import spaces
from datetime import datetime, timedelta
import json
from typing import Dict, List, Tuple, Any, Optional
import logging

Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])

class EnhancedTradingEnvironment(gym.Env):
    """
    80ì°¨ì› Signal ê¸°ë°˜ ì•”í˜¸í™”í ê±°ë˜ í™˜ê²½
    - Signalì˜ ëª¨ë“  ì •ë³´ í™œìš©
    - ì¤‘ë³µ ê³„ì‚° ì œê±°
    """
    
    def __init__(self, price_data: pd.DataFrame, signal_data: List[Dict], 
                 initial_balance: float = 10000.0, max_position: float = 1.0):
        super().__init__()
        
        self.price_data = price_data
        self.signal_data = signal_data
        self.initial_balance = initial_balance
        self.max_position = max_position
        
        # ì•¡ì…˜ ìŠ¤í˜ì´ìŠ¤: [í¬ì§€ì…˜ ë³€ê²½ëŸ‰, ë ˆë²„ë¦¬ì§€, í™€ë”© ì‹œê°„]
        self.action_space = spaces.Box(
            low=np.array([-2.0, 1.0, 0.0]), 
            high=np.array([2.0, 20.0, 1440.0]), 
            dtype=np.float32
        )
        
        # ìƒíƒœ ìŠ¤í˜ì´ìŠ¤: 80ì°¨ì› (20 + 25 + 25 + 10)
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, 
            shape=(80,), 
            dtype=np.float32
        )
        
        # Signal ê¸°ë°˜ ìƒíƒœ êµ¬ì„±ê¸°
        self.state_builder = EnhancedSignalStateBuilder()
        
        self.reset()
    
    def _get_state_size(self) -> int:
        return 80
    
    def _extract_signal_features(self, signals: Dict) -> np.ndarray:
        """Signalì—ì„œ ëª¨ë“  íŠ¹ì„± ì¶”ì¶œ (50ì°¨ì›: 25 technical + 25 decision)"""
        technical_features = self._extract_technical_scores(signals)
        decision_features = self._extract_decision_features(signals)
        
        return np.concatenate([technical_features, decision_features])
    
    def _extract_technical_scores(self, signals: Dict) -> np.ndarray:
        """ê° ì „ëµì˜ raw scoreë“¤ì„ íŠ¹ì„±ìœ¼ë¡œ í™œìš© (25ì°¨ì›)"""
        features = []
        
        # ëª¨ë“  raw score í‚¤ë“¤ ìˆ˜ì§‘
        all_raw_scores = []
        for key, value in signals.items():
            if '_raw_' in key and '_score' in key and value is not None:
                try:
                    all_raw_scores.append(float(value))
                except:
                    all_raw_scores.append(0.0)
        
        # 25ê°œë¡œ ë§ì¶”ê¸°
        if len(all_raw_scores) >= 25:
            sorted_scores = sorted(all_raw_scores, key=abs, reverse=True)
            features = sorted_scores[:25]
        else:
            features = all_raw_scores + [0.0] * (25 - len(all_raw_scores))
        
        return np.array(features, dtype=np.float32)
    
    def _extract_decision_features(self, signals: Dict) -> np.ndarray:
        """Decision íŠ¹ì„±ë“¤ (25ì°¨ì›)"""
        features = []
        
        # ê° ì‹œê°„ëŒ€ë³„ íŠ¹ì„± (3 Ã— 6 = 18ê°œ)
        for timeframe in ['short_term', 'medium_term', 'long_term']:
            action = signals.get(f'{timeframe}_action', 'HOLD')
            action_strength = 1.0 if action == 'LONG' else (-1.0 if action == 'SHORT' else 0.0)
            
            net_score = float(signals.get(f'{timeframe}_net_score', 0.0))
            buy_score = float(signals.get(f'{timeframe}_buy_score', 0.0))
            sell_score = float(signals.get(f'{timeframe}_sell_score', 0.0))
            
            confidence = signals.get(f'{timeframe}_confidence', 'LOW')
            confidence_val = 1.0 if confidence == 'HIGH' else (0.5 if confidence == 'MEDIUM' else 0.0)
            
            leverage = min(float(signals.get(f'{timeframe}_leverage', 1.0)) / 20.0, 1.0)
            
            features.extend([action_strength, net_score, buy_score, sell_score, confidence_val, leverage])
        
        # ì¶”ê°€ ë©”íƒ€ ì •ë³´ (7ê°œ)
        signals_used = []
        for timeframe in ['short_term', 'medium_term', 'long_term']:
            used = signals.get(f'{timeframe}_signals_used', 0)
            signals_used.append(min(float(used) / 10.0, 1.0))
        
        market_contexts = []
        for timeframe in ['short_term', 'medium_term']:
            context = signals.get(f'{timeframe}_market_context', 'NEUTRAL')
            context_val = 1.0 if context == 'TRENDING' else 0.0
            market_contexts.append(context_val)
        
        bias = signals.get('long_term_institutional_bias', 'NEUTRAL')
        bias_val = 1.0 if bias == 'BULLISH' else (-1.0 if bias == 'BEARISH' else 0.0)
        
        strength = signals.get('long_term_macro_trend_strength', 'MEDIUM')
        strength_val = 1.0 if strength == 'HIGH' else (0.5 if strength == 'MEDIUM' else 0.0)
        
        additional_features = signals_used + market_contexts + [bias_val, strength_val]
        features.extend(additional_features)
        
        return np.array(features[:25], dtype=np.float32)
    
    def _extract_price_features(self, idx: int) -> np.ndarray:
        """Signalì˜ indicatorë“¤ì„ í™œìš©í•œ ê°€ê²© íŠ¹ì„± (20ì°¨ì›)"""
        if idx >= len(self.signal_data):
            return np.zeros(20, dtype=np.float32)
        
        current_signal = self.signal_data[idx]
        current_candle = {
            'open': self.price_data.iloc[idx]['open'],
            'high': self.price_data.iloc[idx]['high'],
            'low': self.price_data.iloc[idx]['low'],
            'close': self.price_data.iloc[idx]['close'],
            'volume': self.price_data.iloc[idx]['volume'],
        }
        
        return self.state_builder._extract_price_indicators(current_signal, current_candle)
    
    def _get_portfolio_state(self) -> np.ndarray:
        """í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ì •ë³´ (10ì°¨ì›)"""
        features = [
            self.current_position,
            self.current_leverage / 20.0,
            (self.balance - self.initial_balance) / self.initial_balance,
            self.unrealized_pnl / self.initial_balance if self.initial_balance > 0 else 0.0,
            min(self.total_trades / 100.0, 1.0),
            self.winning_trades / max(self.total_trades, 1),
            self.max_drawdown,
            min(self.consecutive_losses / 10.0, 1.0),
            min(self.holding_time / 1440.0, 1.0),
            1.0 if self.in_position else 0.0
        ]
        return np.array(features, dtype=np.float32)
    
    def reset(self):
        """í™˜ê²½ ì´ˆê¸°í™”"""
        self.current_step = 20
        self.balance = self.initial_balance
        self.current_position = 0.0
        self.current_leverage = 1.0
        self.entry_price = 0.0
        self.unrealized_pnl = 0.0
        self.total_trades = 0
        self.winning_trades = 0
        self.max_drawdown = 0.0
        self.peak_balance = self.initial_balance
        self.consecutive_losses = 0
        self.last_trade_win = True
        self.holding_time = 0
        self.position_entry_step = 0
        self.in_position = False
        
        return self._get_observation()
    
    def _get_observation(self) -> np.ndarray:
        """80ì°¨ì› ìƒíƒœ ê´€ì°°ê°’ ë°˜í™˜ (ì°¨ì› ë³´ì¥)"""
        if self.current_step >= min(len(self.price_data), len(self.signal_data)):
            return np.zeros(80, dtype=np.float32)
        
        # Signalê³¼ í˜„ì¬ ìº”ë“¤ ë°ì´í„°
        current_signal = self.signal_data[self.current_step]
        current_candle = {
            'open': self.price_data.iloc[self.current_step]['open'],
            'high': self.price_data.iloc[self.current_step]['high'],
            'low': self.price_data.iloc[self.current_step]['low'],
            'close': self.price_data.iloc[self.current_step]['close'],
            'volume': self.price_data.iloc[self.current_step]['volume'],
        }
        
        # 1. Price Indicators (20ì°¨ì›)
        price_features = self.state_builder._extract_price_indicators(current_signal, current_candle)
        
        # 2. Technical Scores (25ì°¨ì›)  
        technical_features = self._extract_technical_scores(current_signal)
        
        # 3. Decision Features (25ì°¨ì›)
        decision_features = self._extract_decision_features(current_signal)
        
        # 4. Portfolio Features (10ì°¨ì›)
        portfolio_features = self._get_portfolio_state()
        
        # ëª¨ë“  íŠ¹ì„± ê²°í•©
        observation = np.concatenate([price_features, technical_features, decision_features, portfolio_features])
        
        # ğŸ”¥ ì°¨ì› ë³´ì • (78ì°¨ì› â†’ 80ì°¨ì›)
        current_dim = len(observation)
        if current_dim != 80:
            print(f"âš ï¸ ì°¨ì› ë¶ˆì¼ì¹˜ ê°ì§€: {current_dim} â†’ 80ì°¨ì›ìœ¼ë¡œ ë³´ì •")
            
            if current_dim < 80:
                # ë¶€ì¡±í•œ ì°¨ì›ì„ 0ìœ¼ë¡œ íŒ¨ë”©
                padding = np.zeros(80 - current_dim, dtype=np.float32)
                observation = np.concatenate([observation, padding])
            else:
                # ì´ˆê³¼í•œ ì°¨ì›ì„ ìë¦„
                observation = observation[:80]
        
        return observation.astype(np.float32)
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:
        """í•œ ìŠ¤í… ì‹¤í–‰"""
        if self.current_step >= min(len(self.signal_data), len(self.price_data)) - 1:
            return self._get_observation(), 0.0, True, {}
        
        # ì•¡ì…˜ í•´ì„
        position_change = np.clip(action[0], -2.0, 2.0)
        leverage = np.clip(action[1], 1.0, 20.0)
        target_holding_minutes = np.clip(action[2], 1.0, 1440.0)
        
        # í˜„ì¬ ê°€ê²©
        current_price = self.price_data.iloc[self.current_step]['close']
        next_price = self.price_data.iloc[self.current_step + 1]['close']
        
        # ë³´ìƒ ê³„ì‚° (Signal ì •ë³´ í™œìš©)
        reward = self._calculate_reward(position_change, leverage, current_price, next_price)
        
        # í¬ì§€ì…˜ ì—…ë°ì´íŠ¸
        self._update_position(position_change, leverage, current_price, target_holding_minutes)
        
        # ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ
        self.current_step += 1
        self.holding_time += 3
        
        # í¬ì§€ì…˜ í™€ë”© ì‹œê°„ ì²´í¬
        if self.in_position and self.holding_time >= target_holding_minutes:
            self._close_position(next_price)
        
        # ë‹¤ìŒ ìƒíƒœ
        next_state = self._get_observation()
        
        # ì¢…ë£Œ ì¡°ê±´
        done = (self.current_step >= min(len(self.signal_data), len(self.price_data)) - 1 or 
                self.balance <= self.initial_balance * 0.1)
        
        info = self._create_info_dict()
        
        return next_state, reward, done, info
    
    def _calculate_reward(self, position_change: float, leverage: float, 
                         current_price: float, next_price: float) -> float:
        """Signal ì •ë³´ë¥¼ í™œìš©í•œ ë³´ìƒ í•¨ìˆ˜"""
        reward = 0.0
        
        # 1. PnL ê¸°ë°˜ ë³´ìƒ
        if abs(self.current_position) > 0.01:
            price_change = (next_price - current_price) / current_price
            position_pnl = self.current_position * price_change * self.current_leverage
            reward += position_pnl * 100
        
        # 2. Signal ì •ë³´ë¥¼ í™œìš©í•œ ì‹ í˜¸ ì¼ì¹˜ë„ ë³´ìƒ
        current_signal = self.signal_data[self.current_step]
        signal_alignment = self._calculate_signal_alignment(position_change, current_signal)
        reward += signal_alignment * 10
        
        # 3. ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë³´ìƒ
        risk_penalty = self._calculate_risk_penalty(leverage, self.current_position)
        reward -= risk_penalty
        
        # 4. ê±°ë˜ ë¹ˆë„ íŒ¨ë„í‹°
        if abs(position_change) > 0.1:
            reward -= 0.5
        
        # 5. ì—°ì† ì†ì‹¤ íŒ¨ë„í‹°
        reward -= self.consecutive_losses * 0.2
        
        # 6. í™€ë”© ì‹œê°„ ìµœì í™”
        if self.in_position:
            holding_reward = self._calculate_holding_reward()
            reward += holding_reward
        
        return reward
    
    def _calculate_signal_alignment(self, position_change: float, signals: Dict) -> float:
        """Signalê³¼ ì•¡ì…˜ ì¼ì¹˜ë„ ê³„ì‚°"""
        alignment_score = 0.0
        
        # ê° ì‹œê°„ëŒ€ë³„ ì‹ í˜¸ì™€ì˜ ì¼ì¹˜ë„
        for timeframe in ['short_term', 'medium_term', 'long_term']:
            action = signals.get(f'{timeframe}_action', 'HOLD')
            net_score = float(signals.get(f'{timeframe}_net_score', 0.0))
            
            if action == 'LONG' and position_change > 0:
                alignment_score += abs(net_score)
            elif action == 'SHORT' and position_change < 0:
                alignment_score += abs(net_score)
            elif action == 'HOLD' and abs(position_change) < 0.1:
                alignment_score += 0.1
        
        return alignment_score / 3
    
    def _calculate_risk_penalty(self, leverage: float, position: float) -> float:
        """ë¦¬ìŠ¤í¬ íŒ¨ë„í‹° ê³„ì‚°"""
        penalty = 0.0
        
        if leverage > 10:
            penalty += (leverage - 10) * 0.1
        
        if abs(position) > 0.8:
            penalty += (abs(position) - 0.8) * 5
        
        return penalty
    
    def _calculate_holding_reward(self) -> float:
        """í™€ë”© ì‹œê°„ ìµœì í™” ë³´ìƒ"""
        if self.holding_time > 60:
            return -0.01 * (self.holding_time - 60) / 60
        return 0.0
    
    def _update_position(self, position_change: float, leverage: float, 
                        current_price: float, target_holding_minutes: float):
        """í¬ì§€ì…˜ ì—…ë°ì´íŠ¸"""
        new_position = np.clip(self.current_position + position_change, -1.0, 1.0)
        
        if abs(new_position - self.current_position) > 0.01:
            # ê¸°ì¡´ í¬ì§€ì…˜ ì²­ì‚°
            if abs(self.current_position) > 0.01:
                self._close_position(current_price)
            
            # ìƒˆ í¬ì§€ì…˜ ì§„ì…
            if abs(new_position) > 0.01:
                self.current_position = new_position
                self.current_leverage = leverage
                self.entry_price = current_price
                self.position_entry_step = self.current_step
                self.holding_time = 0
                self.in_position = True
    
    def _close_position(self, exit_price: float):
        """í¬ì§€ì…˜ ì²­ì‚°"""
        if abs(self.current_position) < 0.01:
            return
        
        # PnL ê³„ì‚°
        price_change = (exit_price - self.entry_price) / self.entry_price
        pnl = self.current_position * price_change * self.current_leverage * self.balance
        
        # ì”ê³  ì—…ë°ì´íŠ¸
        self.balance += pnl
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.total_trades += 1
        if pnl > 0:
            self.winning_trades += 1
            self.consecutive_losses = 0
            self.last_trade_win = True
        else:
            self.consecutive_losses += 1
            self.last_trade_win = False
        
        # ìµœëŒ€ ë‚™í­ ì—…ë°ì´íŠ¸
        if self.balance > self.peak_balance:
            self.peak_balance = self.balance
        else:
            drawdown = (self.peak_balance - self.balance) / self.peak_balance
            self.max_drawdown = max(self.max_drawdown, drawdown)
        
        # í¬ì§€ì…˜ ì´ˆê¸°í™”
        self.current_position = 0.0
        self.unrealized_pnl = 0.0
        self.in_position = False
        self.holding_time = 0
    
    def _create_info_dict(self) -> Dict:
        """ì •ë³´ ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        current_price = self.price_data.iloc[min(self.current_step, len(self.price_data)-1)]['close']
        
        return {
            'balance': self.balance,
            'position': self.current_position,
            'unrealized_pnl': self.unrealized_pnl,
            'total_trades': self.total_trades,
            'win_rate': self.winning_trades / max(self.total_trades, 1),
            'current_price': current_price,
            'entry_price': self.entry_price,
            'holding_time': self.holding_time,
            'max_drawdown': self.max_drawdown,
            'step': self.current_step
        }

# EnhancedSignalStateBuilderë¥¼ importí•˜ê¸° ìœ„í•œ í´ë˜ìŠ¤ ì •ì˜
class EnhancedSignalStateBuilder:
    """Signal ê¸°ë°˜ ìƒíƒœ ë²¡í„° êµ¬ì„±ê¸° (live_trading_agent.pyì™€ ë™ì¼)"""
    
    def _extract_price_indicators(self, signal_data: Dict, current_candle: Dict) -> np.ndarray:
        """Signalì˜ indicatorë“¤ì„ price featureë¡œ í™œìš© (20ì°¨ì›)"""
        features = []
        current_price = current_candle['close']
        
        # 1. ê°€ê²© ëŒ€ë¹„ ì§€í‘œ ìœ„ì¹˜ (ì •ê·œí™”)
        vwap = signal_data.get('indicator_vwap', current_price)
        poc = signal_data.get('indicator_poc', current_price)
        hvn = signal_data.get('indicator_hvn', current_price)
        lvn = signal_data.get('indicator_lvn', current_price)
        
        features.extend([
            (current_price - vwap) / current_price if current_price > 0 else 0.0,
            (current_price - poc) / current_price if current_price > 0 else 0.0,   
            (current_price - hvn) / current_price if current_price > 0 else 0.0,   
            (current_price - lvn) / current_price if current_price > 0 else 0.0,   
        ])
        
        # 2. ë³€ë™ì„± ì§€í‘œë“¤
        atr = signal_data.get('indicator_atr', 0.0)
        vwap_std = signal_data.get('indicator_vwap_std', 0.0)
        
        features.extend([
            atr / current_price if current_price > 0 else 0.0,
            vwap_std / current_price if current_price > 0 else 0.0,
        ])
        
        # 3. ì¼ë³„ ê¸°ì¤€ì ë“¤ê³¼ì˜ ê´€ê³„
        prev_high = signal_data.get('indicator_prev_day_high', current_price)
        prev_low = signal_data.get('indicator_prev_day_low', current_price)
        or_high = signal_data.get('indicator_opening_range_high', current_price)
        or_low = signal_data.get('indicator_opening_range_low', current_price)
        
        # ì „ì¼ ë ˆì¸ì§€ì—ì„œì˜ ìœ„ì¹˜
        prev_range = prev_high - prev_low
        if prev_range > 0:
            prev_day_position = (current_price - prev_low) / prev_range
        else:
            prev_day_position = 0.5
            
        # ì˜¤í”„ë‹ ë ˆì¸ì§€ì—ì„œì˜ ìœ„ì¹˜
        or_range = or_high - or_low  
        if or_range > 0:
            or_position = (current_price - or_low) / or_range
        else:
            or_position = 0.5
        
        features.extend([
            prev_day_position,
            or_position,
            (current_price - prev_high) / current_price if current_price > 0 else 0.0,
            (prev_low - current_price) / current_price if current_price > 0 else 0.0,
        ])
        
        # 4. í˜„ì¬ ìº”ë“¤ ì •ë³´
        high, low, close, open_price = current_candle['high'], current_candle['low'], current_candle['close'], current_candle['open']
        volume = current_candle.get('volume', 0)
        
        candle_features = [
            (close - open_price) / open_price if open_price > 0 else 0.0,
            (high - low) / close if close > 0 else 0.0,
            (high - close) / (high - low) if high > low else 0.5,
            (close - low) / (high - low) if high > low else 0.5,
            (close - open_price) / (high - low) if high > low else 0.0,
            min(volume / 1000000, 2.0) if volume > 0 else 0.0,
            1.0 if close > open_price else 0.0,
            (high - max(open_price, close)) / (high - low) if high > low else 0.0
        ]
        
        features.extend(candle_features[:8])
        
        return np.array(features[:20], dtype=np.float32)

class StandardDQN(nn.Module):
    """80ì°¨ì› ì…ë ¥ì„ ìœ„í•œ DQN"""
    
    def __init__(self, state_size: int, action_size: int, hidden_size: int = 512):
        super().__init__()
        
        self.feature_layers = nn.Sequential(
            nn.Linear(state_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU()
        )
        
        # ê° ì•¡ì…˜ ì°¨ì›ë³„ë¡œ ë³„ë„ ì¶œë ¥
        self.position_head = nn.Linear(hidden_size // 2, 21)  # -2.0 ~ 2.0
        self.leverage_head = nn.Linear(hidden_size // 2, 20)  # 1 ~ 20
        self.holding_head = nn.Linear(hidden_size // 2, 48)   # 30ë¶„ ~ 1440ë¶„
    
    def forward(self, x):
        features = self.feature_layers(x)
        
        position_q = self.position_head(features)
        leverage_q = self.leverage_head(features)
        holding_q = self.holding_head(features)
        
        return position_q, leverage_q, holding_q

class StandardRLAgent:
    """80ì°¨ì› Signal ê¸°ë°˜ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸"""
    
    def __init__(self, state_size: int = 80, learning_rate: float = 0.001, 
                 gamma: float = 0.99, epsilon: float = 1.0, epsilon_decay: float = 0.995):
        
        self.state_size = state_size
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = 0.01
        
        # ë„¤íŠ¸ì›Œí¬
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.q_network = StandardDQN(state_size, 3).to(self.device)
        self.target_network = StandardDQN(state_size, 3).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # ê²½í—˜ ë¦¬í”Œë ˆì´
        self.memory = deque(maxlen=10000)
        self.batch_size = 64
        
        # ì„±ëŠ¥ ì¶”ì 
        self.training_rewards = []
        self.losses = []
    
    def remember(self, state, action, reward, next_state, done):
        """ê²½í—˜ ì €ì¥"""
        self.memory.append(Experience(state, action, reward, next_state, done))
    
    def act(self, state: np.ndarray) -> np.ndarray:
        """ì•¡ì…˜ ì„ íƒ"""
        if np.random.random() <= self.epsilon:
            return np.array([
                np.random.uniform(-2.0, 2.0),
                np.random.uniform(1.0, 20.0), 
                np.random.uniform(30.0, 1440.0)
            ])
        
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            position_q, leverage_q, holding_q = self.q_network(state_tensor)
            
            position_idx = torch.argmax(position_q).item()
            leverage_idx = torch.argmax(leverage_q).item()
            holding_idx = torch.argmax(holding_q).item()
            
            # ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€í™˜
            position = -2.0 + (position_idx * 0.2)
            leverage = 1.0 + leverage_idx
            holding = 30.0 + (holding_idx * 30.0)
            
            return np.array([position, leverage, holding])
    
    def replay(self):
        """ê²½í—˜ ë¦¬í”Œë ˆì´ í•™ìŠµ"""
        if len(self.memory) < self.batch_size:
            return
        
        batch = random.sample(self.memory, self.batch_size)
        loss = self._compute_loss(batch)
        
        # ì—­ì „íŒŒ
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()
        
        self.losses.append(loss.item())
        
        # ì—¡ì‹¤ë¡  ê°ì†Œ
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def _compute_loss(self, batch) -> torch.Tensor:
        """ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°"""
        states = torch.FloatTensor([e.state for e in batch]).to(self.device)
        actions = [e.action for e in batch]
        rewards = torch.FloatTensor([e.reward for e in batch]).to(self.device)
        next_states = torch.FloatTensor([e.next_state for e in batch]).to(self.device)
        dones = [bool(e.done) for e in batch]
        
        # í˜„ì¬ Qê°’ë“¤
        current_position_q, current_leverage_q, current_holding_q = self.q_network(states)
        
        # íƒ€ê²Ÿ Qê°’ë“¤
        with torch.no_grad():
            next_position_q, next_leverage_q, next_holding_q = self.target_network(next_states)
            
            target_position_q = current_position_q.clone()
            target_leverage_q = current_leverage_q.clone()
            target_holding_q = current_holding_q.clone()
            
            for i, (action, reward, done) in enumerate(zip(actions, rewards, dones)):
                if not done:
                    pos_idx = int(np.clip((action[0] + 2.0) / 0.2, 0, 20))
                    lev_idx = int(np.clip(action[1] - 1, 0, 19))
                    hold_idx = int(np.clip((action[2] - 30.0) / 30.0, 0, 47))
                    
                    target_position_q[i, pos_idx] = reward + self.gamma * torch.max(next_position_q[i])
                    target_leverage_q[i, lev_idx] = reward + self.gamma * torch.max(next_leverage_q[i])
                    target_holding_q[i, hold_idx] = reward + self.gamma * torch.max(next_holding_q[i])
                else:
                    pos_idx = int(np.clip((action[0] + 2.0) / 0.2, 0, 20))
                    lev_idx = int(np.clip(action[1] - 1, 0, 19))
                    hold_idx = int(np.clip((action[2] - 30.0) / 30.0, 0, 47))
                    
                    target_position_q[i, pos_idx] = reward
                    target_leverage_q[i, lev_idx] = reward
                    target_holding_q[i, hold_idx] = reward
        
        # ì†ì‹¤ ê³„ì‚°
        pos_loss = F.mse_loss(current_position_q, target_position_q)
        lev_loss = F.mse_loss(current_leverage_q, target_leverage_q)
        hold_loss = F.mse_loss(current_holding_q, target_holding_q)
        
        return pos_loss + lev_loss + hold_loss
    
    def update_target_network(self):
        """íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def save_model(self, filepath: str):
        """ëª¨ë¸ ì €ì¥"""
        torch.save({
            'q_network': self.q_network.state_dict(),
            'target_network': self.target_network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'training_rewards': self.training_rewards,
            'losses': self.losses,
            'state_size': self.state_size
        }, filepath)
    
    def load_model(self, filepath: str):
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(filepath, map_location=self.device, weights_only=False)
        self.q_network.load_state_dict(checkpoint['q_network'])
        self.target_network.load_state_dict(checkpoint['target_network'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.epsilon = checkpoint['epsilon']
        self.training_rewards = checkpoint['training_rewards']
        self.losses = checkpoint['losses']

def train_enhanced_agent(price_data: pd.DataFrame, signal_data: List[Dict], 
                        episodes: int = 1000, save_interval: int = 100):
    """80ì°¨ì› Signal ê¸°ë°˜ ì—ì´ì „íŠ¸ í›ˆë ¨"""
    
    # í™˜ê²½ê³¼ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    env = EnhancedTradingEnvironment(price_data, signal_data)
    agent = StandardRLAgent(env.observation_space.shape[0])
    
    episode_rewards = []
    best_reward = -float('inf')
    
    print(f"80ì°¨ì› Signal ê¸°ë°˜ í›ˆë ¨ ì‹œì‘ (í™˜ê²½: {env.observation_space.shape[0]}ì°¨ì›)")
    
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        steps = 0
        
        while True:
            # ì•¡ì…˜ ì„ íƒ ë° ì‹¤í–‰
            action = agent.act(state)
            next_state, reward, done, info = env.step(action)
            
            # ê²½í—˜ ì €ì¥
            agent.remember(state, action, reward, next_state, done)
            
            state = next_state
            total_reward += reward
            steps += 1
            
            # í•™ìŠµ
            if len(agent.memory) > agent.batch_size:
                agent.replay()
            
            if done:
                break
        
        episode_rewards.append(total_reward)
        agent.training_rewards.append(total_reward)
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ (ë§¤ 10 ì—í”¼ì†Œë“œ)
        if episode % 10 == 0:
            agent.update_target_network()
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if episode % 10 == 0:
            avg_reward = np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards)
            print(f"Episode {episode}, Average Reward: {avg_reward:.2f}, "
                  f"Epsilon: {agent.epsilon:.3f}, Balance: ${info['balance']:.2f}, "
                  f"Win Rate: {info['win_rate']:.3f}")
        
        # ëª¨ë¸ ì €ì¥
        if episode % save_interval == 0 and total_reward > best_reward:
            best_reward = total_reward
            agent.save_model(f'best_enhanced_rl_model_80d_ep{episode}.pth')
            print(f"New best 80d model saved at episode {episode} with reward {best_reward:.2f}")
    
    return agent, episode_rewards

def evaluate_enhanced_agent(agent: StandardRLAgent, price_data: pd.DataFrame, 
                           signal_data: List[Dict], episodes: int = 10):
    """80ì°¨ì› ì—ì´ì „íŠ¸ ì„±ëŠ¥ í‰ê°€"""
    env = EnhancedTradingEnvironment(price_data, signal_data)
    agent.epsilon = 0  # íƒí—˜ ë¹„í™œì„±í™”
    
    results = []
    
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        trades = []
        
        while True:
            action = agent.act(state)
            next_state, reward, done, info = env.step(action)
            
            # ê±°ë˜ ê¸°ë¡
            if env.total_trades > len(trades):
                trades.append({
                    'step': env.current_step,
                    'price': info['current_price'],
                    'action': 'CLOSE',
                    'balance': info['balance'],
                    'pnl': info['balance'] - env.initial_balance
                })
            
            state = next_state
            total_reward += reward
            
            if done:
                break
        
        results.append({
            'episode': episode,
            'total_reward': total_reward,
            'final_balance': info['balance'],
            'total_return': (info['balance'] - env.initial_balance) / env.initial_balance,
            'total_trades': info['total_trades'],
            'win_rate': info['win_rate'],
            'max_drawdown': env.max_drawdown,
            'trades': trades
        })
    
    return results

def load_signal_data_with_conversion(agent_folder: str = "agent") -> Optional[List[Dict]]:
    """Signal ë°ì´í„° ë¡œë“œ ë° ë³€í™˜"""
    try:
        parquet_files = list(Path(agent_folder).glob("*.parquet"))
        
        if parquet_files:
            print(f"Signal ë°ì´í„° ë¡œë“œ ì¤‘: {parquet_files[0].name}")
            signal_df = pd.read_parquet(parquet_files[0])
            print(f"Signal ë°ì´í„° ë¡œë“œ: {len(signal_df):,}ê°œ ë ˆì½”ë“œ")
            
            # DataFrameì„ Dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            signal_data = []
            for idx, row in signal_df.iterrows():
                signal_dict = row.to_dict()
                signal_data.append(signal_dict)
            
            print(f"Signal ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(signal_data):,}ê°œ")
            return signal_data
        else:
            print("Parquet íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
    except Exception as e:
        print(f"Signal ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def main_example():
    """80ì°¨ì› Signal ê¸°ë°˜ ì‚¬ìš© ì˜ˆì‹œ"""
    
    print("80ì°¨ì› Signal ê¸°ë°˜ ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© AI")
    print("="*60)
    
    # 1. ë°ì´í„° ë¡œë“œ
    try:
        df_3m = pd.read_csv('data/ETHUSDC_3m_historical_data.csv')
        df_3m['timestamp'] = pd.to_datetime(df_3m['timestamp'])
        df_3m = df_3m.set_index('timestamp')
        price_data = df_3m.reset_index()
        
        print(f"ê°€ê²© ë°ì´í„°: {len(price_data)}ê°œ ìº”ë“¤")
        
        # Signal ë°ì´í„° ë¡œë“œ
        signal_data = load_signal_data_with_conversion()
        
        if signal_data is None:
            print("Signal ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            return
        
        # ë°ì´í„° ê¸¸ì´ ë§ì¶”ê¸°
        min_length = min(len(price_data), len(signal_data))
        price_data = price_data.iloc[:min_length].reset_index(drop=True)
        signal_data = signal_data[:min_length]
        
        print(f"ìµœì¢… ë°ì´í„°: {min_length:,}ê°œ")
        
        # 2. í™˜ê²½ í…ŒìŠ¤íŠ¸
        print("\n80ì°¨ì› í™˜ê²½ í…ŒìŠ¤íŠ¸...")
        env = EnhancedTradingEnvironment(price_data, signal_data)
        state = env.reset()
        print(f"ìƒíƒœ ë²¡í„° ì°¨ì›: {state.shape}")
        print(f"ìƒíƒœ ë²¡í„° ìƒ˜í”Œ: {state[:10]}")
        
        # 3. ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸
        agent = StandardRLAgent(80)
        action = agent.act(state)
        next_state, reward, done, info = env.step(action)
        print(f"ì•¡ì…˜ ìƒ˜í”Œ: {action}")
        print(f"ë³´ìƒ: {reward:.3f}")
        
        print("\n80ì°¨ì› Signal ê¸°ë°˜ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main_example()