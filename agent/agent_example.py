# ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ
import pandas as pd
from datetime import timedelta
import os
import sys
import pickle
from typing import Dict, Any, List, Optional

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# agent ëª¨ë“ˆ import (ì§ì ‘ íŒŒì¼ì—ì„œ import)
import importlib.util

print("ëª¨ë“ˆ ë¡œë”© ì‹œì‘...")

# agent.pyì—ì„œ í•¨ìˆ˜ë“¤ import
agent_path = os.path.join(os.path.dirname(__file__), 'agent.py')
print(f"agent.py ê²½ë¡œ: {agent_path}")
print(f"agent.py ì¡´ì¬ ì—¬ë¶€: {os.path.exists(agent_path)}")

spec = importlib.util.spec_from_file_location("agent", agent_path)
agent_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(agent_module)
train_rl_agent = agent_module.train_rl_agent
evaluate_agent = agent_module.evaluate_agent
print("âœ… agent.py ë¡œë“œ ì™„ë£Œ")

# backtester.pyì—ì„œ í´ë˜ìŠ¤ import (ì„ì‹œë¡œ Noneìœ¼ë¡œ ì„¤ì •)
print("âš ï¸ backtester.py ë¡œë“œ ê±´ë„ˆë›°ê¸° (dataclass ì˜¤ë¥˜)")
BacktestAnalyzer = None
print("âœ… backtester.py ê±´ë„ˆë›°ê¸° ì™„ë£Œ")

def flatten_decision_data(decision_data: Dict[str, Any]) -> Dict[str, Any]:
    """ë³µì¡í•œ ì¤‘ì²© êµ¬ì¡°ë¥¼ í‰ë©´í™”í•˜ì—¬ Parquet ì €ì¥ì— ìµœì í™”"""
    flattened = {}
    
    # ê¸°ë³¸ ì •ë³´
    flattened['timestamp'] = decision_data.get('timestamp')
    
    # indicators ì •ë³´
    indicators = decision_data.get('indicators', {})
    for key, value in indicators.items():
        flattened[f'indicator_{key}'] = value
    
    # decisions ì •ë³´ë¥¼ ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ í‰ë©´í™”
    decisions = decision_data.get('decisions', {})
    
    for category_name, category_data in decisions.items():
        prefix = f"{category_name.lower()}_"
        
        # ê¸°ë³¸ ì •ë³´
        flattened[f'{prefix}action'] = category_data.get('action')
        flattened[f'{prefix}net_score'] = category_data.get('net_score')
        flattened[f'{prefix}leverage'] = category_data.get('leverage')
        flattened[f'{prefix}max_holding_minutes'] = category_data.get('max_holding_minutes')
        flattened[f'{prefix}reason'] = category_data.get('reason')
        
        # sizing ì •ë³´
        sizing = category_data.get('sizing', {})
        flattened[f'{prefix}qty'] = sizing.get('qty')
        flattened[f'{prefix}risk_usd'] = sizing.get('risk_usd')
        flattened[f'{prefix}entry_used'] = sizing.get('entry_used')
        flattened[f'{prefix}stop_used'] = sizing.get('stop_used')
        flattened[f'{prefix}risk_multiplier'] = sizing.get('risk_multiplier')
        
        # meta ì •ë³´
        meta = category_data.get('meta', {})
        flattened[f'{prefix}timeframe'] = meta.get('timeframe')
        
        # synergy_meta ì •ë³´
        synergy_meta = meta.get('synergy_meta', {})
        flattened[f'{prefix}confidence'] = synergy_meta.get('confidence')
        flattened[f'{prefix}market_context'] = synergy_meta.get('market_context')
        flattened[f'{prefix}buy_score'] = synergy_meta.get('buy_score')
        flattened[f'{prefix}sell_score'] = synergy_meta.get('sell_score')
        flattened[f'{prefix}signals_used'] = synergy_meta.get('signals_used')
        
        # ì¥ê¸° ì „ëµ ì¶”ê°€ ì •ë³´
        if category_name == 'LONG_TERM':
            flattened[f'{prefix}institutional_bias'] = synergy_meta.get('institutional_bias')
            flattened[f'{prefix}macro_trend_strength'] = synergy_meta.get('macro_trend_strength')
        
        # raw ì „ëµ ë°ì´í„°ë¥¼ JSON ë¬¸ìì—´ë¡œ ì €ì¥ (í•„ìš”ì‹œ)
        raw_data = category_data.get('raw', {})
        # ì£¼ìš” ì „ëµë“¤ë§Œ ê°œë³„ ì»¬ëŸ¼ìœ¼ë¡œ ì €ì¥
        for strategy_name, strategy_data in raw_data.items():
            if isinstance(strategy_data, dict):
                flattened[f'{prefix}raw_{strategy_name.lower()}_action'] = strategy_data.get('action')
                flattened[f'{prefix}raw_{strategy_name.lower()}_score'] = strategy_data.get('score')
                flattened[f'{prefix}raw_{strategy_name.lower()}_entry'] = strategy_data.get('entry')
                flattened[f'{prefix}raw_{strategy_name.lower()}_stop'] = strategy_data.get('stop')
    
    return flattened

def safe_concat(existing_df, new_df):
    if existing_df is None or existing_df.empty:
        return new_df.copy() if not new_df.empty else pd.DataFrame()
    elif new_df.empty:
        return existing_df.copy()
    else:
        # ì»¬ëŸ¼ ì¼ì¹˜ í™•ì¸
        if list(existing_df.columns) != list(new_df.columns):
            # ê³µí†µ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
            common_cols = list(set(existing_df.columns) & set(new_df.columns))
            if common_cols:
                existing_df = existing_df[common_cols]
                new_df = new_df[common_cols]
            else:
                return existing_df.copy()
        
        # FutureWarning ë°©ì§€: ë¹ˆ DataFrameì´ë‚˜ ëª¨ë“  NAì¸ ì»¬ëŸ¼ ì²˜ë¦¬
        if existing_df.empty or new_df.empty:
            return existing_df if not existing_df.empty else new_df
        
        # ëª¨ë“  NAì¸ ì»¬ëŸ¼ ì œê±°
        existing_df_clean = existing_df.dropna(axis=1, how='all')
        new_df_clean = new_df.dropna(axis=1, how='all')
        
        # ê³µí†µ ì»¬ëŸ¼ë§Œ ìœ ì§€
        common_cols = list(set(existing_df_clean.columns) & set(new_df_clean.columns))
        if not common_cols:
            return existing_df_clean if not existing_df_clean.empty else new_df_clean
        
        existing_df_clean = existing_df_clean[common_cols]
        new_df_clean = new_df_clean[common_cols]
        
        return pd.concat([existing_df_clean, new_df_clean], ignore_index=True)

def save_decisions_to_parquet(
    decision_data_list: List[Dict[str, Any]], 
    filename: str = "agent/decisions_data.parquet",
    append: bool = True
):
    """Decision ë°ì´í„°ë¥¼ Parquet íŒŒì¼ë¡œ ì €ì¥"""
    try:
        if not decision_data_list:
            print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ë°ì´í„° í‰ë©´í™”
        flattened_data = [flatten_decision_data(decision) for decision in decision_data_list]
        new_df = pd.DataFrame(flattened_data)
        
        # timestampë¥¼ datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        if 'timestamp' in new_df.columns:
            new_df['timestamp'] = pd.to_datetime(new_df['timestamp'])
        
        # ê¸°ì¡´ íŒŒì¼ì´ ìˆê³  append ëª¨ë“œì¸ ê²½ìš°
        if append and os.path.exists(filename):
            try:
                existing_df = pd.read_parquet(filename)
                
                # ì¤‘ë³µ ì œê±° (timestamp ê¸°ì¤€)
                if 'timestamp' in existing_df.columns and 'timestamp' in new_df.columns:
                    # ê¸°ì¡´ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ timestamp ì´í›„ ë°ì´í„°ë§Œ ì¶”ê°€
                    last_timestamp = existing_df['timestamp'].max()
                    new_df = new_df[new_df['timestamp'] > last_timestamp]
                
                if not new_df.empty:
                    # ì»¬ëŸ¼ ìˆœì„œ ë§ì¶”ê¸°
                    common_columns = list(set(existing_df.columns) & set(new_df.columns))
                    new_columns = [col for col in new_df.columns if col not in existing_df.columns]
                    
                    # ê¸°ì¡´ DataFrameì— ìƒˆ ì»¬ëŸ¼ ì¶”ê°€ (NaNìœ¼ë¡œ ì±„ì›Œì§)
                    for col in new_columns:
                        existing_df[col] = None
                    
                    # ìƒˆ DataFrameì— ê¸°ì¡´ ì»¬ëŸ¼ ì¶”ê°€ (NaNìœ¼ë¡œ ì±„ì›Œì§)
                    for col in existing_df.columns:
                        if col not in new_df.columns:
                            new_df[col] = None
                    
                    # ì»¬ëŸ¼ ìˆœì„œ ë§ì¶”ê¸°
                    all_columns = list(existing_df.columns)
                    new_df = new_df.reindex(columns=all_columns)
                    
                    # ë°ì´í„° í•©ì¹˜ê¸°
                    combined_df = safe_concat(existing_df, new_df)

                else:
                    combined_df = existing_df
                    print("ìƒˆë¡œ ì¶”ê°€í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (ì¤‘ë³µ ì œê±°ë¨)")
            except Exception as e:
                print(f"ê¸°ì¡´ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨, ìƒˆ íŒŒì¼ë¡œ ì €ì¥: {e}")
                combined_df = new_df
        else:
            combined_df = new_df
        
        # Parquet íŒŒì¼ë¡œ ì €ì¥ (ì••ì¶• ì ìš©)
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        combined_df.to_parquet(filename, compression='snappy', index=False)
        
        print(f"Decision ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename} ({len(combined_df)}ê°œ ë ˆì½”ë“œ)")
        print(f"íŒŒì¼ í¬ê¸°: {os.path.getsize(filename) / 1024 / 1024:.2f} MB")
        return True
        
    except Exception as e:
        print(f"Parquet ì €ì¥ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False

def load_decisions_from_parquet(filename: str = "agent/decisions_data.parquet") -> Optional[pd.DataFrame]:
    """Parquet íŒŒì¼ì—ì„œ Decision ë°ì´í„° ë¡œë“œ"""
    try:
        if not os.path.exists(filename):
            print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
            return None
            
        df = pd.read_parquet(filename)
        print(f"Decision ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {filename} ({len(df)}ê°œ ë ˆì½”ë“œ)")
        return df
        
    except Exception as e:
        print(f"Parquet ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

def inspect_parquet_structure(filename: str = "agent/decisions_data.parquet") -> None:
    """Parquet íŒŒì¼ì˜ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
    try:
        if not os.path.exists(filename):
            print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
            return
            
        df = pd.read_parquet(filename)
        print(f"\n=== Parquet íŒŒì¼ êµ¬ì¡° ë¶„ì„ ===")
        print(f"íŒŒì¼: {filename}")
        print(f"ì´ ë ˆì½”ë“œ ìˆ˜: {len(df)}")
        print(f"ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
        print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
        
        print(f"\nì»¬ëŸ¼ ëª©ë¡:")
        for i, col in enumerate(df.columns):
            non_null_count = df[col].count()
            print(f"  {i+1:2d}. {col:<30} (non-null: {non_null_count}/{len(df)})")
        
        print(f"\nì²« ë²ˆì§¸ ë ˆì½”ë“œ ìƒ˜í”Œ:")
        if len(df) > 0:
            sample_record = df.iloc[0].to_dict()
            for key, value in list(sample_record.items())[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
                print(f"  {key}: {value}")
            if len(sample_record) > 10:
                print(f"  ... (ì´ {len(sample_record)}ê°œ í•„ë“œ)")
        
        print(f"\në°ì´í„° íƒ€ì…:")
        print(df.dtypes)
        
    except Exception as e:
        print(f"Parquet êµ¬ì¡° ë¶„ì„ ì˜¤ë¥˜: {e}")

def convert_parquet_to_signal_data(
    df: pd.DataFrame, 
    max_samples: Optional[int] = None,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
) -> List[Dict[str, Any]]:
    """DataFrameì„ signal_data ë¦¬ìŠ¤íŠ¸ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ë³€í™˜
    
    Args:
        df: ë³€í™˜í•  DataFrame (ì´ë¯¸ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥ë¨)
        max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´ ì‚¬ìš©)
        start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
    """
    try:
        # ë‚ ì§œ í•„í„°ë§
        if start_date or end_date:
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                if start_date:
                    df = df[df['timestamp'] >= start_date]
                if end_date:
                    df = df[df['timestamp'] <= end_date]
                print(f"ë‚ ì§œ í•„í„°ë§ í›„: {len(df)}ê°œ ë ˆì½”ë“œ")
        
        # ìƒ˜í”Œ ìˆ˜ ì œí•œ
        if max_samples and len(df) > max_samples:
            # ìµœê·¼ ë°ì´í„°ë¶€í„° ìƒ˜í”Œë§
            df = df.tail(max_samples)
            print(f"ìƒ˜í”Œë§ í›„: {len(df)}ê°œ ë ˆì½”ë“œ")
        
        # NaN ê°’ì„ Noneìœ¼ë¡œ ë³€í™˜í•˜ê³  ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        df_clean = df.where(pd.notnull(df), None)
        signal_data = df_clean.to_dict('records')
        
        print(f"Parquet ë°ì´í„°ë¥¼ signal_dataë¡œ ë³€í™˜ ì™„ë£Œ: {len(signal_data)}ê°œ ë ˆì½”ë“œ")
        return signal_data
        
    except Exception as e:
        print(f"signal_data ë³€í™˜ ì˜¤ë¥˜: {e}")
        return []

def load_signal_data_directly(
    filename: str = "agent/decisions_data.parquet",
    max_samples: Optional[int] = 5000,  # ê¸°ë³¸ê°’ 5000ê°œë¡œ ì œí•œ
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
) -> List[Dict[str, Any]]:
    """Parquet íŒŒì¼ì—ì„œ ì§ì ‘ signal_dataë¥¼ ë¡œë“œí•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜"""
    try:
        # parquet íŒŒì¼ ë¡œë“œ
        df = pd.read_parquet(filename)
        
        # ë‚ ì§œ í•„í„°ë§
        if start_date or end_date:
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                if start_date:
                    df = df[df['timestamp'] >= start_date]
                if end_date:
                    df = df[df['timestamp'] <= end_date]
        
        # ìƒ˜í”Œ ìˆ˜ ì œí•œ
        if max_samples and len(df) > max_samples:
            df = df.tail(max_samples)
        
        # NaNì„ Noneìœ¼ë¡œ ë³€í™˜í•˜ê³  ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        signal_data = df.where(pd.notnull(df), None).to_dict('records')
        
        print(f"Parquetì—ì„œ signal_data ì§ì ‘ ë¡œë“œ ì™„ë£Œ: {len(signal_data)}ê°œ ë ˆì½”ë“œ")
        return signal_data
        
    except Exception as e:
        print(f"signal_data ì§ì ‘ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return []

def save_progress_state(current_index: int, total_count: int, filename: str = "agent/progress_state.pkl"):
    """ì§„í–‰ ìƒíƒœ ì €ì¥"""
    try:
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        state = {
            'current_index': current_index,
            'total_count': total_count,
            'timestamp': pd.Timestamp.now()
        }
        with open(filename, 'wb') as f:
            pickle.dump(state, f)
        print(f"ì§„í–‰ ìƒíƒœ ì €ì¥: {current_index}/{total_count}")
    except Exception as e:
        print(f"ì§„í–‰ ìƒíƒœ ì €ì¥ ì˜¤ë¥˜: {e}")

def load_progress_state(filename: str = "agent/progress_state.pkl") -> Optional[Dict[str, Any]]:
    """ì§„í–‰ ìƒíƒœ ë¡œë“œ"""
    try:
        if not os.path.exists(filename):
            return None
        
        with open(filename, 'rb') as f:
            state = pickle.load(f)
        
        print(f"ì§„í–‰ ìƒíƒœ ë³µì›: {state['current_index']}/{state['total_count']} "
              f"(ì €ì¥ ì‹œê°„: {state['timestamp']})")
        return state
    except Exception as e:
        print(f"ì§„í–‰ ìƒíƒœ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

def clear_progress_state(filename: str = "agent/progress_state.pkl"):
    """ì§„í–‰ ìƒíƒœ íŒŒì¼ ì‚­ì œ"""
    try:
        if os.path.exists(filename):
            os.remove(filename)
            print("ì§„í–‰ ìƒíƒœ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
    except Exception as e:
        print(f"ì§„í–‰ ìƒíƒœ íŒŒì¼ ì‚­ì œ ì˜¤ë¥˜: {e}")

def load_ethusdc_data():
    """ETHUSDC CSV ë°ì´í„° ë¡œë“œ - 3ë¶„, 15ë¶„, 1ì‹œê°„ë´‰"""
    try:
        required_columns = [ 'open', 'high', 'low', 'close', 'volume', 'quote_volume']
        # 3ë¶„ë´‰ ë°ì´í„° ë¡œë“œ
        df_3m = pd.read_csv('data/ETHUSDC_3m_historical_data.csv')
        df_3m['timestamp'] = pd.to_datetime(df_3m['timestamp'])
        df_3m = df_3m.set_index('timestamp')
        df_3m = df_3m[required_columns]

        df_15m = pd.read_csv('data/ETHUSDC_15m_historical_data.csv')
        df_15m['timestamp'] = pd.to_datetime(df_15m['timestamp'])
        df_15m = df_15m.set_index('timestamp')
        df_15m = df_15m[required_columns]

        # 3ë¶„ë´‰ì—ì„œ 1ì‹œê°„ë´‰ ìƒì„±
        df_1h = pd.read_csv('data/ETHUSDC_1h_historical_data.csv')
        df_1h['timestamp'] = pd.to_datetime(df_1h['timestamp'])
        df_1h = df_1h.set_index('timestamp')
        df_1h = df_1h[required_columns]

        print(f"ETHUSDC 3ë¶„ë´‰ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_3m)}ê°œ ìº”ë“¤")
        print(f"ETHUSDC 15ë¶„ë´‰ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(df_15m)}ê°œ ìº”ë“¤")
        print(f"ETHUSDC 1ì‹œê°„ë´‰ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(df_1h)}ê°œ ìº”ë“¤")
        
        return df_3m, df_15m, df_1h

    except FileNotFoundError as e:
        print(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return None, None, None
    except Exception as e:
        print(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None, None

def generate_signal_data_with_indicators(
    price_data: pd.DataFrame, 
    price_data_15m: pd.DataFrame, 
    price_data_1h: pd.DataFrame,
    resume_from_progress: bool = True
):
    """CSV ë°ì´í„°ë¡œ ì‹¤ì œ ì§€í‘œ ì—…ë°ì´íŠ¸ ë° ì „ëµ ì‹¤í–‰ (ì¤‘ë‹¨ì  ì¬ì‹œì‘ ì§€ì›)"""
    from data.strategy_executor import StrategyExecutor
    from data.data_manager import get_data_manager
    from engines.trade_decision_engine import TradeDecisionEngine
    from indicators.global_indicators import get_global_indicator_manager
    from indicators.global_indicators import get_atr, get_daily_levels, get_opening_range, get_vpvr, get_vwap

    # ì§„í–‰ ìƒíƒœ í™•ì¸
    progress_state = None
    start_idx = None
    
    if resume_from_progress:
        progress_state = load_progress_state()
    
    # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    data_manager = get_data_manager()
    
    print("CSV ë°ì´í„°ë¡œ ì§€í‘œ ì—…ë°ì´íŠ¸ ë° ì „ëµ ì‹¤í–‰ ì¤‘...")
    print(f"   - 3ë¶„ë´‰: {len(price_data)}ê°œ ìº”ë“¤")
    print(f"   - 15ë¶„ë´‰: {len(price_data_15m)}ê°œ ìº”ë“¤")
    print(f"   - 1ì‹œê°„ë´‰: {len(price_data_1h)}ê°œ ìº”ë“¤")
    
    # ì‹œì‘ ìœ„ì¹˜ ê²°ì •
    if progress_state:
        start_idx = progress_state['current_index']
        print(f"ì´ì „ ì§„í–‰ ìƒíƒœì—ì„œ ì¬ì‹œì‘: {start_idx}ë²ˆì§¸ ìº”ë“¤ë¶€í„°")
    else:
        # ìµœê·¼ ë°ì´í„°ë¶€í„° ì²˜ë¦¬ (ìµœëŒ€ max_periodsê°œ)
        target_datetime = price_data.iloc[0].name + timedelta(days=4)
        start_idx = price_data.index.get_loc(target_datetime)
        print(f"ê¸°ì¤€ ë‚ ì§œ {target_datetime}ì˜ ì¸ë±ìŠ¤ ìœ„ì¹˜: {start_idx}")
    
    # ì´ˆê¸° ë°ì´í„° ë¡œë”©
    target_time = price_data.index[start_idx]
    data_manager.load_initial_data(
        symbol='ETHUSDC', 
        df_3m=price_data[price_data.index < target_time], 
        df_15m=price_data_15m[price_data_15m.index < target_time], 
        df_1h=price_data_1h[price_data_1h.index < target_time]
    ) 
    
    global_manager = get_global_indicator_manager(target_time)
    global_manager.initialize_indicators()

    strategy_executor = StrategyExecutor()
    decision_engine = TradeDecisionEngine()

    end_idx = len(price_data)
    batch_size = 500  # 500ê°œì”© ë°°ì¹˜ë¡œ ì €ì¥ (Parquetì€ ë” í° ë°°ì¹˜ê°€ íš¨ìœ¨ì )
    temp_decision_data = []  # ì„ì‹œ ì €ì¥ìš©
    
    try:
        for i in range(start_idx, end_idx):
            # í˜„ì¬ ìº”ë“¤ ë°ì´í„°
            series_3m = price_data.iloc[i]
            current_time = price_data.index[i]
            
            # ë°ì´í„° ë§¤ë‹ˆì €ì— ìº”ë“¤ ë°ì´í„° ì—…ë°ì´íŠ¸
            data_manager.update_with_candle(series_3m)

            # 15ë¶„ë´‰ ë§ˆê° ì‹œê°„ ì²´í¬ (15ë¶„ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì§€ëŠ” ì‹œê°„)
            if current_time.minute % 15 == 0:
                try:
                    series_15m = price_data_15m.loc[current_time]
                    data_manager.update_with_candle_15m(series_15m)
                except KeyError:
                    pass  # í•´ë‹¹ ì‹œê°„ì˜ 15ë¶„ë´‰ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            
            # 1ì‹œê°„ë´‰ ë§ˆê° ì‹œê°„ ì²´í¬ (ì •ì‹œ)
            if current_time.minute == 0:
                try:
                    series_1h = price_data_1h.loc[current_time]
                    data_manager.update_with_candle_1h(series_1h)
                except KeyError:
                    pass  # í•´ë‹¹ ì‹œê°„ì˜ 1ì‹œê°„ë´‰ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                    
            # ê¸€ë¡œë²Œ ì§€í‘œ ì—…ë°ì´íŠ¸
            global_manager.update_all_indicators(series_3m)
            atr = get_atr()
            poc, hvn, lvn = get_vpvr()
            vwap, vwap_std = get_vwap()
            opening_range_high, opening_range_low = get_opening_range()
            prev_day_high, prev_day_low = get_daily_levels()
            indicators = {
                'atr': atr,
                'poc': poc,
                'hvn': hvn,
                'lvn': lvn,
                'vwap': vwap,
                'vwap_std': vwap_std,
                'opening_range_high': opening_range_high,
                'opening_range_low': opening_range_low,
                'prev_day_high': prev_day_high,
                'prev_day_low': prev_day_low,
            }
            
            # ì „ëµ ì‹¤í–‰
            strategy_executor.execute_all_strategies()
            
            # ì‹ í˜¸ ìˆ˜ì§‘
            signals = strategy_executor.get_signals()
            
            # ê±°ë˜ ê²°ì •
            decision = decision_engine.decide_trade_realtime(signals)
            decision.update({'timestamp': current_time, 'indicators': indicators})

            temp_decision_data.append(decision)
            
            # ë°°ì¹˜ í¬ê¸°ë§ˆë‹¤ Parquet íŒŒì¼ì— ì €ì¥
            if len(temp_decision_data) >= batch_size:
                save_decisions_to_parquet(temp_decision_data)
                temp_decision_data = []  # ì„ì‹œ ë°ì´í„° ì´ˆê¸°í™”
            
            # ì§„í–‰ ìƒíƒœ ì €ì¥ (100ê°œë§ˆë‹¤)
            if (i - start_idx) % 100 == 0:
                save_progress_state(i, end_idx)
                total_periods = end_idx - start_idx
                print(f"   ì§„í–‰ë¥ : {i - start_idx + 1}/{total_periods} ({((i - start_idx + 1) / total_periods) * 100:.1f}%)")
        
        # ë‚¨ì€ decision ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì €ì¥
        if temp_decision_data:
            save_decisions_to_parquet(temp_decision_data)
        
        # ì™„ë£Œ í›„ ì§„í–‰ ìƒíƒœ íŒŒì¼ ì‚­ì œ
        clear_progress_state()
        
        total_processed = end_idx - start_idx
        print(f"ì‹ í˜¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: {total_processed}ê°œ")
        
        return True
        
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # ë‚¨ì€ ë°ì´í„° ì €ì¥
        if temp_decision_data:
            save_decisions_to_parquet(temp_decision_data)
            print("ì¤‘ë‹¨ ì „ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        
        # ì§„í–‰ ìƒíƒœ ì €ì¥
        save_progress_state(i, end_idx)
        print("ë‹¤ìŒì— '--resume' ì˜µì…˜ìœ¼ë¡œ ì¬ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return False
        
    except Exception as e:
        print(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ë‚¨ì€ ë°ì´í„° ì €ì¥
        if temp_decision_data:
            save_decisions_to_parquet(temp_decision_data)
        
        # ì§„í–‰ ìƒíƒœ ì €ì¥
        if 'i' in locals():
            save_progress_state(i, end_idx)
        
        import traceback
        traceback.print_exc()
        return False

def check_existing_decision_data() -> bool:
    """ê¸°ì¡´ decision_dataê°€ ìˆëŠ”ì§€ í™•ì¸"""
    try:
        df = load_decisions_from_parquet()
        if df is not None and not df.empty:
            print(f"âœ… ê¸°ì¡´ Decision ë°ì´í„° ë°œê²¬: {len(df)}ê°œ ë ˆì½”ë“œ")
            print(f"   - ì‹œê°„ ë²”ìœ„: {df['timestamp'].min()} ~ {df['timestamp'].max()}")
            print(f"   - ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
            return True
        else:
            print("âŒ ê¸°ì¡´ Decision ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
    except Exception as e:
        print(f"âŒ Decision ë°ì´í„° í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def run_reinforcement_learning(price_data, signal_data):
    """ê°•í™”í•™ìŠµ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=== ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ í›ˆë ¨ ì‹œì‘ ===")
    
    try:
        # 4. ì—ì´ì „íŠ¸ í›ˆë ¨ (ì—í”¼ì†Œë“œ ìˆ˜ ì¡°ì •)
        print("ì—ì´ì „íŠ¸ í›ˆë ¨ ì‹œì‘...")
        agent, rewards = train_rl_agent(price_data, signal_data, episodes=200)
        
        print("\n=== í›ˆë ¨ ì™„ë£Œ, ì„±ëŠ¥ í‰ê°€ ì¤‘ ===")
        
        # 5. ì„±ëŠ¥ í‰ê°€
        print("ì„±ëŠ¥ í‰ê°€ ì‹œì‘...")
        eval_results = evaluate_agent(agent, price_data, signal_data, episodes=10)
        
        # 6. ì„±ëŠ¥ ë¶„ì„
        print("ì„±ëŠ¥ ë¶„ì„ ì‹œì‘...")
        if BacktestAnalyzer is not None:
            analyzer = BacktestAnalyzer()
            metrics = analyzer.calculate_performance_metrics(eval_results)
            report = analyzer.generate_report(eval_results, metrics)
            print(report)
        else:
            print("âš ï¸ BacktestAnalyzerë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„±ëŠ¥ ì •ë³´ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.")
            print(f"í›ˆë ¨ ì™„ë£Œ: {len(rewards)} ì—í”¼ì†Œë“œ")
            print(f"í‰ê·  ë³´ìƒ: {sum(rewards)/len(rewards):.4f}")
            metrics = {"episodes": len(rewards), "avg_reward": sum(rewards)/len(rewards)}
        
        # 7. ëª¨ë¸ ì €ì¥
        print("ëª¨ë¸ ì €ì¥ ì¤‘...")
        agent.save_model('ethusdc_crypto_rl_model.pth')
        print("\nëª¨ë¸ì´ 'ethusdc_crypto_rl_model.pth'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return agent, eval_results, metrics
        
    except Exception as e:
        print(f"âŒ ì—ì´ì „íŠ¸ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

def main_example():
    """ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© AI ì‚¬ìš© ì˜ˆì‹œ - Parquet ì €ì¥ ë° ì¬ì‹œì‘ ì§€ì›"""
    
    print("=== ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© AI í›ˆë ¨ ì‹œì‘ ===")
    
    # 1. ê¸°ì¡´ Decision ë°ì´í„° í™•ì¸
    has_existing_data = check_existing_decision_data()
    
    if has_existing_data:
        print("\nğŸš€ ê¸°ì¡´ Decision ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°”ë¡œ ê°•í™”í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!")
        
        # ê°€ê²© ë°ì´í„° ë¡œë“œ (ê°•í™”í•™ìŠµì— í•„ìš”)
        price_data, price_data_15m, price_data_1h = load_ethusdc_data()
        if price_data is None:
            print("ê°€ê²© ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        
        # Decision ë°ì´í„°ë¥¼ signal_dataë¡œ ë³€í™˜
        inspect_parquet_structure()
        signal_data = load_signal_data_directly()
        
        if not signal_data:
            print("signal_data ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return
        
        # ë°”ë¡œ ê°•í™”í•™ìŠµ ì‹¤í–‰
        return run_reinforcement_learning(price_data, signal_data)
    
    else:
        print("\nğŸ“Š Decision ë°ì´í„°ë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
        
        # 2. ì‹¤ì œ ETHUSDC ë°ì´í„° ë¡œë“œ (3ë¶„, 15ë¶„, 1ì‹œê°„ë´‰)
        price_data, price_data_15m, price_data_1h = load_ethusdc_data()
        
        if price_data is None:
            print("ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        
        print(f"ê°€ê²© ë°ì´í„° ì •ë³´:")
        print(f"   - ì´ ìº”ë“¤ ìˆ˜: {len(price_data)}ê°œ")
        print(f"   - ê°€ê²© ë²”ìœ„: ${price_data['close'].min():.2f} ~ ${price_data['close'].max():.2f}")
        
        # 3. CSV ë°ì´í„°ë¡œ ì‹¤ì œ ì§€í‘œ ì—…ë°ì´íŠ¸ ë° ì „ëµ ì‹¤í–‰ (ì¬ì‹œì‘ ì§€ì›)
        success = generate_signal_data_with_indicators(price_data, price_data_15m, price_data_1h, resume_from_progress=True)

        if success:
            print("Decision ë°ì´í„°ê°€ Parquet íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ì €ì¥ëœ ë°ì´í„° í™•ì¸
            df = load_decisions_from_parquet()
            if df is not None:
                print(f"ì €ì¥ëœ ë°ì´í„° ìš”ì•½:")
                print(f"   - ì´ ë ˆì½”ë“œ ìˆ˜: {len(df)}")
                print(f"   - ì‹œê°„ ë²”ìœ„: {df['timestamp'].min()} ~ {df['timestamp'].max()}")
                print(f"   - ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
            
            # Decision ë°ì´í„°ë¥¼ signal_dataë¡œ ë³€í™˜
            inspect_parquet_structure()
            signal_data = load_signal_data_directly()
            
            if not signal_data:
                print("signal_data ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return
            
            # ê°•í™”í•™ìŠµ ì‹¤í–‰
            return run_reinforcement_learning(price_data, signal_data)
        else:
            print("ë°ì´í„° ìƒì„±ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ì¬ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return None, None, None

if __name__ == "__main__":
    # ì˜ˆì‹œ ì‹¤í–‰
    main_example()