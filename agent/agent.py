import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque, namedtuple
import random
import gym
from gym import spaces
from datetime import datetime, timedelta
import json
from typing import Dict, List, Tuple, Any, Optional
import logging

# ê²½í—˜ íŠœí”Œ ì •ì˜
Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])

class TradingEnvironment(gym.Env):
    """
    ì•”í˜¸í™”í ê±°ë˜ ê°•í™”í•™ìŠµ í™˜ê²½
    """
    
    def __init__(self, price_data: pd.DataFrame, signal_data: List[Dict], 
                 initial_balance: float = 10000.0, max_position: float = 1.0):
        """
        Args:
            price_data: OHLCV ê°€ê²© ë°ì´í„°
            signal_data: ì „ëµ ì‹ í˜¸ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            initial_balance: ì´ˆê¸° ìë³¸
            max_position: ìµœëŒ€ í¬ì§€ì…˜ í¬ê¸° (-1.0 ~ 1.0)
        """
        super().__init__()
        
        self.price_data = price_data
        self.signal_data = signal_data
        self.initial_balance = initial_balance
        self.max_position = max_position
        
        # ì•¡ì…˜ ìŠ¤í˜ì´ìŠ¤: [í¬ì§€ì…˜ ë³€ê²½ëŸ‰, ë ˆë²„ë¦¬ì§€, í™€ë”© ì‹œê°„]
        # í¬ì§€ì…˜: -1(í’€ìˆ) ~ 1(í’€ë¡±), ë ˆë²„ë¦¬ì§€: 1~20, í™€ë”©: 0~1440ë¶„
        self.action_space = spaces.Box(
            low=np.array([-2.0, 1.0, 0.0]), 
            high=np.array([2.0, 20.0, 1440.0]), 
            dtype=np.float32
        )
        
        # ìƒíƒœ ìŠ¤í˜ì´ìŠ¤: ê°€ê²© ë°ì´í„° + ì‹ í˜¸ ë°ì´í„° + í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, 
            shape=(self._get_state_size(),), 
            dtype=np.float32
        )
        
        self.reset()
    
    def _get_state_size(self) -> int:
        """ìƒíƒœ ë²¡í„° í¬ê¸° ê³„ì‚°"""
        # ê°€ê²© íŠ¹ì„±(20) + ì‹ í˜¸ íŠ¹ì„±(30) + í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ(10) = 60
        # í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” 56ì´ ë‚˜ì˜¤ë¯€ë¡œ 56ìœ¼ë¡œ ì„¤ì •
        return 56
    
    def _extract_signal_features(self, signals: Dict) -> np.ndarray:
        """ì‹ í˜¸ ë°ì´í„°ì—ì„œ íŠ¹ì„± ì¶”ì¶œ"""
        features = []
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ íŠ¹ì„± ì¶”ì¶œ
        for category in ['SHORT_TERM', 'MEDIUM_TERM', 'LONG_TERM']:
            if category in signals['decisions']:
                decision = signals['decisions'][category]
                
                # ê¸°ë³¸ íŠ¹ì„±
                features.extend([
                    1.0 if decision['action'] == 'LONG' else (-1.0 if decision['action'] == 'SHORT' else 0.0),
                    float(decision['net_score']),
                    decision['leverage'] / 20.0,  # ì •ê·œí™”
                    decision['max_holding_minutes'] / 1440.0,  # ì •ê·œí™”
                ])
                
                # ì‹ í˜¸ ê°•ë„ íŠ¹ì„±
                meta = decision['meta']['synergy_meta']
                features.extend([
                    float(meta['buy_score']) if 'buy_score' in meta else 0.0,
                    float(meta['sell_score']) if 'sell_score' in meta else 0.0,
                    1.0 if meta['confidence'] == 'HIGH' else (0.5 if meta['confidence'] == 'MEDIUM' else 0.0),
                    len(meta.get('conflicts_detected', [])) / 5.0,  # ì •ê·œí™”
                ])
            else:
                features.extend([0.0] * 8)
        
        # ì „ì²´ ê°ˆë“± ì •ë³´
        conflicts = signals['conflicts']
        features.extend([
            1.0 if conflicts['has_conflicts'] else 0.0,
            len(conflicts['long_categories']) / 3.0,
            len(conflicts['short_categories']) / 3.0,
            signals['meta']['active_positions'] / 3.0,
        ])
        
        # ê°œë³„ ì „ëµ ì ìˆ˜ (ìƒìœ„ 6ê°œë§Œ)
        all_strategies = {}
        for category_data in signals['decisions'].values():
            all_strategies.update(category_data['raw'])
        
        strategy_scores = []
        for strategy_name in ['VWAP_PINBALL', 'LIQUIDITY_GRAB', 'ZSCORE_MEAN_REVERSION', 
                             'SUPPORT_RESISTANCE', 'EMA_CONFLUENCE', 'ICHIMOKU']:
            if strategy_name in all_strategies:
                score = all_strategies[strategy_name]['score']
                action = all_strategies[strategy_name]['action']
                action_val = 1.0 if action == 'BUY' else (-1.0 if action == 'SELL' else 0.0)
                strategy_scores.extend([score, action_val])
            else:
                strategy_scores.extend([0.0, 0.0])
        
        features.extend(strategy_scores[:12])  # 6ê°œ ì „ëµ * 2 = 12ê°œ
        
        return np.array(features[:30], dtype=np.float32)  # 30ê°œë¡œ ì œí•œ
    
    def _extract_price_features(self, idx: int) -> np.ndarray:
        """ê°€ê²© ë°ì´í„°ì—ì„œ íŠ¹ì„± ì¶”ì¶œ"""
        if idx < 20:
            # ì´ˆê¸° ë°ì´í„° ë¶€ì¡±ì‹œ íŒ¨ë”©
            return np.zeros(20, dtype=np.float32)
        
        # ìµœê·¼ 20ê°œ ê°€ê²© ë°ì´í„°
        recent_data = self.price_data.iloc[idx-19:idx+1]
        
        features = []
        
        # ê°€ê²© ë³€í™”ìœ¨
        returns = recent_data['close'].pct_change().fillna(0)
        features.extend([
            returns.mean(),
            returns.std(),
            returns.iloc[-1],  # ìµœê·¼ ìˆ˜ìµë¥ 
            returns.iloc[-5:].mean(),  # 5ê¸°ê°„ í‰ê·  ìˆ˜ìµë¥ 
        ])
        
        # ê¸°ìˆ ì  ì§€í‘œ
        close = recent_data['close']
        high = recent_data['high']
        low = recent_data['low']
        volume = recent_data['volume']
        
        # RSI
        delta = close.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        features.append(rsi.iloc[-1] / 100.0 if not pd.isna(rsi.iloc[-1]) else 0.5)
        
        # ë³¼ë¦°ì € ë°´ë“œ
        sma = close.rolling(window=20).mean()
        std = close.rolling(window=20).std()
        bb_upper = sma + (std * 2)
        bb_lower = sma - (std * 2)
        bb_position = (close.iloc[-1] - bb_lower.iloc[-1]) / (bb_upper.iloc[-1] - bb_lower.iloc[-1])
        features.append(bb_position if not pd.isna(bb_position) else 0.5)
        
        # ì´ë™í‰ê· 
        for window in [5, 10, 20]:
            ma = close.rolling(window=window).mean()
            ma_ratio = close.iloc[-1] / ma.iloc[-1] - 1
            features.append(ma_ratio if not pd.isna(ma_ratio) else 0.0)
        
        # ê±°ë˜ëŸ‰ ì§€í‘œ
        features.extend([
            volume.iloc[-1] / volume.mean() - 1,  # ê±°ë˜ëŸ‰ ë¹„ìœ¨
            (high.iloc[-1] - low.iloc[-1]) / close.iloc[-1],  # ë³€ë™ì„±
        ])
        
        # ê°€ê²© ìœ„ì¹˜
        max_high = high.max()
        min_low = low.min()
        price_position = (close.iloc[-1] - min_low) / (max_high - min_low)
        features.append(price_position if not pd.isna(price_position) else 0.5)
        
        # ì¶”ê°€ ê¸°ìˆ ì  ì§€í‘œë“¤
        features.extend([
            (close.iloc[-1] - close.iloc[-5]) / close.iloc[-5],  # 5ê¸°ê°„ ìˆ˜ìµë¥ 
            (close.iloc[-1] - close.iloc[-10]) / close.iloc[-10],  # 10ê¸°ê°„ ìˆ˜ìµë¥ 
            (high.iloc[-5:].max() - close.iloc[-1]) / close.iloc[-1],  # ìµœê·¼ ê³ ì ê³¼ ê±°ë¦¬
            (close.iloc[-1] - low.iloc[-5:].min()) / close.iloc[-1],  # ìµœê·¼ ì €ì ê³¼ ê±°ë¦¬
        ])
        
        return np.array(features[:20], dtype=np.float32)
    
    def _get_portfolio_state(self) -> np.ndarray:
        """í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ì •ë³´"""
        features = [
            self.current_position,
            self.current_leverage / 20.0,
            self.balance / self.initial_balance - 1,  # ìˆ˜ìµë¥ 
            self.unrealized_pnl / self.initial_balance,
            self.total_trades / 100.0,  # ì •ê·œí™”
            self.winning_trades / max(self.total_trades, 1),
            self.max_drawdown,
            self.consecutive_losses / 10.0,  # ì •ê·œí™”
            min(self.holding_time / 1440.0, 1.0),  # ì •ê·œí™”
            1.0 if self.in_position else 0.0
        ]
        return np.array(features, dtype=np.float32)
    
    def reset(self):
        """í™˜ê²½ ì´ˆê¸°í™”"""
        self.current_step = 20  # ì¶©ë¶„í•œ ê°€ê²© íˆìŠ¤í† ë¦¬ í™•ë³´
        self.balance = self.initial_balance
        self.current_position = 0.0
        self.current_leverage = 1.0
        self.entry_price = 0.0
        self.unrealized_pnl = 0.0
        self.total_trades = 0
        self.winning_trades = 0
        self.max_drawdown = 0.0
        self.peak_balance = self.initial_balance
        self.consecutive_losses = 0
        self.last_trade_win = True
        self.holding_time = 0
        self.position_entry_step = 0
        self.in_position = False
        
        return self._get_observation()
    
    def _get_observation(self) -> np.ndarray:
        """í˜„ì¬ ìƒíƒœ ê´€ì°°ê°’ ë°˜í™˜"""
        if self.current_step >= len(self.signal_data):
            return np.zeros(self.observation_space.shape[0], dtype=np.float32)
        
        # ê°€ê²© íŠ¹ì„±
        price_features = self._extract_price_features(self.current_step)
        
        # ì‹ í˜¸ íŠ¹ì„±
        signal_features = self._extract_signal_features(self.signal_data[self.current_step])
        
        # í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ
        portfolio_features = self._get_portfolio_state()
        
        # ëª¨ë“  íŠ¹ì„± ê²°í•©
        observation = np.concatenate([price_features, signal_features, portfolio_features])
        
        return observation.astype(np.float32)
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:
        """í•œ ìŠ¤í… ì‹¤í–‰"""
        if self.current_step >= len(self.signal_data) - 1:
            return self._get_observation(), 0.0, True, {}
        
        # ì•¡ì…˜ í•´ì„
        position_change = np.clip(action[0], -2.0, 2.0)
        leverage = np.clip(action[1], 1.0, 20.0)
        target_holding_minutes = np.clip(action[2], 1.0, 1440.0)
        
        # í˜„ì¬ ê°€ê²©
        current_price = self.price_data.iloc[self.current_step]['close']
        next_price = self.price_data.iloc[self.current_step + 1]['close']
        
        # ë³´ìƒ ê³„ì‚°
        reward = self._calculate_reward(position_change, leverage, current_price, next_price)
        
        # í¬ì§€ì…˜ ì—…ë°ì´íŠ¸
        self._update_position(position_change, leverage, current_price, target_holding_minutes)
        
        # ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ
        self.current_step += 1
        self.holding_time += 3  # 3ë¶„ ë‹¨ìœ„
        
        # í¬ì§€ì…˜ í™€ë”© ì‹œê°„ ì²´í¬
        if self.in_position and self.holding_time >= target_holding_minutes:
            self._close_position(next_price)
        
        # ë‹¤ìŒ ìƒíƒœ
        next_state = self._get_observation()
        
        # ì¢…ë£Œ ì¡°ê±´
        done = (self.current_step >= len(self.signal_data) - 1 or 
                self.balance <= self.initial_balance * 0.1)  # 90% ì†ì‹¤ì‹œ ì¢…ë£Œ
        
        info = {
            'balance': self.balance,
            'position': self.current_position,
            'unrealized_pnl': self.unrealized_pnl,
            'total_trades': self.total_trades,
            'win_rate': self.winning_trades / max(self.total_trades, 1)
        }
        
        return next_state, reward, done, info
    
    def _calculate_reward(self, position_change: float, leverage: float, 
                         current_price: float, next_price: float) -> float:
        """ë³´ìƒ í•¨ìˆ˜"""
        reward = 0.0
        
        # 1. PnL ê¸°ë°˜ ë³´ìƒ
        if abs(self.current_position) > 0.01:
            price_change = (next_price - current_price) / current_price
            position_pnl = self.current_position * price_change * self.current_leverage
            reward += position_pnl * 100  # ìŠ¤ì¼€ì¼ë§
        
        # 2. ì‹ í˜¸ ë°©í–¥ì„±ê³¼ ì¼ì¹˜ë„ ë³´ìƒ
        signals = self.signal_data[self.current_step]
        signal_alignment = self._calculate_signal_alignment(position_change, signals)
        reward += signal_alignment * 10
        
        # 3. ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë³´ìƒ
        risk_penalty = self._calculate_risk_penalty(leverage, self.current_position)
        reward -= risk_penalty
        
        # 4. ê±°ë˜ ë¹ˆë„ íŒ¨ë„í‹° (ê³¼ë„í•œ ê±°ë˜ ë°©ì§€)
        if abs(position_change) > 0.1:
            reward -= 0.5
        
        # 5. ì—°ì† ì†ì‹¤ íŒ¨ë„í‹°
        reward -= self.consecutive_losses * 0.2
        
        # 6. í™€ë”© ì‹œê°„ ìµœì í™” ë³´ìƒ
        if self.in_position:
            holding_reward = self._calculate_holding_reward()
            reward += holding_reward
        
        return reward
    
    def _calculate_signal_alignment(self, position_change: float, signals: Dict) -> float:
        """ì‹ í˜¸ì™€ ì•¡ì…˜ ì¼ì¹˜ë„ ê³„ì‚°"""
        alignment_score = 0.0
        
        for category, decision in signals['decisions'].items():
            if decision['action'] == 'LONG' and position_change > 0:
                alignment_score += abs(decision['net_score'])
            elif decision['action'] == 'SHORT' and position_change < 0:
                alignment_score += abs(decision['net_score'])
            elif decision['action'] == 'HOLD' and abs(position_change) < 0.1:
                alignment_score += 0.1
        
        return alignment_score / 3  # 3ê°œ ì¹´í…Œê³ ë¦¬ë¡œ ì •ê·œí™”
    
    def _calculate_risk_penalty(self, leverage: float, position: float) -> float:
        """ë¦¬ìŠ¤í¬ íŒ¨ë„í‹° ê³„ì‚°"""
        penalty = 0.0
        
        # ê³¼ë„í•œ ë ˆë²„ë¦¬ì§€ íŒ¨ë„í‹°
        if leverage > 10:
            penalty += (leverage - 10) * 0.1
        
        # ê³¼ë„í•œ í¬ì§€ì…˜ í¬ê¸° íŒ¨ë„í‹°
        if abs(position) > 0.8:
            penalty += (abs(position) - 0.8) * 5
        
        return penalty
    
    def _calculate_holding_reward(self) -> float:
        """í™€ë”© ì‹œê°„ ìµœì í™” ë³´ìƒ"""
        # ë‹¨íƒ€ì˜ ê²½ìš° ë¹ ë¥¸ ì²­ì‚°ì´ ìœ ë¦¬
        if self.holding_time > 60:  # 1ì‹œê°„ ì´ìƒ
            return -0.01 * (self.holding_time - 60) / 60
        return 0.0
    
    def _update_position(self, position_change: float, leverage: float, 
                        current_price: float, target_holding_minutes: float):
        """í¬ì§€ì…˜ ì—…ë°ì´íŠ¸"""
        # ìƒˆë¡œìš´ í¬ì§€ì…˜ ê³„ì‚°
        new_position = np.clip(self.current_position + position_change, -1.0, 1.0)
        
        # í¬ì§€ì…˜ ë³€ê²½ì´ ìˆëŠ” ê²½ìš°
        if abs(new_position - self.current_position) > 0.01:
            # ê¸°ì¡´ í¬ì§€ì…˜ ì²­ì‚°
            if abs(self.current_position) > 0.01:
                self._close_position(current_price)
            
            # ìƒˆ í¬ì§€ì…˜ ì§„ì…
            if abs(new_position) > 0.01:
                self.current_position = new_position
                self.current_leverage = leverage
                self.entry_price = current_price
                self.position_entry_step = self.current_step
                self.holding_time = 0
                self.in_position = True
    
    def _close_position(self, exit_price: float):
        """í¬ì§€ì…˜ ì²­ì‚°"""
        if abs(self.current_position) < 0.01:
            return
        
        # PnL ê³„ì‚°
        price_change = (exit_price - self.entry_price) / self.entry_price
        pnl = self.current_position * price_change * self.current_leverage * self.balance
        
        # ì”ê³  ì—…ë°ì´íŠ¸
        self.balance += pnl
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.total_trades += 1
        if pnl > 0:
            self.winning_trades += 1
            self.consecutive_losses = 0
            self.last_trade_win = True
        else:
            self.consecutive_losses += 1
            self.last_trade_win = False
        
        # ìµœëŒ€ ë‚™í­ ì—…ë°ì´íŠ¸
        if self.balance > self.peak_balance:
            self.peak_balance = self.balance
        else:
            drawdown = (self.peak_balance - self.balance) / self.peak_balance
            self.max_drawdown = max(self.max_drawdown, drawdown)
        
        # í¬ì§€ì…˜ ì´ˆê¸°í™”
        self.current_position = 0.0
        self.unrealized_pnl = 0.0
        self.in_position = False
        self.holding_time = 0


class DQNNetwork(nn.Module):
    """Deep Q-Network"""
    
    def __init__(self, state_size: int, action_size: int, hidden_size: int = 512):
        super().__init__()
        
        self.feature_layers = nn.Sequential(
            nn.Linear(state_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU()
        )
        
        # ê° ì•¡ì…˜ ì°¨ì›ë³„ë¡œ ë³„ë„ ì¶œë ¥
        self.position_head = nn.Linear(hidden_size // 2, 21)  # -2.0 ~ 2.0 (0.2 ê°„ê²©)
        self.leverage_head = nn.Linear(hidden_size // 2, 20)  # 1 ~ 20
        self.holding_head = nn.Linear(hidden_size // 2, 48)   # 30ë¶„ ~ 1440ë¶„ (30ë¶„ ê°„ê²©)
    
    def forward(self, x):
        features = self.feature_layers(x)
        
        position_q = self.position_head(features)
        leverage_q = self.leverage_head(features)
        holding_q = self.holding_head(features)
        
        return position_q, leverage_q, holding_q


class CryptoRLAgent:
    """ì•”í˜¸í™”í ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸"""
    
    def __init__(self, state_size: int, learning_rate: float = 0.001, 
                 gamma: float = 0.99, epsilon: float = 1.0, epsilon_decay: float = 0.995):
        
        self.state_size = state_size
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = 0.01
        
        # ë„¤íŠ¸ì›Œí¬
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.q_network = DQNNetwork(state_size, 3).to(self.device)
        self.target_network = DQNNetwork(state_size, 3).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # ê²½í—˜ ë¦¬í”Œë ˆì´
        self.memory = deque(maxlen=10000)
        self.batch_size = 64
        
        # ì„±ëŠ¥ ì¶”ì 
        self.training_rewards = []
        self.losses = []
    
    def remember(self, state, action, reward, next_state, done):
        """ê²½í—˜ ì €ì¥"""
        self.memory.append(Experience(state, action, reward, next_state, done))
    
    def act(self, state: np.ndarray) -> np.ndarray:
        """ì•¡ì…˜ ì„ íƒ"""
        if np.random.random() <= self.epsilon:
            # ëœë¤ ì•¡ì…˜
            return np.array([
                np.random.uniform(-2.0, 2.0),
                np.random.uniform(1.0, 20.0), 
                np.random.uniform(30.0, 1440.0)
            ])
        
        # Q-ê°’ ê¸°ë°˜ ì•¡ì…˜ ì„ íƒ
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            position_q, leverage_q, holding_q = self.q_network(state_tensor)
            
            # ê° ì°¨ì›ë³„ ìµœì  ì•¡ì…˜ ì„ íƒ
            position_idx = torch.argmax(position_q).item()
            leverage_idx = torch.argmax(leverage_q).item()
            holding_idx = torch.argmax(holding_q).item()
            
            # ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€í™˜
            position = -2.0 + (position_idx * 0.2)
            leverage = 1.0 + leverage_idx
            holding = 30.0 + (holding_idx * 30.0)
            
            return np.array([position, leverage, holding])
    
    def replay(self):
        """ê²½í—˜ ë¦¬í”Œë ˆì´ í•™ìŠµ"""
        if len(self.memory) < self.batch_size:
            return
        
        batch = random.sample(self.memory, self.batch_size)
        states = torch.FloatTensor([e.state for e in batch]).to(self.device)
        actions = [e.action for e in batch]
        rewards = torch.FloatTensor([e.reward for e in batch]).to(self.device)
        next_states = torch.FloatTensor([e.next_state for e in batch]).to(self.device)
        dones = torch.BoolTensor([e.done for e in batch]).to(self.device)
        
        # í˜„ì¬ Qê°’
        current_position_q, current_leverage_q, current_holding_q = self.q_network(states)
        
        # íƒ€ê²Ÿ Qê°’
        with torch.no_grad():
            next_position_q, next_leverage_q, next_holding_q = self.target_network(next_states)
            
            target_position_q = current_position_q.clone()
            target_leverage_q = current_leverage_q.clone()
            target_holding_q = current_holding_q.clone()
            
            for i, (action, reward, done) in enumerate(zip(actions, rewards, dones)):
                if not done:
                    # ê° ì•¡ì…˜ ì°¨ì›ë³„ íƒ€ê²Ÿ ê³„ì‚°
                    pos_idx = int((action[0] + 2.0) / 0.2)
                    lev_idx = int(action[1] - 1)
                    hold_idx = int((action[2] - 30.0) / 30.0)
                    
                    pos_idx = np.clip(pos_idx, 0, 20)
                    lev_idx = np.clip(lev_idx, 0, 19)
                    hold_idx = np.clip(hold_idx, 0, 47)
                    
                    target_position_q[i, pos_idx] = reward + self.gamma * torch.max(next_position_q[i])
                    target_leverage_q[i, lev_idx] = reward + self.gamma * torch.max(next_leverage_q[i])
                    target_holding_q[i, hold_idx] = reward + self.gamma * torch.max(next_holding_q[i])
                else:
                    pos_idx = int((action[0] + 2.0) / 0.2)
                    lev_idx = int(action[1] - 1)
                    hold_idx = int((action[2] - 30.0) / 30.0)
                    
                    pos_idx = np.clip(pos_idx, 0, 20)
                    lev_idx = np.clip(lev_idx, 0, 19)
                    hold_idx = np.clip(hold_idx, 0, 47)
                    
                    target_position_q[i, pos_idx] = reward
                    target_leverage_q[i, lev_idx] = reward
                    target_holding_q[i, hold_idx] = reward
        
        # ì†ì‹¤ ê³„ì‚°
        pos_loss = F.mse_loss(current_position_q, target_position_q)
        lev_loss = F.mse_loss(current_leverage_q, target_leverage_q)
        hold_loss = F.mse_loss(current_holding_q, target_holding_q)
        
        total_loss = pos_loss + lev_loss + hold_loss
        
        # ì—­ì „íŒŒ
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()
        
        self.losses.append(total_loss.item())
        
        # ì—¡ì‹¤ë¡  ê°ì†Œ
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def update_target_network(self):
        """íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def save_model(self, filepath: str):
        """ëª¨ë¸ ì €ì¥"""
        torch.save({
            'q_network': self.q_network.state_dict(),
            'target_network': self.target_network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'training_rewards': self.training_rewards,
            'losses': self.losses
        }, filepath)
    
    def load_model(self, filepath: str):
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.q_network.load_state_dict(checkpoint['q_network'])
        self.target_network.load_state_dict(checkpoint['target_network'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.epsilon = checkpoint['epsilon']
        self.training_rewards = checkpoint['training_rewards']
        self.losses = checkpoint['losses']


def train_rl_agent(price_data: pd.DataFrame, signal_data: List[Dict], 
                  episodes: int = 1000, save_interval: int = 100):
    """ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ í›ˆë ¨"""
    
    # í™˜ê²½ê³¼ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    env = TradingEnvironment(price_data, signal_data)
    agent = CryptoRLAgent(env.observation_space.shape[0])
    
    episode_rewards = []
    best_reward = -float('inf')
    
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        steps = 0
        
        while True:
            # ì•¡ì…˜ ì„ íƒ ë° ì‹¤í–‰
            action = agent.act(state)
            next_state, reward, done, info = env.step(action)
            
            # ê²½í—˜ ì €ì¥
            agent.remember(state, action, reward, next_state, done)
            
            state = next_state
            total_reward += reward
            steps += 1
            
            # í•™ìŠµ
            if len(agent.memory) > agent.batch_size:
                agent.replay()
            
            if done:
                break
        
        episode_rewards.append(total_reward)
        agent.training_rewards.append(total_reward)
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ (ë§¤ 10 ì—í”¼ì†Œë“œ)
        if episode % 10 == 0:
            agent.update_target_network()
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if episode % 50 == 0:
            avg_reward = np.mean(episode_rewards[-50:])
            print(f"Episode {episode}, Average Reward: {avg_reward:.2f}, "
                  f"Epsilon: {agent.epsilon:.3f}, Balance: {info['balance']:.2f}, "
                  f"Win Rate: {info['win_rate']:.3f}")
        
        # ëª¨ë¸ ì €ì¥
        if episode % save_interval == 0 and total_reward > best_reward:
            best_reward = total_reward
            agent.save_model(f'best_crypto_rl_model_ep{episode}.pth')
            print(f"New best model saved at episode {episode} with reward {best_reward:.2f}")
    
    return agent, episode_rewards


def evaluate_agent(agent: CryptoRLAgent, price_data: pd.DataFrame, 
                  signal_data: List[Dict], episodes: int = 10):
    """ì—ì´ì „íŠ¸ ì„±ëŠ¥ í‰ê°€"""
    env = TradingEnvironment(price_data, signal_data)
    agent.epsilon = 0  # íƒí—˜ ë¹„í™œì„±í™”
    
    results = []
    
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        trades = []
        
        while True:
            action = agent.act(state)
            next_state, reward, done, info = env.step(action)
            
            # ê±°ë˜ ê¸°ë¡
            if env.total_trades > len(trades):
                trades.append({
                    'step': env.current_step,
                    'price': env.price_data.iloc[env.current_step]['close'],
                    'action': 'CLOSE',
                    'balance': info['balance'],
                    'pnl': info['balance'] - env.initial_balance
                })
            
            state = next_state
            total_reward += reward
            
            if done:
                break
        
        results.append({
            'episode': episode,
            'total_reward': total_reward,
            'final_balance': info['balance'],
            'total_return': (info['balance'] - env.initial_balance) / env.initial_balance,
            'total_trades': info['total_trades'],
            'win_rate': info['win_rate'],
            'max_drawdown': env.max_drawdown,
            'trades': trades
        })
    
    return results


class BacktestAnalyzer:
    """ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ê¸°"""
    
    @staticmethod
    def calculate_performance_metrics(results: List[Dict]) -> Dict:
        """ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        returns = [r['total_return'] for r in results]
        
        metrics = {
            'avg_return': np.mean(returns),
            'std_return': np.std(returns),
            'sharpe_ratio': np.mean(returns) / np.std(returns) if np.std(returns) > 0 else 0,
            'max_return': np.max(returns),
            'min_return': np.min(returns),
            'win_rate': np.mean([r['win_rate'] for r in results]),
            'avg_trades': np.mean([r['total_trades'] for r in results]),
            'max_drawdown': np.mean([r['max_drawdown'] for r in results]),
            'profit_episodes': sum(1 for r in returns if r > 0),
            'loss_episodes': sum(1 for r in returns if r < 0),
        }
        
        return metrics
    
    @staticmethod
    def generate_report(results: List[Dict], metrics: Dict) -> str:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = f"""
=== ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© AI ì„±ëŠ¥ ë¦¬í¬íŠ¸ ===

ê¸°ë³¸ í†µê³„:
- í‰ê·  ìˆ˜ìµë¥ : {metrics['avg_return']:.2%}
- ìˆ˜ìµë¥  í‘œì¤€í¸ì°¨: {metrics['std_return']:.2%}
- ìƒ¤í”„ ë¹„ìœ¨: {metrics['sharpe_ratio']:.3f}
- ìµœëŒ€ ìˆ˜ìµë¥ : {metrics['max_return']:.2%}
- ìµœì†Œ ìˆ˜ìµë¥ : {metrics['min_return']:.2%}

ê±°ë˜ í†µê³„:
- í‰ê·  ìŠ¹ë¥ : {metrics['win_rate']:.2%}
- í‰ê·  ê±°ë˜ íšŸìˆ˜: {metrics['avg_trades']:.1f}
- ìµœëŒ€ ë‚™í­: {metrics['max_drawdown']:.2%}
- ìˆ˜ìµ ì—í”¼ì†Œë“œ: {metrics['profit_episodes']}ê°œ
- ì†ì‹¤ ì—í”¼ì†Œë“œ: {metrics['loss_episodes']}ê°œ

ìœ„í—˜ ì§€í‘œ:
- ìµœëŒ€ ë‚™í­: {metrics['max_drawdown']:.2%}
- ë³€ë™ì„±: {metrics['std_return']:.2%}
- ì†ì‹¤ í™•ë¥ : {metrics['loss_episodes']/(metrics['profit_episodes']+metrics['loss_episodes']):.2%}
        """
        
        return report


# ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ
def load_ethusdc_data():
    """ETHUSDC CSV ë°ì´í„° ë¡œë“œ - 3ë¶„, 15ë¶„, 1ì‹œê°„ë´‰"""
    try:
        # 3ë¶„ë´‰ ë°ì´í„° ë¡œë“œ
        df_3m = pd.read_csv('data/ETHUSDC_3m_historical_data.csv')
        df_3m['timestamp'] = pd.to_datetime(df_3m['timestamp'])
        df_3m = df_3m.set_index('timestamp')
        
        df_15m = pd.read_csv('data/ETHUSDC_15m_historical_data.csv')
        df_15m['timestamp'] = pd.to_datetime(df_15m['timestamp'])
        df_15m = df_15m.set_index('timestamp')
        
        # 3ë¶„ë´‰ì—ì„œ 1ì‹œê°„ë´‰ ìƒì„±
        df_1h = pd.read_csv('data/ETHUSDC_1h_historical_data.csv')
        df_1h['timestamp'] = pd.to_datetime(df_1h['timestamp'])
        df_1h = df_1h.set_index('timestamp')
        
        print(f"âœ… ETHUSDC 3ë¶„ë´‰ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_3m)}ê°œ ìº”ë“¤")
        print(f"âœ… ETHUSDC 15ë¶„ë´‰ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(df_15m)}ê°œ ìº”ë“¤")
        print(f"âœ… ETHUSDC 1ì‹œê°„ë´‰ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(df_1h)}ê°œ ìº”ë“¤")
        
        return df_3m, df_15m, df_1h

    except FileNotFoundError as e:
        print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return None, None, None, None
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None, None, None

def generate_signal_data_with_indicators(price_data: pd.DataFrame, price_data_15m: pd.DataFrame, 
                                        price_data_1h: pd.DataFrame, max_periods: int = 1000):
    """CSV ë°ì´í„°ë¡œ ì‹¤ì œ ì§€í‘œ ì—…ë°ì´íŠ¸ ë° ì „ëµ ì‹¤í–‰ (3ë¶„, 15ë¶„, 1ì‹œê°„ë´‰ ì‚¬ìš©)"""
    from data.strategy_executor import StrategyExecutor
    from engines.trade_decision_engine import TradeDecisionEngine
    from data.candle_creator import CandleCreator
    from data.data_manager import get_data_manager
    from indicators.global_indicators import get_global_indicator_manager
    from utils.time_manager import get_time_manager
    
    # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    strategy_executor = StrategyExecutor()
    decision_engine = TradeDecisionEngine()
    global_manager = get_global_indicator_manager()
    time_manager = get_time_manager()
    
    signal_data = []
    
    print("ğŸ”„ CSV ë°ì´í„°ë¡œ ì§€í‘œ ì—…ë°ì´íŠ¸ ë° ì „ëµ ì‹¤í–‰ ì¤‘...")
    print(f"   - 3ë¶„ë´‰: {len(price_data)}ê°œ ìº”ë“¤")
    print(f"   - 15ë¶„ë´‰: {len(price_data_15m)}ê°œ ìº”ë“¤")
    print(f"   - 1ì‹œê°„ë´‰: {len(price_data_1h)}ê°œ ìº”ë“¤")
    
    # ìµœê·¼ ë°ì´í„°ë¶€í„° ì²˜ë¦¬ (ìµœëŒ€ max_periodsê°œ)
    start_idx = 500
    
    for i in range(start_idx, len(price_data)):
        try:
            # í˜„ì¬ ìº”ë“¤ ë°ì´í„°
            series_3m = price_data.iloc[i][['open', 'high', 'low', 'close', 'volume', 'quote_volume', 'timestamp']]
            
            # ê¸€ë¡œë²Œ ì§€í‘œ ì—…ë°ì´íŠ¸
            global_manager.update_all_indicators(series_3m)
            
            # ì „ëµ ì‹¤í–‰
            strategy_executor.execute_all_strategies()
            
            # ì‹ í˜¸ ìˆ˜ì§‘
            signals = strategy_executor.get_signals()
            
            # ê±°ë˜ ê²°ì •
            decision = decision_engine.decide_trade_realtime(signals)
            
            signal_data.append(decision)
            
            if (i - start_idx) % 100 == 0:
                print(f"   ì§„í–‰ë¥ : {i - start_idx + 1}/{max_periods} ({((i - start_idx + 1) / max_periods) * 100:.1f}%)")
                
        except Exception as e:
            print(f"âŒ ì‹ í˜¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ (ì¸ë±ìŠ¤ {i}): {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì‹ í˜¸ ìƒì„±
            signal_data.append({
                'decisions': {
                    'SHORT_TERM': {'action': 'HOLD', 'net_score': 0.0, 'leverage': 1, 'max_holding_minutes': 60, 'raw': {}, 'meta': {'synergy_meta': {'confidence': 'LOW', 'buy_score': 0.0, 'sell_score': 0.0, 'conflicts_detected': []}}},
                    'MEDIUM_TERM': {'action': 'HOLD', 'net_score': 0.0, 'leverage': 1, 'max_holding_minutes': 240, 'raw': {}, 'meta': {'synergy_meta': {'confidence': 'LOW', 'buy_score': 0.0, 'sell_score': 0.0, 'conflicts_detected': []}}},
                    'LONG_TERM': {'action': 'HOLD', 'net_score': 0.0, 'leverage': 1, 'max_holding_minutes': 1440, 'raw': {}, 'meta': {'synergy_meta': {'confidence': 'LOW', 'buy_score': 0.0, 'sell_score': 0.0, 'conflicts_detected': []}}}
                },
                'conflicts': {'has_conflicts': False, 'long_categories': [], 'short_categories': []},
                'meta': {'active_positions': 0}
            })
    
    print(f"âœ… ì‹ í˜¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(signal_data)}ê°œ")
    return signal_data

def main_example():
    """ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© AI ì‚¬ìš© ì˜ˆì‹œ - ì‹¤ì œ ë°”ì´ë‚¸ìŠ¤ ë°ì´í„° ì‚¬ìš©"""
    
    print("=== ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© AI í›ˆë ¨ ì‹œì‘ (ì‹¤ì œ ë°ì´í„°) ===")
    
    # 1. ì‹¤ì œ ETHUSDC ë°ì´í„° ë¡œë“œ (3ë¶„, 15ë¶„, 1ì‹œê°„ë´‰)
    price_data_3m, price_data_15m, price_data_1h = load_ethusdc_data()
    
    if price_data_3m is None:
        print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return None, None, None
    
    # 2. ê°€ê²© ë°ì´í„° ì „ì²˜ë¦¬ (3ë¶„ë´‰ì„ ë©”ì¸ìœ¼ë¡œ ì‚¬ìš©)
    price_data = price_data_3m.reset_index()
    price_data = price_data.rename(columns={'timestamp': 'timestamp'})
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
    required_columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
    price_data = price_data[required_columns]
    
    print(f"ğŸ“Š ê°€ê²© ë°ì´í„° ì •ë³´:")
    print(f"   - ê¸°ê°„: {price_data['timestamp'].min()} ~ {price_data['timestamp'].max()}")
    print(f"   - ì´ ìº”ë“¤ ìˆ˜: {len(price_data)}ê°œ")
    print(f"   - ê°€ê²© ë²”ìœ„: ${price_data['close'].min():.2f} ~ ${price_data['close'].max():.2f}")
    
    # 3. CSV ë°ì´í„°ë¡œ ì‹¤ì œ ì§€í‘œ ì—…ë°ì´íŠ¸ ë° ì „ëµ ì‹¤í–‰ (3ë¶„, 15ë¶„, 1ì‹œê°„ë´‰ ì‚¬ìš©)
    signal_data = generate_signal_data_with_indicators(price_data, price_data_15m, price_data_1h, 
                                                     max_periods=min(1000, len(price_data)))
    
    if not signal_data:
        print("âŒ ì‹ í˜¸ ë°ì´í„° ìƒì„± ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return None, None, None
    
    print("=== ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ í›ˆë ¨ ì‹œì‘ ===")
    
    # 4. ì—ì´ì „íŠ¸ í›ˆë ¨ (ì—í”¼ì†Œë“œ ìˆ˜ ì¡°ì •)
    agent, rewards = train_rl_agent(price_data, signal_data, episodes=200)
    
    print("\n=== í›ˆë ¨ ì™„ë£Œ, ì„±ëŠ¥ í‰ê°€ ì¤‘ ===")
    
    # 5. ì„±ëŠ¥ í‰ê°€
    eval_results = evaluate_agent(agent, price_data, signal_data, episodes=10)
    
    # 6. ì„±ëŠ¥ ë¶„ì„
    analyzer = BacktestAnalyzer()
    metrics = analyzer.calculate_performance_metrics(eval_results)
    report = analyzer.generate_report(eval_results, metrics)
    
    print(report)
    
    # 7. ëª¨ë¸ ì €ì¥
    agent.save_model('ethusdc_crypto_rl_model.pth')
    print("\nëª¨ë¸ì´ 'ethusdc_crypto_rl_model.pth'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return agent, eval_results, metrics


# ì‹¤ì‹œê°„ ê±°ë˜ë¥¼ ìœ„í•œ í´ë˜ìŠ¤
class LiveTradingBot:
    """ì‹¤ì‹œê°„ ê±°ë˜ ë´‡"""
    
    def __init__(self, agent: CryptoRLAgent, exchange_api=None):
        self.agent = agent
        self.agent.epsilon = 0  # ì‹¤ê±°ë˜ì—ì„œëŠ” íƒí—˜ ë¹„í™œì„±í™”
        self.exchange_api = exchange_api
        self.current_position = 0.0
        self.last_action_time = datetime.now()
        
    def should_trade(self, current_signals: Dict) -> bool:
        """ê±°ë˜ ì¡°ê±´ í™•ì¸"""
        # ìµœì†Œ ì‹œê°„ ê°„ê²© ì²´í¬ (3ë¶„)
        if (datetime.now() - self.last_action_time).seconds < 180:
            return False
        
        # ì‹ í˜¸ í’ˆì§ˆ ì²´í¬
        high_confidence_signals = 0
        for category in current_signals['decisions'].values():
            if category['meta']['synergy_meta']['confidence'] == 'HIGH':
                high_confidence_signals += 1
        
        return high_confidence_signals >= 1
    
    def get_trading_action(self, price_data: pd.DataFrame, current_signals: Dict) -> Dict:
        """ê±°ë˜ ì•¡ì…˜ ê²°ì •"""
        if not self.should_trade(current_signals):
            return {'action': 'HOLD', 'reason': 'ê±°ë˜ ì¡°ê±´ ë¯¸ì¶©ì¡±'}
        
        # í™˜ê²½ ì„¤ì •
        env = TradingEnvironment(price_data, [current_signals])
        state = env._get_observation()
        
        # AI ì•¡ì…˜ ì˜ˆì¸¡
        action = self.agent.act(state)
        
        return {
            'action': 'TRADE',
            'position_change': action[0],
            'leverage': action[1],
            'holding_minutes': action[2],
            'confidence': self._calculate_action_confidence(state, action),
            'timestamp': datetime.now()
        }
    
    def _calculate_action_confidence(self, state: np.ndarray, action: np.ndarray) -> float:
        """ì•¡ì…˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        # Qê°’ë“¤ì˜ ë¶„ì‚°ì„ ì´ìš©í•œ ì‹ ë¢°ë„ ì¸¡ì •
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.agent.device)
        
        with torch.no_grad():
            pos_q, lev_q, hold_q = self.agent.q_network(state_tensor)
            
            # ê° ì°¨ì›ë³„ Qê°’ ë¶„ì‚°
            pos_var = torch.var(pos_q).item()
            lev_var = torch.var(lev_q).item()
            hold_var = torch.var(hold_q).item()
            
            # ë¶„ì‚°ì´ í´ìˆ˜ë¡ ì‹ ë¢°ë„ ë†’ìŒ (ëª…í™•í•œ ì„ íƒ)
            confidence = (pos_var + lev_var + hold_var) / 3
            
            return min(confidence, 1.0)


if __name__ == "__main__":
    # ì˜ˆì‹œ ì‹¤í–‰
    main_example()