"""
ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ì™€ ë©€í‹°í”„ë ˆì„ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ì˜ˆì¸¡ ì˜ˆì‹œ
- Signal ë°ì´í„°ë§Œ ì…ë ¥í•˜ë©´ ë‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¹„êµ
- ì‹¤ì‹œê°„ ì˜ˆì¸¡ ë° ì„±ëŠ¥ ë¶„ì„
"""

import numpy as np
import pandas as pd
import torch
import random
import os
from datetime import datetime
from typing import Dict, List, Optional
import json

# RL Training Systemì—ì„œ í•„ìš”í•œ í´ë˜ìŠ¤ë“¤ import
from rl_training_system import RLAgent, TradingEnvironment, DataLoader
from multitimeframe_transformer import MultiTimeframeDecisionEngine, DecisionDataLoader

class PredictionComparison:
    """RL ì—ì´ì „íŠ¸ì™€ Multi-Timeframe Transformer ì˜ˆì¸¡ ë¹„êµ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 rl_model_path: str = None,
                 transformer_model_path: str = None):
        
        print("ğŸš€ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # 1. RL ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        print("\n1ï¸âƒ£ RL ì—ì´ì „íŠ¸ ì´ˆê¸°í™”...")
        self.rl_agent = RLAgent(state_size=111)  # 111ì°¨ì›
        
        # RL ëª¨ë¸ ë¡œë“œ
        if rl_model_path and os.path.exists(rl_model_path):
            if self.rl_agent.load_model(rl_model_path):
                print(f"âœ… RL ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {rl_model_path}")
            else:
                print(f"âš ï¸ RL ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ìƒˆ ëª¨ë¸ ì‚¬ìš©")
        else:
            print("âš ï¸ RL ëª¨ë¸ íŒŒì¼ì´ ì—†ì–´ ìƒˆ ëª¨ë¸ ì‚¬ìš©")
        
        # 2. Multi-Timeframe Transformer ì´ˆê¸°í™”
        print("\n2ï¸âƒ£ Multi-Timeframe Transformer ì´ˆê¸°í™”...")
        self.transformer_engine = MultiTimeframeDecisionEngine(
            model_path=transformer_model_path,
            input_size=58,
            d_model=256,
            nhead=8,
            num_layers=6
        )
        
        # Transformer ëª¨ë¸ ë¡œë“œ
        if transformer_model_path and os.path.exists(transformer_model_path):
            if self.transformer_engine.load_model(transformer_model_path):
                print(f"âœ… Transformer ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {transformer_model_path}")
            else:
                print(f"âš ï¸ Transformer ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ìƒˆ ëª¨ë¸ ì‚¬ìš©")
        else:
            print("âš ï¸ Transformer ëª¨ë¸ íŒŒì¼ì´ ì—†ì–´ ìƒˆ ëª¨ë¸ ì‚¬ìš©")
        
        # 3. ë°ì´í„° ë¡œë” ì´ˆê¸°í™”
        self.data_loader = DataLoader()
        
        print("\nâœ… ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def predict_from_signal(self, signal_data: Dict) -> Dict:
        """ë‹¨ì¼ Signal ë°ì´í„°ë¡œ ë‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ ìˆ˜í–‰"""
        
        print(f"\nğŸ”® Signal ë°ì´í„° ì˜ˆì¸¡ ì¤‘...")
        print(f"   íƒ€ì„ìŠ¤íƒ¬í”„: {signal_data.get('timestamp', 'N/A')}")
        print(f"   ê°€ê²©: {signal_data.get('close', 0.0):.2f}")
        
        # 1. RL ì—ì´ì „íŠ¸ ì˜ˆì¸¡
        print("\nğŸ“Š RL ì—ì´ì „íŠ¸ ì˜ˆì¸¡...")
        rl_prediction = self._predict_with_rl_agent(signal_data)
        
        # 2. Multi-Timeframe Transformer ì˜ˆì¸¡
        print("\nğŸ§  Multi-Timeframe Transformer ì˜ˆì¸¡...")
        transformer_prediction = self._predict_with_transformer(signal_data)
        
        # 3. ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ
        comparison = self._compare_predictions(rl_prediction, transformer_prediction)
        
        return {
            'signal_data': signal_data,
            'rl_prediction': rl_prediction,
            'transformer_prediction': transformer_prediction,
            'comparison': comparison,
            'timestamp': datetime.now().isoformat()
        }
    
    def _predict_with_rl_agent(self, signal_data: Dict) -> Dict:
        """RL ì—ì´ì „íŠ¸ë¡œ ì˜ˆì¸¡"""
        try:
            # Signal ë°ì´í„°ë¥¼ RL í™˜ê²½ í˜•íƒœë¡œ ë³€í™˜
            signal_list = [signal_data]
            env = TradingEnvironment(signal_list)
            
            # í™˜ê²½ ì´ˆê¸°í™”
            state, _ = env.reset()
            
            # RL ì—ì´ì „íŠ¸ ì˜ˆì¸¡
            self.rl_agent.model.eval()
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.rl_agent.device)
                q_values = self.rl_agent.q_network(state_tensor)
                action = torch.argmax(q_values).item()
            
            # ì•¡ì…˜ í•´ì„
            action_names = ['HOLD', 'BUY', 'SELL']
            action_name = action_names[action]
            
            # Qê°’ ë¶„ì„
            q_values_np = q_values.cpu().numpy()[0]
            confidence = float(torch.softmax(q_values, dim=1)[0][action].item())
            
            return {
                'action': action_name,
                'action_index': action,
                'q_values': q_values_np.tolist(),
                'confidence': confidence,
                'model_type': 'RL Agent (DuelingDQN)',
                'state_dimension': len(state)
            }
            
        except Exception as e:
            print(f"âŒ RL ì—ì´ì „íŠ¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {
                'action': 'HOLD',
                'action_index': 0,
                'q_values': [0.0, 0.0, 0.0],
                'confidence': 0.33,
                'model_type': 'RL Agent (Error)',
                'error': str(e)
            }
    
    def _predict_with_transformer(self, signal_data: Dict) -> Dict:
        """Multi-Timeframe Transformerë¡œ ì˜ˆì¸¡"""
        try:
            # Transformer ì—”ì§„ìœ¼ë¡œ ì˜ˆì¸¡
            prediction = self.transformer_engine.make_decision(signal_data)
            
            return {
                'action': prediction['action'],
                'confidence': prediction['confidence'],
                'position_size': prediction['position_size'],
                'leverage': prediction['leverage'],
                'holding_time': prediction['holding_time'],
                'profit_prediction': prediction['profit_prediction'],
                'timeframe_analysis': prediction['timeframe_analysis'],
                'model_type': 'Multi-Timeframe Transformer',
                'model_version': prediction.get('model_version', 'v1.0')
            }
            
        except Exception as e:
            print(f"âŒ Transformer ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {
                'action': 'HOLD',
                'confidence': 0.5,
                'position_size': 0.5,
                'leverage': 1.0,
                'holding_time': 30,
                'profit_prediction': 0.0,
                'timeframe_analysis': {},
                'model_type': 'Transformer (Error)',
                'error': str(e)
            }
    
    def _compare_predictions(self, rl_pred: Dict, transformer_pred: Dict) -> Dict:
        """ë‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ"""
        
        # ì•¡ì…˜ ì¼ì¹˜ì„± í™•ì¸
        rl_action = rl_pred['action']
        transformer_action = transformer_pred['action']
        action_match = rl_action == transformer_action
        
        # ì‹ ë¢°ë„ ë¹„êµ
        rl_confidence = rl_pred.get('confidence', 0.0)
        transformer_confidence = transformer_pred.get('confidence', 0.0)
        
        # ìˆ˜ìµë¥  ì˜ˆì¸¡ ë¹„êµ (RLì€ Qê°’ ê¸°ë°˜ ì¶”ì •)
        rl_profit_estimate = self._estimate_profit_from_q_values(rl_pred.get('q_values', [0, 0, 0]))
        transformer_profit = transformer_pred.get('profit_prediction', 0.0)
        
        return {
            'action_match': action_match,
            'rl_action': rl_action,
            'transformer_action': transformer_action,
            'confidence_comparison': {
                'rl_confidence': rl_confidence,
                'transformer_confidence': transformer_confidence,
                'difference': abs(rl_confidence - transformer_confidence)
            },
            'profit_comparison': {
                'rl_estimate': rl_profit_estimate,
                'transformer_prediction': transformer_profit,
                'difference': abs(rl_profit_estimate - transformer_profit)
            },
            'agreement_level': self._calculate_agreement_level(rl_pred, transformer_pred)
        }
    
    def _estimate_profit_from_q_values(self, q_values: List[float]) -> float:
        """Qê°’ì—ì„œ ìˆ˜ìµë¥  ì¶”ì •"""
        if len(q_values) != 3:
            return 0.0
        
        # Qê°’ì˜ ì°¨ì´ë¥¼ ìˆ˜ìµë¥ ë¡œ ê·¼ì‚¬
        max_q = max(q_values)
        min_q = min(q_values)
        return (max_q - min_q) * 0.1  # ìŠ¤ì¼€ì¼ë§
    
    def _calculate_agreement_level(self, rl_pred: Dict, transformer_pred: Dict) -> str:
        """ë‘ ëª¨ë¸ì˜ ì¼ì¹˜ë„ ê³„ì‚°"""
        score = 0
        
        # ì•¡ì…˜ ì¼ì¹˜
        if rl_pred['action'] == transformer_pred['action']:
            score += 3
        
        # ì‹ ë¢°ë„ ì°¨ì´ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        conf_diff = abs(rl_pred.get('confidence', 0) - transformer_pred.get('confidence', 0))
        if conf_diff < 0.1:
            score += 2
        elif conf_diff < 0.3:
            score += 1
        
        # ìˆ˜ìµë¥  ì˜ˆì¸¡ ì°¨ì´
        rl_profit = self._estimate_profit_from_q_values(rl_pred.get('q_values', [0, 0, 0]))
        transformer_profit = transformer_pred.get('profit_prediction', 0)
        profit_diff = abs(rl_profit - transformer_profit)
        if profit_diff < 0.01:
            score += 2
        elif profit_diff < 0.05:
            score += 1
        
        if score >= 6:
            return "ë†’ìŒ (High Agreement)"
        elif score >= 4:
            return "ë³´í†µ (Medium Agreement)"
        else:
            return "ë‚®ìŒ (Low Agreement)"
    
    def batch_predict(self, signal_data_list: List[Dict], max_samples: int = 10) -> List[Dict]:
        """ì—¬ëŸ¬ Signal ë°ì´í„°ì— ëŒ€í•œ ë°°ì¹˜ ì˜ˆì¸¡"""
        
        print(f"\nğŸ”„ ë°°ì¹˜ ì˜ˆì¸¡ ì‹œì‘ ({min(len(signal_data_list), max_samples)}ê°œ ìƒ˜í”Œ)...")
        
        results = []
        for i, signal_data in enumerate(signal_data_list[:max_samples]):
            print(f"\n--- ìƒ˜í”Œ {i+1}/{min(len(signal_data_list), max_samples)} ---")
            result = self.predict_from_signal(signal_data)
            results.append(result)
        
        return results
    
    def print_prediction_result(self, result: Dict):
        """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        
        print("\n" + "="*80)
        print("ğŸ”® ì˜ˆì¸¡ ê²°ê³¼")
        print("="*80)
        
        # Signal ì •ë³´
        signal = result['signal_data']
        print(f"ğŸ“Š Signal ì •ë³´:")
        print(f"   íƒ€ì„ìŠ¤íƒ¬í”„: {signal.get('timestamp', 'N/A')}")
        print(f"   ê°€ê²©: {signal.get('close', 0.0):.2f}")
        print(f"   ê±°ë˜ëŸ‰: {signal.get('volume', 0.0):.0f}")
        
        # RL ì—ì´ì „íŠ¸ ì˜ˆì¸¡
        rl_pred = result['rl_prediction']
        print(f"\nğŸ¤– RL ì—ì´ì „íŠ¸ ì˜ˆì¸¡:")
        print(f"   ì•¡ì…˜: {rl_pred['action']} (ì¸ë±ìŠ¤: {rl_pred['action_index']})")
        print(f"   ì‹ ë¢°ë„: {rl_pred['confidence']:.3f}")
        print(f"   Qê°’: {[f'{q:.3f}' for q in rl_pred['q_values']]}")
        print(f"   ëª¨ë¸: {rl_pred['model_type']}")
        
        # Transformer ì˜ˆì¸¡
        transformer_pred = result['transformer_prediction']
        print(f"\nğŸ§  Multi-Timeframe Transformer ì˜ˆì¸¡:")
        print(f"   ì•¡ì…˜: {transformer_pred['action']}")
        print(f"   ì‹ ë¢°ë„: {transformer_pred['confidence']:.3f}")
        print(f"   í¬ì§€ì…˜ í¬ê¸°: {transformer_pred['position_size']:.3f}")
        print(f"   ë ˆë²„ë¦¬ì§€: {transformer_pred['leverage']:.1f}x")
        print(f"   ë³´ìœ ì‹œê°„: {transformer_pred['holding_time']}ë¶„")
        print(f"   ìˆ˜ìµë¥  ì˜ˆì¸¡: {transformer_pred['profit_prediction']:.3f}")
        
        # ì‹œê°„í”„ë ˆì„ ë¶„ì„
        if 'timeframe_analysis' in transformer_pred:
            print(f"\nğŸ“ˆ ì‹œê°„í”„ë ˆì„ ë¶„ì„:")
            for timeframe, analysis in transformer_pred['timeframe_analysis'].items():
                print(f"   {timeframe}: {analysis['trend']} (ê°•ë„: {analysis['strength']:.3f})")
        
        # ë¹„êµ ê²°ê³¼
        comparison = result['comparison']
        print(f"\nâš–ï¸ ëª¨ë¸ ë¹„êµ:")
        print(f"   ì•¡ì…˜ ì¼ì¹˜: {'âœ…' if comparison['action_match'] else 'âŒ'} ({comparison['rl_action']} vs {comparison['transformer_action']})")
        print(f"   ì‹ ë¢°ë„ ì°¨ì´: {comparison['confidence_comparison']['difference']:.3f}")
        print(f"   ìˆ˜ìµë¥  ì˜ˆì¸¡ ì°¨ì´: {comparison['profit_comparison']['difference']:.3f}")
        print(f"   ì¼ì¹˜ë„: {comparison['agreement_level']}")
        
        print("="*80)

def create_sample_signal_data() -> Dict:
    """ìƒ˜í”Œ Signal ë°ì´í„° ìƒì„±"""
    return {
        'timestamp': datetime.now().isoformat(),
        'open': 2500.0 + random.uniform(-50, 50),
        'high': 2550.0 + random.uniform(-50, 50),
        'low': 2450.0 + random.uniform(-50, 50),
        'close': 2500.0 + random.uniform(-50, 50),
        'volume': random.uniform(1000, 10000),
        'quote_volume': random.uniform(1000000, 10000000),
        
        # Indicator ë°ì´í„°
        'indicator_vwap': 2500.0 + random.uniform(-20, 20),
        'indicator_atr': random.uniform(10, 50),
        'indicator_poc': 2500.0 + random.uniform(-20, 20),
        'indicator_hvn': 2500.0 + random.uniform(-20, 20),
        'indicator_lvn': 2500.0 + random.uniform(-20, 20),
        'indicator_vwap_std': random.uniform(5, 25),
        'indicator_prev_day_high': 2500.0 + random.uniform(-20, 20),
        'indicator_prev_day_low': 2500.0 + random.uniform(-20, 20),
        'indicator_opening_range_high': 2500.0 + random.uniform(-20, 20),
        'indicator_opening_range_low': 2500.0 + random.uniform(-20, 20),
        
        # Strategy ë°ì´í„° (16ê°œ ì „ëµ)
        'session_action': random.choice(['HOLD', 'BUY', 'SELL']),
        'session_score': random.uniform(0, 1),
        'session_confidence': random.choice(['HIGH', 'MEDIUM', 'LOW']),
        'session_entry': 2500.0 + random.uniform(-50, 50),
        'session_stop': 2500.0 + random.uniform(-50, 50),
        
        'vpvr_action': random.choice(['HOLD', 'BUY', 'SELL']),
        'vpvr_score': random.uniform(0, 1),
        'vpvr_confidence': random.choice(['HIGH', 'MEDIUM', 'LOW']),
        'vpvr_entry': 2500.0 + random.uniform(-50, 50),
        'vpvr_stop': 2500.0 + random.uniform(-50, 50),
        
        # ë‚˜ë¨¸ì§€ ì „ëµë“¤ë„ ë¹„ìŠ·í•˜ê²Œ...
        'bollinger_squeeze_action': random.choice(['HOLD', 'BUY', 'SELL']),
        'bollinger_squeeze_score': random.uniform(0, 1),
        'orderflow_cvd_action': random.choice(['HOLD', 'BUY', 'SELL']),
        'orderflow_cvd_score': random.uniform(0, 1),
        'ichimoku_action': random.choice(['HOLD', 'BUY', 'SELL']),
        'ichimoku_score': random.uniform(0, 1),
        'vwap_pinball_action': random.choice(['HOLD', 'BUY', 'SELL']),
        'vwap_pinball_score': random.uniform(0, 1),
        'vol_spike_action': random.choice(['HOLD', 'BUY', 'SELL']),
        'vol_spike_score': random.uniform(0, 1),
        'liquidity_grab_action': random.choice(['HOLD', 'BUY', 'SELL']),
        'liquidity_grab_score': random.uniform(0, 1),
        'vpvr_micro_action': random.choice(['HOLD', 'BUY', 'SELL']),
        'vpvr_micro_score': random.uniform(0, 1),
        'zscore_mean_reversion_action': random.choice(['HOLD', 'BUY', 'SELL']),
        'zscore_mean_reversion_score': random.uniform(0, 1),
        'htf_trend_action': random.choice(['HOLD', 'BUY', 'SELL']),
        'htf_trend_score': random.uniform(0, 1),
        'oi_delta_action': random.choice(['HOLD', 'BUY', 'SELL']),
        'oi_delta_score': random.uniform(0, 1),
        'funding_rate_action': random.choice(['HOLD', 'BUY', 'SELL']),
        'funding_rate_score': random.uniform(0, 1),
        'multi_timeframe_action': random.choice(['HOLD', 'BUY', 'SELL']),
        'multi_timeframe_score': random.uniform(0, 1),
        'support_resistance_action': random.choice(['HOLD', 'BUY', 'SELL']),
        'support_resistance_score': random.uniform(0, 1),
        'ema_confluence_action': random.choice(['HOLD', 'BUY', 'SELL']),
        'ema_confluence_score': random.uniform(0, 1),
    }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”® RL ì—ì´ì „íŠ¸ vs Multi-Timeframe Transformer ì˜ˆì¸¡ ë¹„êµ")
    print("="*80)
    
    try:
        # 1. ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        print("\n1ï¸âƒ£ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ íŒŒì¼ ì°¾ê¸°
        rl_model_path = None
        transformer_model_path = None
        
        # RL ëª¨ë¸ ì°¾ê¸°
        rl_models = [
            'agent/best_test_performance_model_return0.012.pth',
            'agent/best_test_performance_model_return0.011.pth',
            'agent/final_optimized_model_111d.pth'
        ]
        
        for model_path in rl_models:
            if os.path.exists(model_path):
                rl_model_path = model_path
                break
        
        # Transformer ëª¨ë¸ ì°¾ê¸°
        transformer_models = [
            'agent/multitimeframe_transformer_trained.pth',
            'agent/best_multitimeframe_model.pth'
        ]
        
        for model_path in transformer_models:
            if os.path.exists(model_path):
                transformer_model_path = model_path
                break
        
        # ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        predictor = PredictionComparison(rl_model_path, transformer_model_path)
        
        # 2. ì‹¤ì œ ë°ì´í„° ë¡œë“œ (ì„ íƒì‚¬í•­)
        print("\n2ï¸âƒ£ ë°ì´í„° ë¡œë“œ...")
        signal_data = DataLoader.load_signal_data()
        
        if signal_data and len(signal_data) > 0:
            print(f"âœ… ì‹¤ì œ ë°ì´í„° ë¡œë“œ: {len(signal_data):,}ê°œ")
            # ì‹¤ì œ ë°ì´í„°ì—ì„œ ìƒ˜í”Œ ì„ íƒ
            sample_signals = random.sample(signal_data, min(5, len(signal_data)))
        else:
            print("âš ï¸ ì‹¤ì œ ë°ì´í„° ì—†ìŒ, ìƒ˜í”Œ ë°ì´í„° ìƒì„±")
            # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
            sample_signals = [create_sample_signal_data() for _ in range(5)]
        
        # 3. ì˜ˆì¸¡ ì‹¤í–‰
        print("\n3ï¸âƒ£ ì˜ˆì¸¡ ì‹¤í–‰...")
        
        for i, signal in enumerate(sample_signals):
            print(f"\n{'='*60}")
            print(f"ğŸ“Š ì˜ˆì¸¡ {i+1}/{len(sample_signals)}")
            print(f"{'='*60}")
            
            result = predictor.predict_from_signal(signal)
            predictor.print_prediction_result(result)
        
        # 4. ë°°ì¹˜ ì˜ˆì¸¡ (ì„ íƒì‚¬í•­)
        print(f"\n4ï¸âƒ£ ë°°ì¹˜ ì˜ˆì¸¡ ìš”ì•½...")
        batch_results = predictor.batch_predict(sample_signals, max_samples=3)
        
        # ì „ì²´ í†µê³„
        action_matches = sum(1 for r in batch_results if r['comparison']['action_match'])
        avg_agreement = sum(1 for r in batch_results if 'ë†’ìŒ' in r['comparison']['agreement_level']) / len(batch_results)
        
        print(f"\nğŸ“ˆ ë°°ì¹˜ ì˜ˆì¸¡ í†µê³„:")
        print(f"   ì´ ì˜ˆì¸¡ ìˆ˜: {len(batch_results)}")
        print(f"   ì•¡ì…˜ ì¼ì¹˜: {action_matches}/{len(batch_results)} ({action_matches/len(batch_results)*100:.1f}%)")
        print(f"   ë†’ì€ ì¼ì¹˜ë„: {avg_agreement*100:.1f}%")
        
        print(f"\nâœ… ì˜ˆì¸¡ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
