# ì™„ì „ ë…ë¦½í˜• ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œ
# í•œ íŒŒì¼ ì‹¤í–‰ìœ¼ë¡œ ë°ì´í„° ë¡œë”© -> í›ˆë ¨ -> í‰ê°€ê¹Œì§€ ëª¨ë“  ê¸°ëŠ¥ í¬í•¨

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque, namedtuple
import random
import gym
from gym import spaces
from datetime import datetime, timedelta
import json
from typing import Dict, List, Tuple, Any, Optional
import os
import warnings
from pathlib import Path

# PyTorch 2.6 í˜¸í™˜ì„± ì„¤ì •
def setup_safe_torch_loading():
    """PyTorch 2.6ì—ì„œ ì•ˆì „í•œ ëª¨ë¸ ë¡œë”©ì„ ìœ„í•œ ì„¤ì •"""
    try:
        torch.serialization.add_safe_globals([
            np.core.multiarray.scalar,
            np.ndarray,
            np.dtype,
            np.float32,
            np.float64,
            np.int32,
            np.int64,
        ])
        print("âœ… PyTorch 2.6 í˜¸í™˜ ì„¤ì • ì™„ë£Œ")
    except AttributeError:
        print("â„¹ï¸  PyTorch ì´ì „ ë²„ì „ ê°ì§€ë¨")

setup_safe_torch_loading()

# ê²½í—˜ íŠœí”Œ ì •ì˜
Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])

# =================================================================
# 1. ê°œì„ ëœ ë³´ìƒ í•¨ìˆ˜
# =================================================================

class ImprovedRewardFunction:
    """ìŠ¹ë¥ ê³¼ ìˆ˜ìµì„±ì„ ë™ì‹œì— ê°œì„ í•˜ëŠ” ë³´ìƒ í•¨ìˆ˜"""
    
    def __init__(self):
        self.recent_trades = deque(maxlen=50)
        self.baseline_return = 0.0
        
    def calculate_reward(self, current_price, entry_price, position, action, 
                        holding_time, volatility=0.02, volume_ratio=1.0, trade_pnl=None):
        """ê°œì„ ëœ ë³´ìƒ ê³„ì‚°"""
        reward = 0.0
        
        # 1. í¬ì§€ì…˜ ë³´ìœ  ì¤‘ ì‹¤ì‹œê°„ í‰ê°€
        if abs(position) > 0.01:
            unrealized_pnl = self._calculate_unrealized_pnl(current_price, entry_price, position)
            
            if unrealized_pnl > 0:
                reward += min(unrealized_pnl * 10, 1.0)  # ìˆ˜ìµì‹œ ë³´ìƒ
            else:
                reward += max(unrealized_pnl * 15, -2.0)  # ì†ì‹¤ì‹œ ë” í° íŒ¨ë„í‹°
            
            # í™€ë”© ì‹œê°„ ìµœì í™”
            if holding_time > 30:
                reward -= 0.1 * (holding_time - 30) / 30
        
        # 2. ê±°ë˜ ì™„ë£Œì‹œ ìŠ¹ë¥  ì¤‘ì‹¬ í‰ê°€
        if trade_pnl is not None:  # ê±°ë˜ ì™„ë£Œ
            self.recent_trades.append(1 if trade_pnl > 0 else 0)
            current_win_rate = np.mean(self.recent_trades) if self.recent_trades else 0.5
            
            if trade_pnl > 0:  # ìˆ˜ìµ ê±°ë˜
                reward += 5.0  # ìŠ¹ë¥  í–¥ìƒì„ ìœ„í•œ í° ë³´ìƒ
                if current_win_rate > 0.6:
                    reward += 2.0  # ì—°ì† ìŠ¹ë¥  ë³´ë„ˆìŠ¤
            else:  # ì†ì‹¤ ê±°ë˜
                reward -= 3.0  # ìƒëŒ€ì ìœ¼ë¡œ ì‘ì€ íŒ¨ë„í‹°
        
        # 3. ì‹œì¥ ì»¨í…ìŠ¤íŠ¸ ë°˜ì˜
        if volatility > 0.05:  # ê³ ë³€ë™ì„±
            if abs(position) < 0.3:
                reward += 0.5
            else:
                reward -= 1.0
        
        return reward
    
    def _calculate_unrealized_pnl(self, current_price, entry_price, position):
        """ë¯¸ì‹¤í˜„ ì†ìµ ê³„ì‚°"""
        if entry_price <= 0:
            return 0.0
        if position > 0:  # Long
            return (current_price - entry_price) / entry_price
        else:  # Short  
            return (entry_price - current_price) / entry_price

# =================================================================
# 2. ê°œì„ ëœ DQN ë„¤íŠ¸ì›Œí¬
# =================================================================

class ImprovedDQNNetwork(nn.Module):
    """ê°œì„ ëœ Dueling DQN ì•„í‚¤í…ì²˜"""
    
    def __init__(self, state_size, action_size, hidden_size=256):
        super().__init__()
        
        # ì…ë ¥ ì •ê·œí™”
        self.input_norm = nn.LayerNorm(state_size)
        
        # íŠ¹ì„± ì¶”ì¶œ ë ˆì´ì–´
        self.feature_extraction = nn.Sequential(
            nn.Linear(state_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.LayerNorm(hidden_size),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.LayerNorm(hidden_size)
        )
        
        # Dueling êµ¬ì¡°
        self.value_stream = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, 1)
        )
        
        # ê° ì•¡ì…˜ ì°¨ì›ë³„ advantage stream
        self.position_advantage = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, 21)  # -2.0 ~ 2.0 (0.2 ê°„ê²©)
        )
        
        self.leverage_advantage = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2), 
            nn.ReLU(),
            nn.Linear(hidden_size // 2, 20)  # 1 ~ 20
        )
        
        self.holding_advantage = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, 48)  # 30ë¶„ ~ 1440ë¶„
        )
        
        self.apply(self._init_weights)
    
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            nn.init.orthogonal_(m.weight)
            nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # ì…ë ¥ ì •ê·œí™”
        x = self.input_norm(x)
        
        # íŠ¹ì„± ì¶”ì¶œ
        features = self.feature_extraction(x)
        
        # Dueling ë¶„ë¦¬
        value = self.value_stream(features)
        
        position_adv = self.position_advantage(features)
        leverage_adv = self.leverage_advantage(features) 
        holding_adv = self.holding_advantage(features)
        
        # Dueling ê²°í•©
        position_q = value + position_adv - position_adv.mean(dim=1, keepdim=True)
        leverage_q = value + leverage_adv - leverage_adv.mean(dim=1, keepdim=True)
        holding_q = value + holding_adv - holding_adv.mean(dim=1, keepdim=True)
        
        return position_q, leverage_q, holding_q

# =================================================================
# 3. ê°œì„ ëœ ê±°ë˜ í™˜ê²½
# =================================================================

class ImprovedTradingEnvironment(gym.Env):
    """ê°œì„ ëœ ê±°ë˜ í™˜ê²½"""
    
    def __init__(self, price_data: pd.DataFrame, signal_data: List[Dict], 
                 initial_balance: float = 10000.0, max_position: float = 1.0):
        super().__init__()
        
        self.price_data = price_data
        self.signal_data = signal_data
        self.initial_balance = initial_balance
        self.max_position = max_position
        
        # ê°œì„ ëœ ë³´ìƒ í•¨ìˆ˜
        self.reward_function = ImprovedRewardFunction()
        
        # ì•¡ì…˜ ìŠ¤í˜ì´ìŠ¤ (ì—°ì†í˜•)
        self.action_space = spaces.Box(
            low=np.array([-2.0, 1.0, 0.0]), 
            high=np.array([2.0, 20.0, 1440.0]), 
            dtype=np.float32
        )
        
        # ìƒíƒœ ìŠ¤í˜ì´ìŠ¤
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, 
            shape=(60,),  # 20(ê°€ê²©) + 30(ì‹ í˜¸) + 10(í¬íŠ¸í´ë¦¬ì˜¤)
            dtype=np.float32
        )
        
        self.reset()
    
    def reset(self):
        """í™˜ê²½ ì´ˆê¸°í™”"""
        self.current_step = 20  # ì¶©ë¶„í•œ íˆìŠ¤í† ë¦¬ í™•ë³´
        self.balance = self.initial_balance
        self.current_position = 0.0
        self.current_leverage = 1.0
        self.entry_price = 0.0
        self.unrealized_pnl = 0.0
        self.total_trades = 0
        self.winning_trades = 0
        self.max_drawdown = 0.0
        self.peak_balance = self.initial_balance
        self.consecutive_losses = 0
        self.holding_time = 0
        self.in_position = False
        self.last_trade_pnl = None
        
        return self._get_observation()
    
    def _get_observation(self) -> np.ndarray:
        """ìƒíƒœ ê´€ì°°ê°’ ë°˜í™˜"""
        if self.current_step >= min(len(self.price_data), len(self.signal_data)):
            return np.zeros(60, dtype=np.float32)
        
        # 1. ê°€ê²© íŠ¹ì„± (20ê°œ)
        price_features = self._extract_price_features(self.current_step)
        
        # 2. ì‹ í˜¸ íŠ¹ì„± (30ê°œ)
        signal_features = self._extract_signal_features(self.signal_data[self.current_step])
        
        # 3. í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ (10ê°œ)
        portfolio_features = self._get_portfolio_state()
        
        # ëª¨ë“  íŠ¹ì„± ê²°í•©
        observation = np.concatenate([
            price_features,      # 20ê°œ
            signal_features,     # 30ê°œ  
            portfolio_features   # 10ê°œ
        ])
        
        return observation.astype(np.float32)
    
    def _extract_price_features(self, idx: int) -> np.ndarray:
        """ê°€ê²© íŠ¹ì„± ì¶”ì¶œ"""
        if idx < 20:
            return np.zeros(20, dtype=np.float32)
        
        recent_data = self.price_data.iloc[max(0, idx-19):idx+1]
        features = []
        
        if len(recent_data) == 0:
            return np.zeros(20, dtype=np.float32)
        
        close = recent_data['close']
        high = recent_data['high']
        low = recent_data['low']
        volume = recent_data['volume']
        
        # ìˆ˜ìµë¥  íŠ¹ì„± (4ê°œ)
        returns = close.pct_change().fillna(0)
        features.extend([
            returns.mean(),
            returns.std(),
            returns.iloc[-1] if len(returns) > 0 else 0.0,
            returns.tail(5).mean() if len(returns) >= 5 else 0.0,
        ])
        
        # RSI (1ê°œ)
        if len(close) >= 14:
            delta = close.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14, min_periods=1).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14, min_periods=1).mean()
            rs = gain / (loss + 1e-8)
            rsi = 100 - (100 / (1 + rs))
            features.append(rsi.iloc[-1] / 100.0 if not pd.isna(rsi.iloc[-1]) else 0.5)
        else:
            features.append(0.5)
        
        # ë³¼ë¦°ì € ë°´ë“œ ìœ„ì¹˜ (1ê°œ)
        if len(close) >= 20:
            sma = close.rolling(window=20, min_periods=1).mean()
            std = close.rolling(window=20, min_periods=1).std()
            bb_upper = sma + (std * 2)
            bb_lower = sma - (std * 2)
            bb_width = bb_upper.iloc[-1] - bb_lower.iloc[-1]
            if bb_width > 0:
                bb_position = (close.iloc[-1] - bb_lower.iloc[-1]) / bb_width
            else:
                bb_position = 0.5
            features.append(bb_position)
        else:
            features.append(0.5)
        
        # ì´ë™í‰ê·  ë¹„ìœ¨ (3ê°œ)
        for window in [5, 10, 20]:
            if len(close) >= window:
                ma = close.rolling(window=window, min_periods=1).mean()
                ma_ratio = (close.iloc[-1] / ma.iloc[-1] - 1) if ma.iloc[-1] > 0 else 0.0
                features.append(ma_ratio)
            else:
                features.append(0.0)
        
        # ê±°ë˜ëŸ‰ ë° ë³€ë™ì„± (2ê°œ)
        if len(volume) > 1:
            vol_ratio = (volume.iloc[-1] / volume.mean() - 1) if volume.mean() > 0 else 0.0
            features.append(vol_ratio)
        else:
            features.append(0.0)
            
        if len(high) > 0 and len(low) > 0 and len(close) > 0:
            price_volatility = (high.iloc[-1] - low.iloc[-1]) / close.iloc[-1] if close.iloc[-1] > 0 else 0.0
            features.append(price_volatility)
        else:
            features.append(0.0)
        
        # ë‚˜ë¨¸ì§€ íŠ¹ì„±ë“¤ë¡œ 20ê°œ ë§ì¶”ê¸°
        while len(features) < 20:
            features.append(0.0)
        
        return np.array(features[:20], dtype=np.float32)
    
    def _extract_signal_features(self, signals: Dict) -> np.ndarray:
        """ì‹ í˜¸ íŠ¹ì„± ì¶”ì¶œ"""
        features = []
        
        # ê° ì‹œê°„ëŒ€ë³„ ì‹ í˜¸ (3ê°œ Ã— 8ê°œ = 24ê°œ)
        for category in ['SHORT_TERM', 'MEDIUM_TERM', 'LONG_TERM']:
            if category in signals['decisions']:
                decision = signals['decisions'][category]
                
                # ì•¡ì…˜ ê°•ë„
                action = decision.get('action', 'HOLD')
                action_strength = 1.0 if action == 'LONG' else (-1.0 if action == 'SHORT' else 0.0)
                
                features.extend([
                    action_strength,
                    float(decision.get('net_score', 0.0)),
                    min(float(decision.get('leverage', 1)) / 10.0, 2.0),  # ì •ê·œí™”
                    min(float(decision.get('max_holding_minutes', 60)) / 1440.0, 1.0),  # ì •ê·œí™”
                ])
                
                # ì‹ ë¢°ë„ ë° ì ìˆ˜
                meta = decision.get('meta', {}).get('synergy_meta', {})
                confidence = meta.get('confidence', 'LOW')
                confidence_score = 1.0 if confidence == 'HIGH' else (0.5 if confidence == 'MEDIUM' else 0.0)
                
                features.extend([
                    confidence_score,
                    float(meta.get('buy_score', 0.0)),
                    float(meta.get('sell_score', 0.0)),
                    len(meta.get('conflicts_detected', [])) / 5.0  # ì •ê·œí™”
                ])
            else:
                features.extend([0.0] * 8)
        
        # ê°ˆë“± ë° ë©”íƒ€ ì •ë³´ (6ê°œ)
        conflicts = signals.get('conflicts', {})
        features.extend([
            1.0 if conflicts.get('has_conflicts', False) else 0.0,
            len(conflicts.get('long_categories', [])) / 3.0,
            len(conflicts.get('short_categories', [])) / 3.0,
            float(signals.get('meta', {}).get('active_positions', 0)) / 3.0,
            0.0,  # ì˜ˆë¹„
            0.0   # ì˜ˆë¹„
        ])
        
        return np.array(features[:30], dtype=np.float32)
    
    def _get_portfolio_state(self) -> np.ndarray:
        """í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ"""
        features = [
            self.current_position,
            self.current_leverage / 20.0,
            (self.balance - self.initial_balance) / self.initial_balance,
            self.unrealized_pnl / self.initial_balance if self.initial_balance > 0 else 0.0,
            min(self.total_trades / 100.0, 1.0),
            self.winning_trades / max(self.total_trades, 1),
            self.max_drawdown,
            min(self.consecutive_losses / 10.0, 1.0),
            min(self.holding_time / 1440.0, 1.0),
            1.0 if self.in_position else 0.0
        ]
        return np.array(features, dtype=np.float32)
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:
        """ìŠ¤í… ì‹¤í–‰"""
        if self.current_step >= min(len(self.price_data), len(self.signal_data)) - 1:
            return self._get_observation(), 0.0, True, {}
        
        # ì•¡ì…˜ í•´ì„
        position_change = np.clip(action[0], -2.0, 2.0)
        leverage = np.clip(action[1], 1.0, 20.0)
        target_holding_minutes = np.clip(action[2], 1.0, 1440.0)
        
        # í˜„ì¬ ê°€ê²©
        current_price = self.price_data.iloc[self.current_step]['close']
        next_price = self.price_data.iloc[self.current_step + 1]['close']
        
        # í¬ì§€ì…˜ ë³€ê²½ì‚¬í•­ ì²˜ë¦¬
        trade_completed = False
        old_position = self.current_position
        
        # í¬ì§€ì…˜ ì—…ë°ì´íŠ¸
        self._update_position(position_change, leverage, current_price, target_holding_minutes)
        
        # ê±°ë˜ ì™„ë£Œ í™•ì¸
        if abs(old_position) > 0.01 and abs(self.current_position) < 0.01:
            trade_completed = True
            self.last_trade_pnl = self._calculate_trade_pnl(current_price, self.entry_price, old_position)
        
        # ê°œì„ ëœ ë³´ìƒ ê³„ì‚°
        reward = self.reward_function.calculate_reward(
            current_price=next_price,
            entry_price=self.entry_price,
            position=self.current_position,
            action='TRADE' if abs(position_change) > 0.1 else 'HOLD',
            holding_time=self.holding_time,
            volatility=self._calculate_volatility(),
            volume_ratio=self._calculate_volume_ratio(),
            trade_pnl=self.last_trade_pnl if trade_completed else None
        )
        
        # ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ
        self.current_step += 1
        self.holding_time += 3  # 3ë¶„ ì¦ê°€
        
        # í™€ë”© ì‹œê°„ ì´ˆê³¼ì‹œ ê°•ì œ ì²­ì‚°
        if self.in_position and self.holding_time >= target_holding_minutes:
            self._close_position(next_price)
        
        # ë‹¤ìŒ ìƒíƒœ
        next_state = self._get_observation()
        
        # ì¢…ë£Œ ì¡°ê±´
        done = (self.current_step >= min(len(self.price_data), len(self.signal_data)) - 1 or 
                self.balance <= self.initial_balance * 0.1)
        
        info = {
            'balance': self.balance,
            'position': self.current_position,
            'unrealized_pnl': self.unrealized_pnl,
            'total_trades': self.total_trades,
            'win_rate': self.winning_trades / max(self.total_trades, 1),
            'current_price': next_price,
            'entry_price': self.entry_price,
            'holding_time': self.holding_time,
            'volatility': self._calculate_volatility(),
            'volume_ratio': self._calculate_volume_ratio(),
            'trade_completed': trade_completed,
            'trade_pnl': self.last_trade_pnl if trade_completed else None
        }
        
        return next_state, reward, done, info
    
    def _calculate_volatility(self):
        """í˜„ì¬ ë³€ë™ì„± ê³„ì‚°"""
        if self.current_step < 20:
            return 0.02
        recent_data = self.price_data.iloc[max(0, self.current_step-20):self.current_step+1]
        if len(recent_data) < 2:
            return 0.02
        returns = recent_data['close'].pct_change().dropna()
        return returns.std() if len(returns) > 1 else 0.02
    
    def _calculate_volume_ratio(self):
        """ê±°ë˜ëŸ‰ ë¹„ìœ¨ ê³„ì‚°"""
        if self.current_step < 20:
            return 1.0
        recent_volume = self.price_data.iloc[max(0, self.current_step-20):self.current_step+1]['volume']
        if len(recent_volume) < 2:
            return 1.0
        current_volume = self.price_data.iloc[self.current_step]['volume']
        avg_volume = recent_volume.mean()
        return current_volume / avg_volume if avg_volume > 0 else 1.0
    
    def _calculate_trade_pnl(self, exit_price, entry_price, position):
        """ê±°ë˜ ì†ìµ ê³„ì‚°"""
        if entry_price <= 0:
            return 0.0
        if position > 0:  # Long
            return (exit_price - entry_price) / entry_price
        else:  # Short
            return (entry_price - exit_price) / entry_price
    
    def _update_position(self, position_change, leverage, current_price, target_holding_minutes):
        """í¬ì§€ì…˜ ì—…ë°ì´íŠ¸"""
        new_position = np.clip(self.current_position + position_change, -1.0, 1.0)
        
        if abs(new_position - self.current_position) > 0.01:
            # ê¸°ì¡´ í¬ì§€ì…˜ ì²­ì‚°
            if abs(self.current_position) > 0.01:
                self._close_position(current_price)
            
            # ìƒˆ í¬ì§€ì…˜ ì§„ì…
            if abs(new_position) > 0.01:
                self.current_position = new_position
                self.current_leverage = leverage
                self.entry_price = current_price
                self.holding_time = 0
                self.in_position = True
    
    def _close_position(self, exit_price):
        """í¬ì§€ì…˜ ì²­ì‚°"""
        if abs(self.current_position) < 0.01:
            return
        
        # PnL ê³„ì‚°
        pnl = self._calculate_trade_pnl(exit_price, self.entry_price, self.current_position)
        pnl_usd = pnl * self.current_leverage * self.balance
        
        # ê±°ë˜ ìˆ˜ìˆ˜ë£Œ ì°¨ê° (0.1%)
        fee = abs(pnl_usd) * 0.001
        pnl_usd -= fee
        
        # ì”ê³  ì—…ë°ì´íŠ¸
        self.balance += pnl_usd
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.total_trades += 1
        if pnl_usd > 0:
            self.winning_trades += 1
            self.consecutive_losses = 0
        else:
            self.consecutive_losses += 1
        
        # ìµœëŒ€ ë‚™í­ ì—…ë°ì´íŠ¸
        if self.balance > self.peak_balance:
            self.peak_balance = self.balance
        else:
            drawdown = (self.peak_balance - self.balance) / self.peak_balance
            self.max_drawdown = max(self.max_drawdown, drawdown)
        
        # í¬ì§€ì…˜ ì´ˆê¸°í™”
        self.current_position = 0.0
        self.unrealized_pnl = 0.0
        self.in_position = False
        self.holding_time = 0
        self.last_trade_pnl = pnl

# =================================================================
# 4. ê°œì„ ëœ RL ì—ì´ì „íŠ¸
# =================================================================

class ImprovedCryptoRLAgent:
    """ê°œì„ ëœ ì•”í˜¸í™”í ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸"""
    
    def __init__(self, state_size: int, learning_rate: float = 5e-5, 
                 gamma: float = 0.995, epsilon: float = 0.9, epsilon_decay: float = 0.9995):
        
        self.state_size = state_size
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = 0.05
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")
        
        # ë„¤íŠ¸ì›Œí¬
        self.q_network = ImprovedDQNNetwork(state_size, 3).to(self.device)
        self.target_network = ImprovedDQNNetwork(state_size, 3).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # ê²½í—˜ ë¦¬í”Œë ˆì´
        self.memory = deque(maxlen=100000)
        self.batch_size = 128
        
        # í•™ìŠµ ì¶”ì 
        self.training_rewards = []
        self.losses = []
        self.win_rates = []
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
        self.target_update_freq = 1000
        self.update_count = 0
    
    def remember(self, state, action, reward, next_state, done):
        """ê²½í—˜ ì €ì¥"""
        self.memory.append(Experience(state, action, reward, next_state, done))
    
    def act(self, state: np.ndarray) -> np.ndarray:
        """ì•¡ì…˜ ì„ íƒ"""
        if np.random.random() <= self.epsilon:
            # ìŠ¤ë§ˆíŠ¸í•œ ëœë¤ ì•¡ì…˜
            return np.array([
                np.random.uniform(-1.0, 1.0),    # í¬ì§€ì…˜ ë³€ê²½
                np.random.uniform(1.0, 5.0),     # ë ˆë²„ë¦¬ì§€ (ë³´ìˆ˜ì )
                np.random.uniform(30.0, 180.0)   # í™€ë”© ì‹œê°„
            ])
        
        # Q-ê°’ ê¸°ë°˜ ì•¡ì…˜ ì„ íƒ
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            position_q, leverage_q, holding_q = self.q_network(state_tensor)
            
            position_idx = torch.argmax(position_q).item()
            leverage_idx = torch.argmax(leverage_q).item()
            holding_idx = torch.argmax(holding_q).item()
            
            # ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€í™˜
            position = -2.0 + (position_idx * 0.2)
            leverage = 1.0 + leverage_idx
            holding = 30.0 + (holding_idx * 30.0)
            
            return np.array([position, leverage, holding])
    
    def replay(self):
        """ê²½í—˜ ë¦¬í”Œë ˆì´ í•™ìŠµ"""
        if len(self.memory) < self.batch_size * 2:
            return
        
        batch = random.sample(self.memory, self.batch_size)
        states = torch.FloatTensor([e.state for e in batch]).to(self.device)
        actions = [e.action for e in batch]
        rewards = torch.FloatTensor([e.reward for e in batch]).to(self.device)
        next_states = torch.FloatTensor([e.next_state for e in batch]).to(self.device)
        # numpy.bool_ë¡œ ì¸í•œ íƒ€ì… ë¬¸ì œ ë°©ì§€: íŒŒì´ì¬ bool ë¦¬ìŠ¤íŠ¸ë¡œ ìœ ì§€ (ë¡œì§ ë¶ˆë³€)
        dones = [bool(e.done) for e in batch]
        
        # í˜„ì¬ Qê°’ë“¤
        current_position_q, current_leverage_q, current_holding_q = self.q_network(states)
        
        # íƒ€ê²Ÿ Qê°’ë“¤
        with torch.no_grad():
            next_position_q, next_leverage_q, next_holding_q = self.target_network(next_states)
            
            target_position_q = current_position_q.clone()
            target_leverage_q = current_leverage_q.clone()
            target_holding_q = current_holding_q.clone()
            
            for i, (action, reward, done) in enumerate(zip(actions, rewards, dones)):
                if not done:
                    # ì•¡ì…˜ì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
                    pos_idx = int(np.clip((action[0] + 2.0) / 0.2, 0, 20))
                    lev_idx = int(np.clip(action[1] - 1, 0, 19))
                    hold_idx = int(np.clip((action[2] - 30.0) / 30.0, 0, 47))
                    
                    # Double DQN ì ìš©
                    target_position_q[i, pos_idx] = reward + self.gamma * torch.max(next_position_q[i])
                    target_leverage_q[i, lev_idx] = reward + self.gamma * torch.max(next_leverage_q[i])
                    target_holding_q[i, hold_idx] = reward + self.gamma * torch.max(next_holding_q[i])
                else:
                    pos_idx = int(np.clip((action[0] + 2.0) / 0.2, 0, 20))
                    lev_idx = int(np.clip(action[1] - 1, 0, 19))
                    hold_idx = int(np.clip((action[2] - 30.0) / 30.0, 0, 47))
                    
                    target_position_q[i, pos_idx] = reward
                    target_leverage_q[i, lev_idx] = reward
                    target_holding_q[i, hold_idx] = reward
        
        # ì†ì‹¤ ê³„ì‚°
        pos_loss = F.smooth_l1_loss(current_position_q, target_position_q)
        lev_loss = F.smooth_l1_loss(current_leverage_q, target_leverage_q)
        hold_loss = F.smooth_l1_loss(current_holding_q, target_holding_q)
        
        total_loss = pos_loss + lev_loss + hold_loss
        
        # ì—­ì „íŒŒ
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()
        
        self.losses.append(total_loss.item())
        
        # ì—¡ì‹¤ë¡  ê°ì†Œ
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
        self.update_count += 1
        if self.update_count % self.target_update_freq == 0:
            self.update_target_network()
    
    def update_target_network(self):
        """íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def safe_save_model(self, filepath: str):
        """ì•ˆì „í•œ ëª¨ë¸ ì €ì¥"""
        try:
            os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)
            
            save_dict = {
                'q_network': self.q_network.state_dict(),
                'target_network': self.target_network.state_dict(),
                'optimizer': self.optimizer.state_dict(),
                'epsilon': float(self.epsilon),
                'training_rewards': [float(r) for r in self.training_rewards],
                'losses': [float(l) for l in self.losses],
                'win_rates': [float(w) for w in self.win_rates],
                'update_count': int(self.update_count),
                'state_size': int(self.state_size)
            }
            
            torch.save(save_dict, filepath)
            print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def safe_load_model(self, filepath: str):
        """ì•ˆì „í•œ ëª¨ë¸ ë¡œë“œ"""
        if not os.path.exists(filepath):
            print(f"âš ï¸  ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {filepath}")
            return False
        
        try:
            # PyTorch 2.6 í˜¸í™˜ ë¡œë”©
            try:
                checkpoint = torch.load(filepath, map_location=self.device, weights_only=True)
            except:
                checkpoint = torch.load(filepath, map_location=self.device, weights_only=False)
            
            self.q_network.load_state_dict(checkpoint['q_network'])
            self.target_network.load_state_dict(checkpoint['target_network'])
            self.optimizer.load_state_dict(checkpoint['optimizer'])
            self.epsilon = checkpoint.get('epsilon', self.epsilon)
            self.training_rewards = checkpoint.get('training_rewards', [])
            self.losses = checkpoint.get('losses', [])
            self.win_rates = checkpoint.get('win_rates', [])
            self.update_count = checkpoint.get('update_count', 0)
            
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ! ì—¡ì‹¤ë¡ : {self.epsilon:.3f}")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

# =================================================================
# 5. ë°ì´í„° ë¡œë”© í•¨ìˆ˜ë“¤
# =================================================================

def load_price_data():
    """ê°€ê²© ë°ì´í„° ë¡œë“œ"""
    try:
        required_columns = ['open', 'high', 'low', 'close', 'volume', 'quote_volume']
        
        df_3m = pd.read_csv('data/ETHUSDC_3m_historical_data.csv')
        df_3m['timestamp'] = pd.to_datetime(df_3m['timestamp'])
        df_3m = df_3m.set_index('timestamp')
        df_3m = df_3m[required_columns]
        
        price_data = df_3m.reset_index()
        print(f"âœ… ê°€ê²© ë°ì´í„° ë¡œë“œ: {len(price_data):,}ê°œ ìº”ë“¤")
        return price_data
        
    except Exception as e:
        print(f"âŒ ê°€ê²© ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def load_signal_data():
    """ì‹ í˜¸ ë°ì´í„° ë¡œë“œ (Parquet ë˜ëŠ” ìƒì„±)"""
    
    # 1. Parquet íŒŒì¼ ì°¾ê¸°
    agent_folder = Path("agent")
    parquet_files = []
    
    if agent_folder.exists():
        parquet_files = list(agent_folder.glob("*.parquet"))
    
    if parquet_files:
        try:
            print(f"ğŸ“– ì‹ í˜¸ ë°ì´í„° ë¡œë“œ ì¤‘: {parquet_files[0].name}")
            signal_df = pd.read_parquet(parquet_files[0])
            print(f"âœ… ì‹ í˜¸ ë°ì´í„° ë¡œë“œ: {len(signal_df):,}ê°œ ë ˆì½”ë“œ")
            
            # ê°„ë‹¨í•œ ë³€í™˜
            signal_data = convert_parquet_to_signals(signal_df)
            return signal_data
            
        except Exception as e:
            print(f"âŒ Parquet ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 2. Parquet íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì‹ í˜¸ ìƒì„±
    print("âš ï¸  Parquet íŒŒì¼ì´ ì—†ì–´ ê¸°ë³¸ ì‹ í˜¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
    return None

def convert_parquet_to_signals(signal_df):
    """Parquetì„ ì‹ í˜¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    signal_data = []
    
    print("ğŸ”„ ì‹ í˜¸ ë°ì´í„° ë³€í™˜ ì¤‘...")
    
    for idx, row in signal_df.iterrows():
        signal_dict = {
            'decisions': {},
            'conflicts': {'has_conflicts': False, 'long_categories': [], 'short_categories': []},
            'meta': {'active_positions': 0}
        }
        
        # ê° ì‹œê°„ëŒ€ë³„ ì‹ í˜¸ ì¶”ì¶œ
        for category in ['SHORT_TERM', 'MEDIUM_TERM', 'LONG_TERM']:
            prefix = f"{category.lower()}_"
            
            action = row.get(f'{prefix}action', 'HOLD')
            net_score = row.get(f'{prefix}net_score', 0.0)
            confidence = row.get(f'{prefix}confidence', 'LOW')
            leverage = row.get(f'{prefix}leverage', 1)
            max_holding = row.get(f'{prefix}max_holding_minutes', 60 if category == 'SHORT_TERM' else (240 if category == 'MEDIUM_TERM' else 1440))
            buy_score = row.get(f'{prefix}buy_score', 0.0)
            sell_score = row.get(f'{prefix}sell_score', 0.0)
            
            signal_dict['decisions'][category] = {
                'action': action,
                'net_score': float(net_score) if pd.notna(net_score) else 0.0,
                'leverage': int(leverage) if pd.notna(leverage) else 1,
                'max_holding_minutes': int(max_holding) if pd.notna(max_holding) else (60 if category == 'SHORT_TERM' else (240 if category == 'MEDIUM_TERM' else 1440)),
                'raw': {},
                'meta': {
                    'synergy_meta': {
                        'confidence': confidence if pd.notna(confidence) else 'LOW',
                        'buy_score': float(buy_score) if pd.notna(buy_score) else 0.0,
                        'sell_score': float(sell_score) if pd.notna(sell_score) else 0.0,
                        'conflicts_detected': []
                    }
                }
            }
            
            # ê°ˆë“± ì •ë³´
            if action == 'LONG':
                signal_dict['conflicts']['long_categories'].append(category)
            elif action == 'SHORT':
                signal_dict['conflicts']['short_categories'].append(category)
        
        # ê°ˆë“± ì—¬ë¶€
        if (len(signal_dict['conflicts']['long_categories']) > 0 and 
            len(signal_dict['conflicts']['short_categories']) > 0):
            signal_dict['conflicts']['has_conflicts'] = True
        
        signal_data.append(signal_dict)
        
        if (idx + 1) % 5000 == 0:
            print(f"   ë³€í™˜ ì§„í–‰: {idx + 1:,}/{len(signal_df):,}")
    
    print(f"âœ… ì‹ í˜¸ ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(signal_data):,}ê°œ")
    return signal_data

def generate_basic_signals(length):
    """ê¸°ë³¸ ì‹ í˜¸ ë°ì´í„° ìƒì„± (Parquetì´ ì—†ì„ ë•Œ)"""
    print(f"ğŸ”„ ê¸°ë³¸ ì‹ í˜¸ ë°ì´í„° ìƒì„± ì¤‘: {length:,}ê°œ")
    
    signal_data = []
    for i in range(length):
        # RSI ê¸°ë°˜ ê°„ë‹¨í•œ ì‹ í˜¸ ìƒì„±
        rsi_value = 30 + (i % 40)  # 30~70 ì‚¬ì´ ìˆœí™˜
        
        if rsi_value > 60:
            short_action = 'SHORT'
            short_score = (rsi_value - 60) / 10
        elif rsi_value < 40:
            short_action = 'LONG' 
            short_score = (40 - rsi_value) / 10
        else:
            short_action = 'HOLD'
            short_score = 0.0
        
        signal_dict = {
            'decisions': {
                'SHORT_TERM': {
                    'action': short_action,
                    'net_score': short_score,
                    'leverage': 1 + int(short_score * 3),  # 1~4ë°°
                    'max_holding_minutes': 60,
                    'raw': {},
                    'meta': {'synergy_meta': {'confidence': 'MEDIUM' if short_score > 0.5 else 'LOW', 'buy_score': short_score if short_action == 'LONG' else 0.0, 'sell_score': short_score if short_action == 'SHORT' else 0.0, 'conflicts_detected': []}}
                },
                'MEDIUM_TERM': {
                    'action': 'HOLD',
                    'net_score': 0.0,
                    'leverage': 1,
                    'max_holding_minutes': 240,
                    'raw': {},
                    'meta': {'synergy_meta': {'confidence': 'LOW', 'buy_score': 0.0, 'sell_score': 0.0, 'conflicts_detected': []}}
                },
                'LONG_TERM': {
                    'action': 'HOLD',
                    'net_score': 0.0,
                    'leverage': 1,
                    'max_holding_minutes': 1440,
                    'raw': {},
                    'meta': {'synergy_meta': {'confidence': 'LOW', 'buy_score': 0.0, 'sell_score': 0.0, 'conflicts_detected': []}}
                }
            },
            'conflicts': {'has_conflicts': False, 'long_categories': [], 'short_categories': []},
            'meta': {'active_positions': 0}
        }
        
        signal_data.append(signal_dict)
    
    print(f"âœ… ê¸°ë³¸ ì‹ í˜¸ ë°ì´í„° ìƒì„± ì™„ë£Œ")
    return signal_data

# =================================================================
# 6. ì„±ëŠ¥ ë¶„ì„ í´ë˜ìŠ¤
# =================================================================

class PerformanceAnalyzer:
    """ì„±ëŠ¥ ë¶„ì„ ë° í‰ê°€"""
    
    @staticmethod
    def evaluate_agent(agent, env, num_episodes=10):
        """ì—ì´ì „íŠ¸ ì„±ëŠ¥ í‰ê°€"""
        print(f"ğŸ” ì—ì´ì „íŠ¸ ì„±ëŠ¥ í‰ê°€ ì¤‘ ({num_episodes} ì—í”¼ì†Œë“œ)...")
        
        original_epsilon = agent.epsilon
        agent.epsilon = 0.0  # íƒí—˜ ë¹„í™œì„±í™”
        
        results = []
        all_trades = []
        
        for episode in range(num_episodes):
            state = env.reset()
            episode_reward = 0
            episode_trades = []
            episode_balance = env.initial_balance
            
            for step in range(500):  # ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ 500 ìŠ¤í…
                action = agent.act(state)
                next_state, reward, done, info = env.step(action)
                
                episode_reward += reward
                episode_balance = info['balance']
                
                if info.get('trade_completed', False):
                    trade_pnl = info.get('trade_pnl', 0.0)
                    episode_trades.append(1 if trade_pnl > 0 else 0)
                
                state = next_state
                if done:
                    break
            
            # ì—í”¼ì†Œë“œ ê²°ê³¼
            episode_return = (episode_balance - env.initial_balance) / env.initial_balance
            win_rate = np.mean(episode_trades) if episode_trades else 0.0
            
            results.append({
                'episode': episode,
                'total_reward': episode_reward,
                'final_balance': episode_balance,
                'return': episode_return,
                'win_rate': win_rate,
                'total_trades': len(episode_trades),
                'max_drawdown': info.get('max_drawdown', 0.0)
            })
            
            all_trades.extend(episode_trades)
        
        # ì›ë˜ epsilon ë³µì›
        agent.epsilon = original_epsilon
        
        # í†µí•© ì„±ëŠ¥ ì§€í‘œ
        overall_stats = {
            'avg_return': np.mean([r['return'] for r in results]),
            'avg_reward': np.mean([r['total_reward'] for r in results]),
            'overall_win_rate': np.mean(all_trades) if all_trades else 0.0,
            'avg_trades_per_episode': np.mean([r['total_trades'] for r in results]),
            'avg_max_drawdown': np.mean([r['max_drawdown'] for r in results]),
            'consistency': 1.0 - np.std([r['return'] for r in results]) if len(results) > 1 else 1.0,
            'total_trades': len(all_trades)
        }
        
        return results, overall_stats
    
    @staticmethod
    def print_performance_report(results, stats):
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š ì„±ëŠ¥ í‰ê°€ ê²°ê³¼")
        print("="*60)
        print(f"ğŸ¯ ì „ì²´ ìŠ¹ë¥ : {stats['overall_win_rate']:.3f}")
        print(f"ğŸ’° í‰ê·  ìˆ˜ìµë¥ : {stats['avg_return']:.3f} ({stats['avg_return']*100:.1f}%)")
        print(f"ğŸ† í‰ê·  ë¦¬ì›Œë“œ: {stats['avg_reward']:.1f}")
        print(f"ğŸ“ˆ ì—í”¼ì†Œë“œë‹¹ í‰ê·  ê±°ë˜ ìˆ˜: {stats['avg_trades_per_episode']:.1f}")
        print(f"ğŸ“‰ í‰ê·  ìµœëŒ€ ë‚™í­: {stats['avg_max_drawdown']:.3f}")
        print(f"ğŸ² ì„±ê³¼ ì¼ê´€ì„±: {stats['consistency']:.3f}")
        print(f"ğŸ”¢ ì´ ê±°ë˜ ìˆ˜: {stats['total_trades']}")
        
        # ì„±ëŠ¥ ë“±ê¸‰
        grade = PerformanceAnalyzer.get_performance_grade(stats)
        print(f"\nğŸ¯ ì„±ëŠ¥ ë“±ê¸‰: {grade}")
        
        # ê°œì„  ì œì•ˆ
        recommendations = PerformanceAnalyzer.get_recommendations(stats)
        print("\nğŸ’¡ ê°œì„  ì œì•ˆ:")
        for rec in recommendations:
            print(f"  â€¢ {rec}")
    
    @staticmethod
    def get_performance_grade(stats):
        """ì„±ëŠ¥ ë“±ê¸‰ ë¶€ì—¬"""
        win_rate = stats['overall_win_rate']
        avg_return = stats['avg_return']
        consistency = stats['consistency']
        
        score = 0
        if win_rate >= 0.65: score += 3
        elif win_rate >= 0.6: score += 2
        elif win_rate >= 0.55: score += 1
        
        if avg_return >= 0.15: score += 3
        elif avg_return >= 0.05: score += 2
        elif avg_return >= 0.0: score += 1
        
        if consistency >= 0.8: score += 2
        elif consistency >= 0.6: score += 1
        
        grades = {8: "A+ (ìš°ìˆ˜)", 7: "A (ì¢‹ìŒ)", 6: "B+ (ì–‘í˜¸)", 5: "B (ë³´í†µ)", 
                 4: "C+ (ë¯¸í¡)", 3: "C (ê°œì„ í•„ìš”)", 2: "D (ë‚˜ì¨)", 1: "F (ë§¤ìš°ë‚˜ì¨)", 0: "F (ì‹¤íŒ¨)"}
        
        return grades.get(score, "F (ì‹¤íŒ¨)")
    
    @staticmethod
    def get_recommendations(stats):
        """ì„±ëŠ¥ ê¸°ë°˜ ê°œì„  ì œì•ˆ"""
        recommendations = []
        
        if stats['overall_win_rate'] < 0.55:
            recommendations.append("ìŠ¹ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ë” ê¸´ í›ˆë ¨ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if stats['avg_return'] < 0.02:
            recommendations.append("ìˆ˜ìµë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ë¦¬ìŠ¤í¬-ë¦¬ì›Œë“œ ë¹„ìœ¨ì„ ê°œì„ í•˜ì„¸ìš”.")
        
        if stats['avg_max_drawdown'] > 0.2:
            recommendations.append("ìµœëŒ€ ë‚™í­ì´ í½ë‹ˆë‹¤. í¬ì§€ì…˜ í¬ê¸°ë¥¼ ì¤„ì´ì„¸ìš”.")
        
        if stats['consistency'] < 0.5:
            recommendations.append("ì„±ê³¼ ì¼ê´€ì„±ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤. ë” ë§ì€ í›ˆë ¨ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if stats['avg_trades_per_episode'] < 3:
            recommendations.append("ê±°ë˜ ë¹ˆë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ì‹ í˜¸ ê°ë„ë¥¼ ë†’ì—¬ë³´ì„¸ìš”.")
        
        if not recommendations:
            recommendations.append("ì „ë°˜ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì…ë‹ˆë‹¤!")
        
        return recommendations

# =================================================================
# 7. í›ˆë ¨ í•¨ìˆ˜
# =================================================================

def train_agent(agent, env, episodes=500, save_interval=100):
    """ì—ì´ì „íŠ¸ í›ˆë ¨"""
    print(f"ğŸš€ ê°•í™”í•™ìŠµ í›ˆë ¨ ì‹œì‘ ({episodes} ì—í”¼ì†Œë“œ)")
    
    episode_rewards = []
    episode_win_rates = []
    best_win_rate = 0.0
    
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        episode_trades = []
        steps = 0
        
        while steps < 500:  # ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ 500 ìŠ¤í…
            action = agent.act(state)
            next_state, reward, done, info = env.step(action)
            
            agent.remember(state, action, reward, next_state, done)
            
            if info.get('trade_completed', False):
                trade_pnl = info.get('trade_pnl', 0.0)
                episode_trades.append(1 if trade_pnl > 0 else 0)
            
            state = next_state
            total_reward += reward
            steps += 1
            
            # í•™ìŠµ
            if len(agent.memory) > agent.batch_size * 2:
                agent.replay()
            
            if done:
                break
        
        episode_rewards.append(total_reward)
        episode_win_rate = np.mean(episode_trades) if episode_trades else 0.0
        episode_win_rates.append(episode_win_rate)
        
        agent.training_rewards.append(total_reward)
        agent.win_rates.append(episode_win_rate)
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if episode % 10 == 0 or episode < 10:
            recent_rewards = episode_rewards[-50:] if len(episode_rewards) >= 50 else episode_rewards
            recent_win_rates = episode_win_rates[-50:] if len(episode_win_rates) >= 50 else episode_win_rates
            
            avg_reward = np.mean(recent_rewards)
            avg_win_rate = np.mean(recent_win_rates)
            
            print(f"Episode {episode:4d} | "
                  f"ìŠ¹ë¥ : {avg_win_rate:.3f} | "
                  f"ë¦¬ì›Œë“œ: {avg_reward:7.1f} | "
                  f"ì”ê³ : ${info['balance']:7.0f} | "
                  f"Îµ: {agent.epsilon:.3f}")
        
        # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥
        if episode % save_interval == 0 and episode > 0:
            current_avg_win_rate = np.mean(episode_win_rates[-100:]) if len(episode_win_rates) >= 100 else np.mean(episode_win_rates)
            
            if current_avg_win_rate > best_win_rate:
                best_win_rate = current_avg_win_rate
                agent.safe_save_model(f'best_model_ep{episode}_wr{current_avg_win_rate:.3f}.pth')
                print(f"âœ… ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! ìŠ¹ë¥ : {current_avg_win_rate:.3f}")
        
        # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´
        if episode > 200:
            recent_100_win_rate = np.mean(episode_win_rates[-100:])
            if recent_100_win_rate >= 0.65:
                print(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±! ìŠ¹ë¥  {recent_100_win_rate:.3f} ë„ë‹¬")
                agent.safe_save_model('agent/final_optimized_model.pth')
                break
    
    print(f"\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
    print(f"   ì´ ì—í”¼ì†Œë“œ: {episode + 1}")
    print(f"   ìµœê³  ìŠ¹ë¥ : {best_win_rate:.3f}")
    print(f"   ìµœì¢… ìŠ¹ë¥ : {np.mean(episode_win_rates[-50:]) if episode_win_rates else 0:.3f}")
    
    return agent, episode_rewards, episode_win_rates

# =================================================================
# 8. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# =================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ì™„ì „ ë…ë¦½í˜• ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    try:
        # 1. ë°ì´í„° ë¡œë“œ
        print("\n1ï¸âƒ£ ë°ì´í„° ë¡œë”©...")
        price_data = load_price_data()
        if price_data is None:
            print("âŒ ê°€ê²© ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
            return
        
        signal_data = load_signal_data()
        if signal_data is None:
            # ê¸°ë³¸ ì‹ í˜¸ ìƒì„±
            signal_data = generate_basic_signals(min(len(price_data), 10000))  # ìµœëŒ€ 10,000ê°œ
        
        # ë°ì´í„° ê¸¸ì´ ë§ì¶”ê¸°
        min_length = min(len(price_data), len(signal_data))
        price_data = price_data.iloc[:min_length].reset_index(drop=True)
        signal_data = signal_data[:min_length]
        
        print(f"âœ… ìµœì¢… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {min_length:,}ê°œ")
        
        # 2. í™˜ê²½ ë° ì—ì´ì „íŠ¸ ìƒì„±
        print("\n2ï¸âƒ£ í™˜ê²½ ë° ì—ì´ì „íŠ¸ ìƒì„±...")
        env = ImprovedTradingEnvironment(price_data, signal_data)
        agent = ImprovedCryptoRLAgent(env.observation_space.shape[0])
        
        # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹œë„
        model_files = ['agent/final_optimized_model.pth', 'agent/improved_crypto_rl_model.pth']
        model_loaded = False
        
        for model_file in model_files:
            if agent.safe_load_model(model_file):
                model_loaded = True
                break
        
        if not model_loaded:
            print("â„¹ï¸  ìƒˆë¡œìš´ ëª¨ë¸ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        # 3. í˜„ì¬ ì„±ëŠ¥ í‰ê°€
        print("\n3ï¸âƒ£ í˜„ì¬ ì„±ëŠ¥ í‰ê°€...")
        results, stats = PerformanceAnalyzer.evaluate_agent(agent, env, num_episodes=5)
        PerformanceAnalyzer.print_performance_report(results, stats)
        
        # 4. í›ˆë ¨ ì—¬ë¶€ ê²°ì •
        if stats['overall_win_rate'] < 0.55 or not model_loaded:
            print(f"\n4ï¸âƒ£ í›ˆë ¨ ì‹œì‘...")
            print(f"   í˜„ì¬ ìŠ¹ë¥ : {stats['overall_win_rate']:.3f}")
            print(f"   ëª©í‘œ ìŠ¹ë¥ : 0.65+")
            
            # í›ˆë ¨ ì‹¤í–‰
            trained_agent, rewards, win_rates = train_agent(agent, env, episodes=500)
            
            # í›ˆë ¨ í›„ ì„±ëŠ¥ ì¬í‰ê°€
            print("\n5ï¸âƒ£ í›ˆë ¨ í›„ ì„±ëŠ¥ í‰ê°€...")
            final_results, final_stats = PerformanceAnalyzer.evaluate_agent(trained_agent, env, num_episodes=10)
            PerformanceAnalyzer.print_performance_report(final_results, final_stats)
            
            # ê°œì„ ë„ ì¶œë ¥
            improvement = final_stats['overall_win_rate'] - stats['overall_win_rate']
            print(f"\nğŸ¯ ì„±ëŠ¥ ê°œì„ ë„:")
            print(f"   ìŠ¹ë¥ : {stats['overall_win_rate']:.3f} â†’ {final_stats['overall_win_rate']:.3f} ({improvement:+.3f})")
            print(f"   í‰ê·  ìˆ˜ìµë¥ : {stats['avg_return']:.3f} â†’ {final_stats['avg_return']:.3f}")
            
            # ìµœì¢… ëª¨ë¸ ì €ì¥
            trained_agent.safe_save_model('agent/improved_crypto_rl_model.pth')
            
        else:
            print(f"âœ… í˜„ì¬ ì„±ëŠ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤ (ìŠ¹ë¥ : {stats['overall_win_rate']:.3f})")
            
            # ì¶”ê°€ í›ˆë ¨ ì—¬ë¶€ ë¬»ê¸°
            user_input = input("\nğŸ’« ì¶”ê°€ í›ˆë ¨ì„ ì›í•˜ì‹œë‚˜ìš”? (y/n): ")
            if user_input.lower() == 'y':
                print("ğŸš€ ì¶”ê°€ í›ˆë ¨ ì‹œì‘...")
                train_agent(agent, env, episodes=200)
    
    except KeyboardInterrupt:
        print("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()