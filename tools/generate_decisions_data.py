#!/usr/bin/env python3
"""
decisions_data.parquet íŒŒì¼ ìƒì„± ìŠ¤í¬ë¦½íŠ¸
"""

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from agent.decision_generator import (
    generate_signal_data_with_indicators,
    load_decisions_from_parquet,
    analyze_decision_data,
    inspect_parquet_structure,
    check_existing_decision_data,
    clear_progress_state
)
from managers.binance_dataloader import BinanceDataLoader
from datetime import datetime, timezone, timedelta
import pandas as pd
import time
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from collections import defaultdict


def fetch_single_batch(
    interval: str,
    batch_start: datetime,
    batch_end: datetime,
    batch_num: int,
    print_lock: Lock
):
    """ë‹¨ì¼ ë°°ì¹˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    try:
        # ê° ìŠ¤ë ˆë“œë§ˆë‹¤ ë…ë¦½ì ì¸ dataloader ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìŠ¤ë ˆë“œ ì•ˆì „)
        dataloader = BinanceDataLoader()
        batch_size = 1500
        
        df = dataloader.fetch_data(
            interval=interval,
            symbol="ETHUSDT",
            limit=batch_size,
            start_time=batch_start,
            end_time=batch_end
        )
        
        if df is None or df.empty:
            return batch_num, None
        
        # ì¸ë±ìŠ¤ ì²˜ë¦¬
        if not isinstance(df.index, pd.DatetimeIndex):
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)
                df = df.set_index('timestamp')
        
        if batch_num % 20 == 0:
            with print_lock:
                print(f"      [{interval}] ë°°ì¹˜ {batch_num} ì™„ë£Œ ({len(df)}ê°œ ìº”ë“¤)")
        
        return batch_num, df
        
    except Exception as e:
        with print_lock:
            print(f"      âš ï¸ [{interval}] ë°°ì¹˜ {batch_num} ì˜¤ë¥˜: {e}")
        return batch_num, None


def load_interval_data_parallel(
    interval: str,
    start_time: datetime,
    end_time: datetime,
    print_lock: Lock,
    max_workers: int = 10
):
    """ë‹¨ì¼ ê°„ê²© ë°ì´í„°ë¥¼ ë³‘ë ¬ë¡œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ (ê³ ì† ë³‘ë ¬ ì²˜ë¦¬)"""
    with print_lock:
        print(f"\n   {interval} ë°ì´í„° ë¡œë“œ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬: ìµœëŒ€ {max_workers}ê°œ ë™ì‹œ ìš”ì²­)...")
    
    batch_size = 1500
    interval_minutes = {'3m': 3, '15m': 15, '1h': 60}[interval]
    
    # ëª¨ë“  ë°°ì¹˜ ì‹œê°„ ë²”ìœ„ë¥¼ ë¯¸ë¦¬ ê³„ì‚°
    batch_ranges = []
    current_start = start_time
    batch_num = 0
    
    while current_start < end_time:
        batch_num += 1
        batch_end = min(
            current_start + timedelta(minutes=batch_size * interval_minutes),
            end_time
        )
        batch_ranges.append((batch_num, current_start, batch_end))
        current_start = batch_end
    
    with print_lock:
        print(f"      [{interval}] ì´ {len(batch_ranges)}ê°œ ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
    
    # ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ê°€ì ¸ì˜¤ê¸° (ë” ë§ì€ ë™ì‹œ ìš”ì²­)
    batch_results = {}
    completed_count = 0
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  ë°°ì¹˜ì— ëŒ€í•´ ë³‘ë ¬ ì‘ì—… ì œì¶œ
        future_to_batch = {
            executor.submit(
                fetch_single_batch,
                interval,
                batch_start,
                batch_end,
                batch_num,
                print_lock
            ): batch_num
            for batch_num, batch_start, batch_end in batch_ranges
        }
        
        # ì™„ë£Œëœ ì‘ì—…ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘
        for future in as_completed(future_to_batch):
            batch_num = future_to_batch[future]
            try:
                result_batch_num, df = future.result()
                if df is not None:
                    batch_results[result_batch_num] = df
                completed_count += 1
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥ (10%ë§ˆë‹¤)
                if completed_count % max(1, len(batch_ranges) // 10) == 0:
                    progress = (completed_count / len(batch_ranges)) * 100
                    with print_lock:
                        print(f"      [{interval}] ì§„í–‰ë¥ : {progress:.1f}% ({completed_count}/{len(batch_ranges)})")
                        
            except Exception as e:
                with print_lock:
                    print(f"      âš ï¸ [{interval}] ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ë°°ì¹˜ ë²ˆí˜¸ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ í•©ì¹˜ê¸°
    if batch_results:
        sorted_batches = sorted(batch_results.items())
        all_dataframes = [df for _, df in sorted_batches if df is not None]
        
        if all_dataframes:
            combined_df = pd.concat(all_dataframes, ignore_index=False)
            combined_df = combined_df.sort_index()
            combined_df = combined_df[~combined_df.index.duplicated(keep='first')]
            with print_lock:
                print(f"   âœ… {interval} ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(combined_df)}ê°œ ìº”ë“¤")
            return combined_df
    
    with print_lock:
        print(f"   âš ï¸ {interval} ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: ë°ì´í„° ì—†ìŒ")
    return None


def get_cache_filepath(interval: str, months_back: int) -> str:
    """ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
    os.makedirs("data", exist_ok=True)
    end_date = datetime.now(timezone.utc).strftime("%Y%m%d")
    start_date = (datetime.now(timezone.utc) - timedelta(days=months_back * 30)).strftime("%Y%m%d")
    return f"data/ETHUSDT_{interval}_{start_date}_{end_date}.parquet"


def is_cache_valid(cache_path: str, max_age_hours: int = 24) -> bool:
    """ìºì‹œ íŒŒì¼ì´ ìœ íš¨í•œì§€ í™•ì¸ (ê¸°ë³¸ 24ì‹œê°„)"""
    if not os.path.exists(cache_path):
        return False
    
    # íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸
    file_time = datetime.fromtimestamp(os.path.getmtime(cache_path), tz=timezone.utc)
    age_hours = (datetime.now(timezone.utc) - file_time).total_seconds() / 3600
    
    return age_hours < max_age_hours


def load_from_cache(interval: str, months_back: int) -> pd.DataFrame:
    """ìºì‹œ íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
    cache_path = get_cache_filepath(interval, months_back)
    
    if not os.path.exists(cache_path):
        return None
    
    try:
        df = pd.read_parquet(cache_path)
        # ì¸ë±ìŠ¤ê°€ timestampì¸ì§€ í™•ì¸
        if not isinstance(df.index, pd.DatetimeIndex):
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)
                df = df.set_index('timestamp')
        print(f"   âœ… [{interval}] ìºì‹œì—ì„œ ë¡œë“œ: {len(df)}ê°œ ìº”ë“¤")
        return df
    except Exception as e:
        print(f"   âš ï¸ [{interval}] ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def save_to_cache(df: pd.DataFrame, interval: str, months_back: int):
    """ë°ì´í„°ë¥¼ ìºì‹œ íŒŒì¼ë¡œ ì €ì¥"""
    if df is None or df.empty:
        return
    
    cache_path = get_cache_filepath(interval, months_back)
    try:
        # timestampë¥¼ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
        if not isinstance(df.index, pd.DatetimeIndex):
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)
                df = df.set_index('timestamp')
        
        df.to_parquet(cache_path, compression='snappy')
        file_size = os.path.getsize(cache_path) / 1024 / 1024
        print(f"   ğŸ’¾ [{interval}] ìºì‹œ ì €ì¥ ì™„ë£Œ: {cache_path} ({file_size:.2f}MB)")
    except Exception as e:
        print(f"   âš ï¸ [{interval}] ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")


def load_ethusdt_data_from_api(months_back: int = 3, use_cache: bool = True, max_cache_age_hours: int = 24):
    """APIì—ì„œ ETHUSDT ë°ì´í„° ë¡œë“œ (3ë¶„, 15ë¶„, 1ì‹œê°„ë´‰) - ìºì‹œ ì§€ì›"""
    intervals = ['3m', '15m', '1h']
    results = {}
    
    # 1. ìºì‹œì—ì„œ ë¡œë“œ ì‹œë„
    if use_cache:
        print("ğŸ“‚ ìºì‹œ íŒŒì¼ í™•ì¸ ì¤‘...")
        for interval in intervals:
            cache_path = get_cache_filepath(interval, months_back)
            if is_cache_valid(cache_path, max_cache_age_hours):
                df = load_from_cache(interval, months_back)
                if df is not None:
                    results[interval] = df
        
        # ëª¨ë“  ê°„ê²©ì˜ ìºì‹œê°€ ìˆìœ¼ë©´ ë°˜í™˜
        if len(results) == len(intervals):
            print("âœ… ëª¨ë“  ë°ì´í„°ë¥¼ ìºì‹œì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            return results.get('3m'), results.get('15m'), results.get('1h')
        
        # ì¼ë¶€ë§Œ ìºì‹œì— ìˆìœ¼ë©´ ì¶œë ¥
        if results:
            print(f"âš ï¸  ìºì‹œì—ì„œ {len(results)}/{len(intervals)}ê°œ ê°„ê²©ë§Œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤. ë‚˜ë¨¸ì§€ëŠ” APIì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    
    # 2. APIì—ì„œ ë¡œë“œ (ìºì‹œì— ì—†ëŠ” ê°„ê²©ë§Œ)
    print(f"\nğŸ“¥ APIì—ì„œ ê°€ê²© ë°ì´í„° ë¡œë“œ ì¤‘... (ìµœê·¼ {months_back}ê°œì›”)")
    print("   ğŸš€ ê³ ì† ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ í™œì„±í™”")
    
    end_time = datetime.now(timezone.utc)
    start_time = end_time - timedelta(days=months_back * 30)
    
    # ìŠ¤ë ˆë“œ ì•ˆì „ì„ ìœ„í•œ Lock
    print_lock = Lock()
    
    # ìºì‹œì— ì—†ëŠ” ê°„ê²©ë§Œ APIì—ì„œ ë¡œë“œ
    intervals_to_fetch = [iv for iv in intervals if iv not in results]
    
    if intervals_to_fetch:
        # ê°„ê²©ë³„ ë³‘ë ¬ ì²˜ë¦¬ (ìµœëŒ€ 3ê°œ ê°„ê²© ë™ì‹œ ì²˜ë¦¬)
        with ThreadPoolExecutor(max_workers=3) as interval_executor:
            # ëª¨ë“  ê°„ê²©ì— ëŒ€í•´ ë³‘ë ¬ ì‘ì—… ì œì¶œ
            future_to_interval = {
                interval_executor.submit(
                    load_interval_data_parallel,
                    interval,
                    start_time,
                    end_time,
                    print_lock,
                    max_workers=10  # ê° ê°„ê²© ë‚´ì—ì„œ ìµœëŒ€ 10ê°œ ë°°ì¹˜ ë™ì‹œ ì²˜ë¦¬
                ): interval
                for interval in intervals_to_fetch
            }
            
            # ì™„ë£Œëœ ì‘ì—… ì²˜ë¦¬
            for future in as_completed(future_to_interval):
                interval = future_to_interval[future]
                try:
                    df = future.result()
                    results[interval] = df
                    
                    # ìºì‹œì— ì €ì¥
                    if df is not None and use_cache:
                        save_to_cache(df, interval, months_back)
                        
                except Exception as e:
                    with print_lock:
                        print(f"   âŒ {interval} ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
                    results[interval] = None
    
    return results.get('3m'), results.get('15m'), results.get('1h')


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ“Š Decision ë°ì´í„° ìƒì„± ì‹œì‘")
    print("=" * 60)
    
    # 1. ê¸°ì¡´ ë°ì´í„° í™•ì¸
    if check_existing_decision_data():
        print("\nâš ï¸  ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤.")
        print("ê¸°ì¡´ íŒŒì¼ì„ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
        import os
        if os.path.exists("agent/decisions_data.parquet"):
            os.remove("agent/decisions_data.parquet")
            print("âœ… ê¸°ì¡´ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
        # ì§„í–‰ ìƒíƒœë„ ì‚­ì œ
        clear_progress_state()
    
    # 2. ì‹¤ì œ ETHUSDT ë°ì´í„° ë¡œë“œ (3ë¶„, 15ë¶„, 1ì‹œê°„ë´‰)
    print("\nğŸ“¥ ê°€ê²© ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    # ë¨¼ì € ê¸°ì¡´ CSV íŒŒì¼ ì‹œë„ (í•˜ìœ„ í˜¸í™˜ì„±)
    try:
        from agent.decision_generator import load_ethusdt_data
        price_data, price_data_15m, price_data_1h = load_ethusdt_data()
        if price_data is not None:
            print("âœ… ê¸°ì¡´ CSV íŒŒì¼ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except:
        price_data, price_data_15m, price_data_1h = None, None, None
    
    # CSVê°€ ì—†ìœ¼ë©´ ìºì‹œ/APIì—ì„œ ë¡œë“œ (ìºì‹œ ìš°ì„ )
    if price_data is None:
        price_data, price_data_15m, price_data_1h = load_ethusdt_data_from_api(
            months_back=1,  # 1ë‹¬ ë°ì´í„°
            use_cache=True,  # ìºì‹œ ì‚¬ìš©
            max_cache_age_hours=24  # 24ì‹œê°„ ì´ë‚´ ìºì‹œëŠ” ìœ íš¨
        )
    
    if price_data is None:
        print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
        return
    
    print(f"\nâœ… ê°€ê²© ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
    print(f"   - 3ë¶„ë´‰: {len(price_data)}ê°œ ìº”ë“¤")
    print(f"   - 15ë¶„ë´‰: {len(price_data_15m)}ê°œ ìº”ë“¤")
    print(f"   - 1ì‹œê°„ë´‰: {len(price_data_1h)}ê°œ ìº”ë“¤")
    print(f"   - ê°€ê²© ë²”ìœ„: ${price_data['close'].min():.2f} ~ ${price_data['close'].max():.2f}")
    
    # 3. CSV ë°ì´í„°ë¡œ ì‹¤ì œ ì§€í‘œ ì—…ë°ì´íŠ¸ ë° ì „ëµ ì‹¤í–‰
    print("\nğŸ”„ Decision ë°ì´í„° ìƒì„± ì¤‘...")
    print("   (ì´ ì‘ì—…ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    
    success = generate_signal_data_with_indicators(
        price_data, 
        price_data_15m, 
        price_data_1h, 
        resume_from_progress=False  # ì²˜ìŒë¶€í„° ì‹œì‘
    )
    
    if success:
        print("\n" + "=" * 60)
        print("âœ… Decision ë°ì´í„° ìƒì„± ì™„ë£Œ!")
        print("=" * 60)
        
        # ì €ì¥ëœ ë°ì´í„° í™•ì¸ ë° ë¶„ì„
        df = load_decisions_from_parquet()
        if df is not None:
            print(f"\nğŸ“Š ì €ì¥ëœ ë°ì´í„° ìš”ì•½:")
            print(f"   - ì´ ë ˆì½”ë“œ ìˆ˜: {len(df)}ê°œ")
            print(f"   - ì‹œê°„ ë²”ìœ„: {df['timestamp'].min()} ~ {df['timestamp'].max()}")
            print(f"   - ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
            
            # ìƒì„¸ ë¶„ì„
            analyze_decision_data(df)
            
            # Parquet êµ¬ì¡° í™•ì¸
            inspect_parquet_structure()
            
            print(f"\nğŸ¯ íŒŒì¼ ìœ„ì¹˜: agent/decisions_data.parquet")
            print(f"   ì´ ë°ì´í„°ë¥¼ ë©”íƒ€ ë¼ë²¨ë§ í•™ìŠµì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ ë°ì´í„° ìƒì„±ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì§„í–‰ ìƒíƒœê°€ ì €ì¥ë˜ì–´ ìˆì–´ì„œ ë‹¤ìŒì— ì´ì–´ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()

